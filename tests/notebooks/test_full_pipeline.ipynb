{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Test Full Pipeline: End-to-End (Local)\n\nThis notebook runs the complete inspection pipeline end-to-end **locally**.\n\n**Two pipeline modes** — toggle `USE_VISION_PIPELINE` in Cell 2:\n\n| Mode | Extraction Method | GPU Required? | Speed |\n|------|------------------|---------------|-------|\n| **Vision** (NEW) | GPT-4o reads full drawing image | No (API only) | ~10-30s |\n| **YOLO** (legacy) | YOLO detect → OCR → regex parse | Yes (~5.5 GB VRAM) | ~120-380s |\n\nBoth modes share the same downstream pipeline:\n`normalize → validate → expand → match → score → assembly context → GPT-4o report`\n\n**Requirements:**\n- `.env` file with `OPENAI_API_KEY` (required for both modes)\n- `.env` file with `HF_TOKEN` (required for YOLO mode only)\n- Sample drawing image (PNG/JPG)\n- Optional: SolidWorks JSON for comparison\n- Optional: Assembly context databases"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 1: Setup — load .env, add project root to path\nimport sys\nimport os\nfrom pathlib import Path\n\n# Navigate to project root (two levels up from tests/notebooks/)\nNOTEBOOK_DIR = Path(os.getcwd())\nPROJECT_ROOT = NOTEBOOK_DIR\n# Walk up until we find ai_inspector/\nfor _ in range(5):\n    if (PROJECT_ROOT / 'ai_inspector').is_dir():\n        break\n    PROJECT_ROOT = PROJECT_ROOT.parent\n\n# Add project root to Python path so ai_inspector is importable\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\n# Load .env for HF_TOKEN and OPENAI_API_KEY\nfrom dotenv import load_dotenv\nload_dotenv(PROJECT_ROOT / '.env')\n\nHF_TOKEN = os.getenv('HF_TOKEN')\nif HF_TOKEN:\n    print(f'HF_TOKEN loaded (length={len(HF_TOKEN)})')\nelse:\n    print('WARNING: HF_TOKEN not found in .env — OCR model loading will fail.')\n\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nif OPENAI_API_KEY:\n    print(f'OPENAI_API_KEY loaded (length={len(OPENAI_API_KEY)})')\nelse:\n    print('WARNING: OPENAI_API_KEY not found in .env — report generation will be skipped.')\n\nimport torch\nprint(f'Project root: {PROJECT_ROOT}')\nprint(f'Python: {sys.version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 2: Configure paths — EDIT THESE for your test case\nimport os\nfrom pathlib import Path\n\n# ============================================================\n# PIPELINE MODE — set True for GPT-4o vision, False for YOLO+OCR\n# ============================================================\nUSE_VISION_PIPELINE = True\n\n# ============================================================\n# EDIT THESE PATHS to point to your test files\n# ============================================================\nSAMPLE_IMAGE = str(PROJECT_ROOT / 'Drawing_Analysis_By_Type' / '01_Machined_Parts' / '314884_A.png')\nSW_JSON_PATH = str(PROJECT_ROOT / 'debug' / 'sw_temp' / '314884.json')\n# SW_JSON_PATH = ''  # Leave empty to skip SW comparison\n\n# Assembly context databases\nMATING_CONTEXT_PATH = str(PROJECT_ROOT / 'sw_mating_context.json')\nMATE_SPECS_PATH = str(PROJECT_ROOT / 'sw_mate_specs.json')\nPART_CONTEXT_PATH = str(PROJECT_ROOT / 'vba_extraction_legacy' / 'json_databases' / 'sw_part_context_complete.json')\n\n# Pipeline settings\nUSE_VLM = False if USE_VISION_PIPELINE else True  # VLM less useful with vision extraction\nCONFIDENCE = 0.25      # YOLO detection confidence threshold (YOLO mode only)\nOUTPUT_DIR = str(PROJECT_ROOT / 'debug' / ('run_vision' if USE_VISION_PIPELINE else 'run_notebook'))\n\n# OCR quick-win settings (YOLO mode only)\nfrom ai_inspector.config import default_config\n\nif not USE_VISION_PIPELINE:\n    default_config.ocr_retry_enabled = True\n    default_config.ocr_retry_confidence_threshold = 0.55\n    default_config.ocr_retry_max_tokens = 96\n    default_config.ocr_retry_max_crop_dimension = 512\n\n# ============================================================\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f'Pipeline mode:   {\"VISION (GPT-4o)\" if USE_VISION_PIPELINE else \"YOLO + OCR\"}')\nprint(f'Image:           {SAMPLE_IMAGE}')\nprint(f'  exists:        {os.path.exists(SAMPLE_IMAGE)}')\nprint(f'SW JSON:         {SW_JSON_PATH or \"<none>\"}')\nprint(f'  exists:        {os.path.exists(SW_JSON_PATH) if SW_JSON_PATH else \"N/A\"}')\nprint(f'Mating context:  {MATING_CONTEXT_PATH or \"<none>\"}')\nprint(f'  exists:        {os.path.exists(MATING_CONTEXT_PATH) if MATING_CONTEXT_PATH else \"N/A\"}')\nprint(f'Mate specs:      {MATE_SPECS_PATH or \"<none>\"}')\nprint(f'  exists:        {os.path.exists(MATE_SPECS_PATH) if MATE_SPECS_PATH else \"N/A\"}')\nprint(f'Part context:    {PART_CONTEXT_PATH or \"<none>\"}')\nprint(f'  exists:        {os.path.exists(PART_CONTEXT_PATH) if PART_CONTEXT_PATH else \"N/A\"}')\nprint(f'VLM:             {\"enabled\" if USE_VLM else \"disabled\"}')\nprint(f'Output dir:      {OUTPUT_DIR}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: Import pipeline\nfrom ai_inspector.pipeline.yolo_pipeline import PipelineResult\n\nif USE_VISION_PIPELINE:\n    from ai_inspector.pipeline.vision_pipeline import VisionPipeline\n    print('VisionPipeline imported successfully (GPT-4o extraction mode).')\nelse:\n    from ai_inspector.pipeline.yolo_pipeline import YOLOPipeline\n    print('YOLOPipeline imported successfully (YOLO + OCR mode).')\n\nprint(f'PipelineResult fields: {list(PipelineResult.__dataclass_fields__.keys())}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4: Create pipeline\nimport torch\n\ndef gpu_mem():\n    if torch.cuda.is_available():\n        a = torch.cuda.memory_allocated() / 1e9\n        r = torch.cuda.memory_reserved() / 1e9\n        t = torch.cuda.get_device_properties(0).total_memory / 1e9\n        return f'{a:.2f} GB allocated / {r:.2f} GB reserved / {t:.1f} GB total'\n    return 'No CUDA'\n\nif USE_VISION_PIPELINE:\n    # Vision pipeline — no GPU models, just OpenAI API\n    pipeline = VisionPipeline(api_key=OPENAI_API_KEY)\n    pipeline.load()\n    print(f'VisionPipeline ready: {pipeline.is_loaded}')\n    print('No GPU models needed — extraction via GPT-4o API')\nelse:\n    # YOLO pipeline — GPU models load/unload sequentially inside run()\n    print(f'GPU memory before: {gpu_mem()}')\n    pipeline = YOLOPipeline(\n        hf_token=HF_TOKEN,\n        confidence_threshold=CONFIDENCE,\n    )\n    pipeline.load()\n    print(f'YOLOPipeline ready: {pipeline.is_loaded}')\n    print(f'GPU memory after load(): {gpu_mem()}  (no GPU models yet)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 5: Run the full pipeline\nimport time\n\nmode_str = \"VISION (GPT-4o)\" if USE_VISION_PIPELINE else \"YOLO + OCR\"\nprint(f'Running {mode_str} pipeline on: {os.path.basename(SAMPLE_IMAGE)}')\nprint(f'VLM: {\"ON\" if USE_VLM else \"OFF\"}')\nprint(f'Mating context: {\"ON\" if MATING_CONTEXT_PATH else \"OFF\"}')\nprint(f'Mate specs: {\"ON\" if MATE_SPECS_PATH else \"OFF\"}')\nprint(f'Part context: {\"ON\" if PART_CONTEXT_PATH else \"OFF\"}')\nif USE_VISION_PIPELINE:\n    print(f'Extraction: GPT-4o vision (single API call)')\nelse:\n    print(f'Sequential loading: YOLO -> unload -> OCR -> unload -> VLM -> unload')\nprint()\n\nstart = time.time()\n\nresult = pipeline.run(\n    image_path=SAMPLE_IMAGE,\n    sw_json_path=SW_JSON_PATH or None,\n    page_id='test_page_0',\n    output_dir=OUTPUT_DIR,\n    save_crops=True,\n    use_vlm=USE_VLM,\n    mating_context_path=MATING_CONTEXT_PATH or None,\n    mate_specs_path=MATE_SPECS_PATH or None,\n    part_context_path=PART_CONTEXT_PATH or None,\n)\n\nelapsed = time.time() - start\n\nprint(f'Pipeline completed in {elapsed:.1f}s')\nif not USE_VISION_PIPELINE:\n    print(f'GPU memory after run: {gpu_mem()}  (all models unloaded)')\n    print(f'Detections: {len(result.packets)}')\nelse:\n    extracted = result.packet_summary.get('extracted_callouts', 0)\n    print(f'Extracted callouts: {extracted}  (via GPT-4o vision)')\nprint(f'Match results: {len(result.match_results)}')\nprint(f'VLM page understanding: {\"yes\" if result.page_understanding else \"no\"}')\nprint(f'Mating context: {\"found\" if result.mating_context else \"not found\"}')\nprint(f'Mate specs: {\"found\" if result.mate_specs else \"not found\"}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 6: VLM Page Understanding results\nimport json\n\npu = result.page_understanding\nif not pu:\n    print('VLM was disabled or returned no results.')\nelif pu.get('error'):\n    print(f'VLM ERROR: {pu[\"error\"]}')\nelse:\n    print('=== VLM PAGE UNDERSTANDING ===')\n    print()\n\n    if pu.get('units'):\n        print(f'  Units:         {pu[\"units\"]}')\n    if pu.get('drawingType'):\n        print(f'  Drawing type:  {pu[\"drawingType\"]}')\n\n    tb = pu.get('titleBlock', {})\n    if tb:\n        print(f'\\n  --- Title Block ---')\n        for k, v in tb.items():\n            if v:\n                print(f'  {k:15s}: {v}')\n\n    tol = pu.get('toleranceBlock', {})\n    if tol:\n        print(f'\\n  --- Default Tolerances ---')\n        for k, v in tol.items():\n            if v:\n                print(f'  {k:15s}: {v}')\n\n    notes = pu.get('generalNotes', [])\n    if notes:\n        print(f'\\n  --- General Notes ({len(notes)}) ---')\n        for note in notes:\n            print(f'  - {note}')\n\n    sf = pu.get('surfaceFinish', {})\n    if sf and sf.get('note'):\n        print(f'\\n  Surface finish: {sf[\"note\"]}')\n\n    views = pu.get('views', [])\n    if views:\n        print(f'\\n  Views: {\", \".join(views)}')\n\n    datums = pu.get('datumReferences', [])\n    if datums:\n        print(f'  Datums: {\", \".join(datums)}')\n\n    print(f'\\n  (Full output saved to: {OUTPUT_DIR}/page_understanding.json)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Assembly Mating Context\n\nmc = result.mating_context\nif not mc:\n    print('No mating context (path not provided or part not found in database).')\nelse:\n    print('=== ASSEMBLY MATING CONTEXT ===')\n    print()\n    print(f'  Part number:   {mc.get(\"part_number\", \"N/A\")}')\n    print(f'  Description:   {mc.get(\"description\", \"N/A\")}')\n    print(f'  Type:          {mc.get(\"type\", \"N/A\")}')\n    print(f'  Assembly:      {mc.get(\"assembly\", \"N/A\")}')\n\n    siblings = mc.get('siblings', [])\n    if siblings:\n        print(f'\\n  --- Sibling Components ({len(siblings)}) ---')\n        for s in siblings:\n            print(f'    {s.get(\"pn\", \"?\")} — {s.get(\"desc\", \"?\")} ({s.get(\"type\", \"?\")})')\n\n    siblings_str = mc.get('siblings_str', '')\n    if siblings_str:\n        print(f'\\n  Siblings summary: {siblings_str}')\n\n    print(f'\\n  (Full output saved to: {OUTPUT_DIR}/assembly_context.json)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Mate Specifications (thread/interface constraints)\n\nms = result.mate_specs\nif not ms:\n    print('No mate specs (path not provided or part/siblings not found).')\nelse:\n    source = ms.get('source', 'direct')\n    print(f'=== MATE SPECIFICATIONS (source: {source}) ===')\n    print()\n\n    if source == 'sibling_cross_reference':\n        # Specs found via sibling parts\n        print(f'  Part {ms.get(\"part_number\")} not directly in mate_specs.')\n        print(f'  Showing specs from {len(ms.get(\"sibling_specs\", []))} sibling(s):')\n        for spec in ms.get('sibling_specs', []):\n            pn = spec.get('part_number', '?')\n            desc = spec.get('description', '')\n            print(f'\\n  --- Sibling: {pn} ({desc}) ---')\n            for m in spec.get('mates_with', []):\n                line = f'    {m.get(\"mate_type\", \"?\")} with {m.get(\"part\", \"?\")}'\n                if m.get('description'):\n                    line += f' ({m[\"description\"]})'\n                if m.get('thread'):\n                    line += f'  ** THREAD: {m[\"thread\"]} pitch={m.get(\"pitch\",\"\")} len={m.get(\"length\",\"\")} **'\n                print(line)\n    else:\n        # Direct match\n        print(f'  Part: {ms.get(\"part_number\", \"?\")}')\n        print(f'  Description: {ms.get(\"description\", \"\")}')\n        print(f'  Total mates: {ms.get(\"mate_count\", \"?\")}')\n        for m in ms.get('mates_with', []):\n            line = f'    {m.get(\"mate_type\", \"?\")} with {m.get(\"part\", \"?\")}'\n            if m.get('description'):\n                line += f' ({m[\"description\"]})'\n            if m.get('thread'):\n                line += f'  ** THREAD: {m[\"thread\"]} pitch={m.get(\"pitch\",\"\")} len={m.get(\"length\",\"\")} **'\n            print(line)\n\n    print(f'\\n  (Full output saved to: {OUTPUT_DIR}/mate_specs.json)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 9: Scores, expansion, validation\nimport json\n\nprint('=== SCORES ===')\nif result.scores:\n    for key, val in result.scores.items():\n        print(f'  {key:25s}: {val}')\nelse:\n    print('  (no scores — run without SW data)')\n\nprint('\\n=== EXPANSION SUMMARY ===')\nprint(json.dumps(result.expansion_summary, indent=2))\n\nprint('\\n=== VALIDATION STATS ===')\nprint(json.dumps(result.validation_stats, indent=2))\n\nprint('\\n=== PACKET SUMMARY ===')\nprint(json.dumps(result.packet_summary, indent=2))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 10: Match results table\nfrom ai_inspector.comparison.matcher import MatchStatus\n\nif not result.match_results:\n    print('No match results (no SW data provided or no features detected).')\nelse:\n    print(f'{\"#\":>3s} {\"Status\":15s} {\"Type\":18s} {\"Delta\":>10s} {\"Notes\"}')\n    print('=' * 90)\n\n    for i, r in enumerate(result.match_results):\n        callout_type = ''\n        if r.drawing_callout:\n            callout_type = r.drawing_callout.get('calloutType', '')\n        elif r.sw_feature:\n            callout_type = r.sw_feature.feature_type\n\n        delta_str = f'{r.delta:+.4f}' if r.delta is not None else 'N/A'\n\n        marker = {\n            MatchStatus.MATCHED: '[OK]',\n            MatchStatus.MISSING: '[MISS]',\n            MatchStatus.EXTRA: '[EXTRA]',\n            MatchStatus.TOLERANCE_FAIL: '[TOL]',\n            MatchStatus.SKIPPED: '[SKIP]',\n        }.get(r.status, '[?]')\n\n        print(f'{i:3d} {marker + \" \" + r.status.value:15s} '\n              f'{callout_type:18s} {delta_str:>10s} {r.notes[:50]}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 11: Packet provenance / extracted callouts\n\nif USE_VISION_PIPELINE:\n    # Vision pipeline: show extracted callouts from debug file\n    import json\n    callouts_path = Path(OUTPUT_DIR) / 'callouts_extracted.json'\n    if callouts_path.exists():\n        with open(callouts_path, 'r', encoding='utf-8') as f:\n            callouts = json.load(f)\n        print(f'=== GPT-4o Extracted Callouts ({len(callouts)}) ===')\n        print()\n        for i, c in enumerate(callouts):\n            ct = c.get('calloutType', '?')\n            raw = c.get('raw', '')[:60]\n            qty = c.get('quantity', 1)\n            parts = [f'{ct}']\n            if c.get('diameter') is not None:\n                parts.append(f'dia={c[\"diameter\"]}')\n            if c.get('radius') is not None:\n                parts.append(f'R={c[\"radius\"]}')\n            if c.get('size') is not None:\n                parts.append(f'size={c[\"size\"]}')\n            if c.get('thread'):\n                t = c['thread']\n                parts.append(f'thread={t.get(\"raw\", \"\")}')\n            if qty > 1:\n                parts.append(f'qty={qty}')\n            print(f'  {i+1:2d}. {\" | \".join(parts)}')\n            print(f'      raw: \"{raw}\"')\n    else:\n        print('No callouts_extracted.json found.')\nelse:\n    # YOLO pipeline: show packet provenance\n    if not result.packets:\n        print('No packets (no detections found in image).')\n    else:\n        print(f'=== Packet Provenance (first {min(5, len(result.packets))}) ===')\n        print()\n\n        for i, pkt in enumerate(result.packets[:5]):\n            print(f'--- Packet {i}: {pkt.det_id} ---')\n\n            if pkt.detection:\n                print(f'  Detection: class={pkt.detection.class_name}, '\n                      f'conf={pkt.detection.confidence:.3f}')\n\n            if pkt.crop:\n                meta = pkt.crop.meta or {}\n                print(f'  Crop: {meta.get(\"crop_w\", \"?\")}x{meta.get(\"crop_h\", \"?\")}px, '\n                      f'angle={meta.get(\"rotation_angle\", 0):.1f}deg')\n\n            if pkt.rotation:\n                print(f'  Rotation: {pkt.rotation.rotation_used}deg, '\n                      f'quality={pkt.rotation.quality_score:.2f}')\n\n            if pkt.reader:\n                print(f'  Reader: type={pkt.reader.callout_type}, '\n                      f'source={pkt.reader.source}, '\n                      f'ocr_conf={pkt.reader.ocr_confidence:.2f}')\n                print(f'  Raw OCR: \"{pkt.reader.raw[:80]}\"')\n                if pkt.reader.parsed:\n                    parsed_keys = [k for k in pkt.reader.parsed.keys() if not k.startswith('_')]\n                    print(f'  Parsed: {dict((k, pkt.reader.parsed[k]) for k in parsed_keys)}')\n\n            if pkt.normalized:\n                method = pkt.normalized.get('_normalization_method', '?')\n                units = pkt.normalized.get('_detected_units', '?')\n                draw_units = pkt.normalized.get('_drawing_units', '?')\n                print(f'  Normalization: method={method}, detected={units}, drawing={draw_units}')\n\n            print(f'  Validated: {pkt.validated}'\n                  + (f', error=\"{pkt.validation_error}\"' if pkt.validation_error else ''))\n            if hasattr(pkt, 'match_status') and pkt.match_status:\n                print(f'  Match: {pkt.match_status}')\n\n            print()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 12: List saved artifacts\nfrom pathlib import Path\n\nout = Path(OUTPUT_DIR)\nprint(f'Artifacts saved to: {OUTPUT_DIR}/')\nprint()\n\nfor f in sorted(out.rglob('*')):\n    if f.is_file():\n        size_kb = f.stat().st_size / 1024\n        print(f'  {f.relative_to(out)}  ({size_kb:.1f} KB)')\n\n# Save the full result summary\nimport json\nsummary_path = out / 'pipeline_summary.json'\nwith open(summary_path, 'w', encoding='utf-8') as f:\n    json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)\nprint(f'\\nPipeline summary saved to: {summary_path}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 13: GPT-4o QC Report — full JSON context passed to GPT\nimport json, os\nfrom pathlib import Path\n\nif not OPENAI_API_KEY:\n    print('OPENAI_API_KEY not set — skipping report generation.')\nelse:\n    from ai_inspector.report.qc_report import generate_from_pipeline\n\n    # --- Load extracted callouts from debug output ---\n    extracted_callouts = []\n    callouts_path = Path(OUTPUT_DIR) / 'callouts_extracted.json'\n    if callouts_path.exists():\n        with open(callouts_path, 'r', encoding='utf-8') as f:\n            extracted_callouts = json.load(f)\n\n    # --- Load validated callouts from debug output ---\n    validated_callouts = []\n    validated_path = Path(OUTPUT_DIR) / 'validated_callouts.json'\n    if validated_path.exists():\n        with open(validated_path, 'r', encoding='utf-8') as f:\n            validated_callouts = json.load(f)\n\n    # --- Load SW identity (part number, description, material, etc.) ---\n    sw_identity = {}\n    if SW_JSON_PATH and os.path.exists(SW_JSON_PATH):\n        with open(SW_JSON_PATH, 'r', encoding='utf-8-sig') as f:\n            sw_raw = json.load(f)\n        sw_identity = sw_raw.get('identity', {})\n\n    # --- Generate report with full JSON context ---\n    print(f'Part: {sw_identity.get(\"partNumber\", \"?\")} — {sw_identity.get(\"description\", \"?\")}')\n    print(f'Extracted callouts: {len(extracted_callouts)}')\n    print(f'Validated callouts: {len(validated_callouts)}')\n    print(f'Match results: {len(result.match_results)}')\n    print()\n    print('Sending full inspection context as JSON to GPT-4o...')\n\n    report, inspection_context = generate_from_pipeline(\n        result=result,\n        extracted_callouts=extracted_callouts,\n        validated_callouts=validated_callouts,\n        sw_identity=sw_identity,\n        api_key=OPENAI_API_KEY,\n        model='gpt-4o',\n    )\n\n    print(f'Report generated — Status: {report.status}')\n    print(f'Model: {report.model_used}')\n    print()\n    print('=' * 70)\n    print(report.report_text)\n    print('=' * 70)\n\n    # Save report\n    report_path = Path(OUTPUT_DIR) / 'qc_report.md'\n    with open(report_path, 'w', encoding='utf-8') as f:\n        f.write(report.report_text)\n\n    # Save full context that was sent to GPT\n    context_path = Path(OUTPUT_DIR) / 'inspection_context.json'\n    with open(context_path, 'w', encoding='utf-8') as f:\n        json.dump(inspection_context, f, indent=2, ensure_ascii=False, default=str)\n\n    # Save structured report data\n    report_data_path = Path(OUTPUT_DIR) / 'qc_report.json'\n    with open(report_data_path, 'w', encoding='utf-8') as f:\n        json.dump(report.to_dict(), f, indent=2, ensure_ascii=False)\n\n    print(f'\\nReport saved to: {report_path}')\n    print(f'Full context saved to: {context_path}')\n    print(f'Report data saved to: {report_data_path}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 14: Cleanup\nimport gc, torch\n\npipeline.unload()\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(f'GPU memory after cleanup: {gpu_mem()}')\nelse:\n    print('No GPU cleanup needed (vision pipeline is CPU + API only).')\nprint('Done.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}