{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO11-OBB Finetuning for Engineering Drawing Callouts\n",
    "\n",
    "Train a YOLO11n-OBB model to detect 4 callout types:\n",
    "- **Hole** (idx 0) — diameter callouts\n",
    "- **TappedHole** (idx 1) — thread callouts\n",
    "- **Fillet** (idx 4) — radius callouts\n",
    "- **Chamfer** (idx 5) — chamfer callouts\n",
    "\n",
    "**Requirements:** A100 GPU, Roboflow API key, annotated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q ultralytics roboflow wandb\n\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected. Training will be very slow.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repo & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/AI-Drawing-Inspector\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from roboflow import Roboflow\n\n# --- Roboflow credentials ---\nROBOFLOW_API_KEY = \"IHyhfpN5KngIAXiN5MdM\"\nROBOFLOW_WORKSPACE = \"ai-drawing-inspector\"\nROBOFLOW_PROJECT = \"ai-inspector-callout-detection\"\nROBOFLOW_VERSION = 2\n# -----------------------------\n\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\nproject = rf.workspace(ROBOFLOW_WORKSPACE).project(ROBOFLOW_PROJECT)\ndataset = project.version(ROBOFLOW_VERSION).download(\"yolov8-obb\", location=\"/content/dataset\")\n\nDATASET_ROOT = \"/content/dataset\"\nprint(f\"Dataset downloaded to: {DATASET_ROOT}\")\nprint(f\"Train images: {len(os.listdir(os.path.join(DATASET_ROOT, 'train', 'images')))}\")\n\n# Check if valid split exists\nvalid_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\nif os.path.isdir(valid_img_dir) and len(os.listdir(valid_img_dir)) > 0:\n    print(f\"Val images:   {len(os.listdir(valid_img_dir))}\")\nelse:\n    print(\"No valid split from Roboflow — will create one next.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 3b: Create Train/Valid Split (if needed)\n\nRoboflow may export all images into train/. This cell splits 20% into valid/,\nensuring all 4 classes are represented in the validation set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import shutil, random\nfrom collections import defaultdict\n\ntrain_img_dir = os.path.join(DATASET_ROOT, \"train\", \"images\")\ntrain_lbl_dir = os.path.join(DATASET_ROOT, \"train\", \"labels\")\nvalid_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\nvalid_lbl_dir = os.path.join(DATASET_ROOT, \"valid\", \"labels\")\n\n# Only split if valid is empty or missing\nneeds_split = True\nif os.path.isdir(valid_img_dir):\n    valid_files = [f for f in os.listdir(valid_img_dir) if f.endswith(('.jpg','.png','.jpeg'))]\n    if len(valid_files) > 0:\n        needs_split = False\n        print(f\"Valid split already exists ({len(valid_files)} images). Skipping.\")\n\nif needs_split:\n    os.makedirs(valid_img_dir, exist_ok=True)\n    os.makedirs(valid_lbl_dir, exist_ok=True)\n\n    # Roboflow alphabetical: 0=Chamfer, 1=Fillet, 2=Hole, 3=TappedHole\n    RF_CLASS_MAP = {0: \"Chamfer\", 1: \"Fillet\", 2: \"Hole\", 3: \"TappedHole\"}\n\n    # Parse classes per image\n    image_classes = {}\n    for img in sorted(os.listdir(train_img_dir)):\n        if not img.endswith(('.jpg', '.png', '.jpeg')):\n            continue\n        base = os.path.splitext(img)[0]\n        lbl_path = os.path.join(train_lbl_dir, base + '.txt')\n        classes = set()\n        if os.path.exists(lbl_path):\n            with open(lbl_path) as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) >= 9:\n                        classes.add(RF_CLASS_MAP.get(int(parts[0]), 'unk'))\n        image_classes[img] = classes\n\n    random.seed(42)\n    val_set = set()\n\n    # Ensure each class is represented in valid\n    for target_cls in [\"Chamfer\", \"TappedHole\", \"Fillet\", \"Hole\"]:\n        candidates = [img for img, cls in image_classes.items()\n                      if target_cls in cls and img not in val_set]\n        random.shuffle(candidates)\n        for img in candidates[:3]:\n            val_set.add(img)\n\n    # Fill to ~20% total\n    target_val = max(16, int(len(image_classes) * 0.2))\n    remaining = [img for img in image_classes if img not in val_set]\n    random.shuffle(remaining)\n    while len(val_set) < target_val and remaining:\n        val_set.add(remaining.pop())\n\n    # Move files\n    for img in sorted(val_set):\n        base = os.path.splitext(img)[0]\n        shutil.move(os.path.join(train_img_dir, img), os.path.join(valid_img_dir, img))\n        lbl_file = base + '.txt'\n        lbl_src = os.path.join(train_lbl_dir, lbl_file)\n        if os.path.exists(lbl_src):\n            shutil.move(lbl_src, os.path.join(valid_lbl_dir, lbl_file))\n\n    train_count = len([f for f in os.listdir(train_img_dir) if f.endswith(('.jpg','.png'))])\n    valid_count = len([f for f in os.listdir(valid_img_dir) if f.endswith(('.jpg','.png'))])\n    print(f\"Split complete: {train_count} train / {valid_count} valid\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Remap Class Indices\n",
    "\n",
    "Roboflow exports classes alphabetically (Chamfer=0, Fillet=1, Hole=2, TappedHole=3).\n",
    "Our pipeline uses classes.py indices (Hole=0, TappedHole=1, Fillet=4, Chamfer=5).\n",
    "This cell remaps all label files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "from ai_inspector.fine_tuning.data_generator import (\n",
    "    remap_labels,\n",
    "    ROBOFLOW_TO_CLASSES_PY,\n",
    ")\n",
    "\n",
    "print(\"Remapping class indices:\")\n",
    "print(f\"  Roboflow -> classes.py: {ROBOFLOW_TO_CLASSES_PY}\")\n",
    "print()\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    label_dir = os.path.join(DATASET_ROOT, split, \"labels\")\n",
    "    if os.path.isdir(label_dir):\n",
    "        result = remap_labels(label_dir, ROBOFLOW_TO_CLASSES_PY)\n",
    "        print(f\"  {split}: {result['files']} files, {result['annotations']} annotations\")\n",
    "    else:\n",
    "        print(f\"  {split}: no labels directory\")\n",
    "\n",
    "# Verify a sample\n",
    "train_labels = os.path.join(DATASET_ROOT, \"train\", \"labels\")\n",
    "sample_file = sorted(os.listdir(train_labels))[0]\n",
    "print(f\"\\nSample label ({sample_file}):\")\n",
    "with open(os.path.join(train_labels, sample_file)) as f:\n",
    "    for line in f.readlines()[:3]:\n",
    "        cls_idx = int(line.split()[0])\n",
    "        from ai_inspector.detection.classes import IDX_TO_CLASS\n",
    "        cls_name = IDX_TO_CLASS.get(cls_idx, f\"UNKNOWN({cls_idx})\")\n",
    "        print(f\"  class {cls_idx} = {cls_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Generate dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_inspector.fine_tuning.data_generator import generate_dataset_yaml\n",
    "\n",
    "yaml_path = os.path.join(DATASET_ROOT, \"dataset.yaml\")\n",
    "generate_dataset_yaml(yaml_path, DATASET_ROOT)\n",
    "\n",
    "print(f\"Generated: {yaml_path}\")\n",
    "print()\n",
    "with open(yaml_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Train YOLO11n-OBB\n\nDrawing-safe augmentation settings:\n- No horizontal/vertical flip (text becomes unreadable)\n- No hue/saturation changes (drawings are monochrome)\n- Minimal rotation (callouts are axis-aligned)\n- Reduced mosaic probability\n- W&B logging enabled for live monitoring"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import wandb\nfrom ultralytics import YOLO\n\n# Initialize W&B — uses API key from Colab secrets or environment\nwandb.init(\n    project=\"ai-inspector-callout-detection\",\n    name=\"callout_v1_yolo11n-obb\",\n    config={\n        \"model\": \"yolo11n-obb\",\n        \"epochs\": 150,\n        \"batch\": 32,\n        \"imgsz\": 1024,\n        \"train_images\": len(os.listdir(os.path.join(DATASET_ROOT, \"train\", \"images\"))),\n        \"valid_images\": len(os.listdir(os.path.join(DATASET_ROOT, \"valid\", \"images\"))),\n        \"classes\": [\"Hole\", \"TappedHole\", \"Fillet\", \"Chamfer\"],\n        \"augmentation\": \"drawing-safe (no flip, no hue/sat)\",\n    },\n)\n\n# Load pretrained model\nmodel = YOLO(\"yolo11n-obb.pt\")\n\n# Train with drawing-safe augmentation\nresults = model.train(\n    data=os.path.join(DATASET_ROOT, \"dataset.yaml\"),\n    epochs=150,\n    batch=32,\n    imgsz=1024,\n    amp=True,\n    device=0,\n    project=\"runs/obb\",\n    name=\"callout_v1\",\n    # Drawing-safe augmentation\n    flipud=0.0,        # No vertical flip\n    fliplr=0.0,        # No horizontal flip\n    hsv_h=0.0,         # No hue shift (monochrome)\n    hsv_s=0.0,         # No saturation shift\n    hsv_v=0.1,         # Tiny brightness variation\n    degrees=2.0,       # Minimal rotation (callouts are axis-aligned)\n    mosaic=0.3,        # Reduced mosaic (preserve spatial context)\n    scale=0.3,         # Moderate scale augmentation\n    translate=0.1,     # Small translation\n    # Performance\n    workers=4,\n    patience=30,       # Early stopping\n    save_period=25,    # Save checkpoint every 25 epochs\n    verbose=True,\n)\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Validate & Print Per-Class mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model\nbest_model = YOLO(\"runs/obb/callout_v1/weights/best.pt\")\n\n# Run validation\nval_results = best_model.val(\n    data=os.path.join(DATASET_ROOT, \"dataset.yaml\"),\n    imgsz=1024,\n    batch=32,\n    device=0,\n)\n\n# Print per-class results\nprint(\"\\n\" + \"=\"*50)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"mAP50:     {val_results.box.map50:.4f}\")\nprint(f\"mAP50-95:  {val_results.box.map:.4f}\")\nprint()\n\n# Per-class mAP50\nfrom ai_inspector.detection.classes import IDX_TO_CLASS\nprint(\"Per-class mAP50:\")\nif hasattr(val_results.box, 'ap50') and val_results.box.ap50 is not None:\n    for i, ap in enumerate(val_results.box.ap50):\n        cls_name = IDX_TO_CLASS.get(i, f\"class_{i}\")\n        if ap > 0:\n            print(f\"  {cls_name:20s}: {ap:.4f}\")\nelse:\n    print(\"  (per-class AP not available in this format)\")\n\n# Log final metrics to W&B\nwandb.log({\n    \"val/mAP50\": val_results.box.map50,\n    \"val/mAP50-95\": val_results.box.map,\n})\n\n# Pass/Fail check\ntarget_map = 0.5\nif val_results.box.map50 >= target_map:\n    print(f\"\\nPASS: mAP50 ({val_results.box.map50:.4f}) >= {target_map}\")\nelse:\n    print(f\"\\nBELOW TARGET: mAP50 ({val_results.box.map50:.4f}) < {target_map}\")\n    print(\"Consider: more annotations, more epochs, or larger model (yolo11s-obb.pt)\")\n\n# Finish W&B run\nwandb.finish()\nprint(\"W&B run finished.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Copy Best Weights to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create output directory\n",
    "drive_dir = \"/content/drive/MyDrive/AI-Inspector-Models\"\n",
    "os.makedirs(drive_dir, exist_ok=True)\n",
    "\n",
    "# Copy best weights\n",
    "src = \"runs/obb/callout_v1/weights/best.pt\"\n",
    "dst = os.path.join(drive_dir, \"callout_v1_best.pt\")\n",
    "shutil.copy2(src, dst)\n",
    "print(f\"Saved: {dst}\")\n",
    "\n",
    "# Also copy last weights as backup\n",
    "src_last = \"runs/obb/callout_v1/weights/last.pt\"\n",
    "if os.path.exists(src_last):\n",
    "    dst_last = os.path.join(drive_dir, \"callout_v1_last.pt\")\n",
    "    shutil.copy2(src_last, dst_last)\n",
    "    print(f\"Saved: {dst_last}\")\n",
    "\n",
    "# Copy training results\n",
    "results_src = \"runs/obb/callout_v1/results.csv\"\n",
    "if os.path.exists(results_src):\n",
    "    shutil.copy2(results_src, os.path.join(drive_dir, \"callout_v1_results.csv\"))\n",
    "    print(f\"Saved: callout_v1_results.csv\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {drive_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Pick a validation image\n",
    "val_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\n",
    "test_images = sorted(os.listdir(val_img_dir))[:3]\n",
    "\n",
    "# Color map for classes\n",
    "CLASS_COLORS = {\n",
    "    0: \"#00FF00\",  # Hole - green\n",
    "    1: \"#FF6600\",  # TappedHole - orange\n",
    "    4: \"#0099FF\",  # Fillet - blue\n",
    "    5: \"#FF00FF\",  # Chamfer - magenta\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, min(3, len(test_images)), figsize=(20, 8))\n",
    "if len(test_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img_name in zip(axes, test_images):\n",
    "    img_path = os.path.join(val_img_dir, img_name)\n",
    "    \n",
    "    # Run inference\n",
    "    results = best_model(img_path, imgsz=1024, conf=0.25)\n",
    "    \n",
    "    # Plot\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(img_name[:30], fontsize=10)\n",
    "    \n",
    "    if results[0].obb is not None:\n",
    "        for obb in results[0].obb:\n",
    "            cls_id = int(obb.cls[0])\n",
    "            conf = float(obb.conf[0])\n",
    "            cls_name = IDX_TO_CLASS.get(cls_id, f\"cls{cls_id}\")\n",
    "            color = CLASS_COLORS.get(cls_id, \"#FFFFFF\")\n",
    "            \n",
    "            # Get OBB corners\n",
    "            if hasattr(obb, 'xyxyxyxy'):\n",
    "                corners = obb.xyxyxyxy[0].cpu().numpy()\n",
    "                polygon = patches.Polygon(\n",
    "                    corners, closed=True,\n",
    "                    linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(polygon)\n",
    "                ax.text(\n",
    "                    corners[0][0], corners[0][1] - 5,\n",
    "                    f\"{cls_name} {conf:.2f}\",\n",
    "                    color=color, fontsize=8,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"black\", alpha=0.7)\n",
    "                )\n",
    "    \n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"inference_preview.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: inference_preview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Integration Test with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_inspector.pipeline import YOLOPipeline\n",
    "from ai_inspector.config import Config\n",
    "\n",
    "# Use the finetuned model\n",
    "config = Config(yolo_model_path=\"runs/obb/callout_v1/weights/best.pt\")\n",
    "pipeline = YOLOPipeline(model_path=config.yolo_model_path, config=config)\n",
    "\n",
    "# Run on a validation image\n",
    "test_img = os.path.join(val_img_dir, test_images[0])\n",
    "result = pipeline.run(test_img)\n",
    "\n",
    "print(f\"Image: {test_images[0]}\")\n",
    "print(f\"Detections: {len(result.detections)}\")\n",
    "print(f\"Callouts:   {len(result.callouts)}\")\n",
    "print()\n",
    "\n",
    "for i, callout in enumerate(result.callouts[:10]):\n",
    "    print(f\"  [{i}] {callout.get('calloutType', '?'):15s} | \"\n",
    "          f\"raw: {callout.get('raw_text', '?')[:40]}\")\n",
    "\n",
    "print(\"\\nPipeline integration: OK\" if result.callouts else \"\\nNo callouts extracted\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}