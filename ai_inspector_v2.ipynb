{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/ai_inspector_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QxQqDl17wtB"
   },
   "source": [
    "# AI Engineering Drawing Inspector v2.0\n",
    "\n",
    "**Single-File QC Pipeline**\n",
    "\n",
    "Outputs:\n",
    "1. `ResolvedPartIdentity.json`\n",
    "2. `DrawingEvidence.json`\n",
    "3. `DiffResult.json`\n",
    "4. `QCReport.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUxTZFYm7wtC"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q pymupdf opencv-python-headless jsonschema pillow pytesseract\n",
    "!pip install -q accelerate qwen-vl-utils bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import os, json, re, gc\n",
    "import torch\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DRAWING_PDF_PATH = \"\"\n",
    "SOLIDWORKS_JSON_DIR = \"sw_json_library\"\n",
    "OUTPUT_DIR = \"qc_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "metadata": {
    "id": "BywKsGSW7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: BOM-Robust JSON Loader\n",
    "def load_json_robust(filepath) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"Load JSON with BOM handling. Tries: utf-8-sig, utf-8, latin-1\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=enc) as f:\n",
    "                return json.load(f), None\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except json.JSONDecodeError as e:\n",
    "            if 'BOM' in str(e) and enc == 'utf-8':\n",
    "                continue\n",
    "            return None, f\"JSON error: {str(e)[:50]}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Error: {str(e)[:50]}\"\n",
    "    return None, \"Failed all encodings\"\n",
    "\n",
    "print(\"load_json_robust defined\")"
   ],
   "metadata": {
    "id": "Fip3em4Z7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: PDF Rendering\n",
    "@dataclass\n",
    "class PageArtifact:\n",
    "    pageIndex0: int\n",
    "    page: int\n",
    "    image: Image.Image\n",
    "    width: int\n",
    "    height: int\n",
    "    dpi: int\n",
    "    direct_text: Optional[str] = None\n",
    "\n",
    "def render_pdf(pdf_path: str, dpi: int = 300) -> List[PageArtifact]:\n",
    "    \"\"\"Render first page of PDF to image.\"\"\"\n",
    "    artifacts = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    zoom = dpi / 72.0\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    direct_text = page.get_text(\"text\")\n",
    "\n",
    "    artifacts.append(PageArtifact(\n",
    "        pageIndex0=0, page=1, image=img,\n",
    "        width=pix.width, height=pix.height, dpi=dpi,\n",
    "        direct_text=direct_text if len(direct_text.strip()) > 10 else None\n",
    "    ))\n",
    "    doc.close()\n",
    "    print(f\"Rendered: {pix.width}x{pix.height}px\")\n",
    "    return artifacts\n",
    "\n",
    "print(\"render_pdf defined\")"
   ],
   "metadata": {
    "id": "g5CgMeOp7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: SolidWorks JSON Library\n",
    "@dataclass\n",
    "class SwPartEntry:\n",
    "    json_path: str\n",
    "    part_number: str\n",
    "    filename_stem: str = \"\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class SwJsonLibrary:\n",
    "    def __init__(self):\n",
    "        self.by_part_number: Dict[str, SwPartEntry] = {}\n",
    "        self.by_filename: Dict[str, SwPartEntry] = {}\n",
    "        self.all_entries: List[SwPartEntry] = []\n",
    "\n",
    "    def _normalize(self, s: str) -> str:\n",
    "        return re.sub(r'[-\\s_]', '', str(s or '')).lower()\n",
    "\n",
    "    def load_from_directory(self, directory: str):\n",
    "        json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for jp in json_files:\n",
    "            data, err = load_json_robust(jp)\n",
    "            if data is None:\n",
    "                continue\n",
    "            pn = data.get('identity', {}).get('partNumber', '')\n",
    "            entry = SwPartEntry(str(jp), pn, jp.stem, data)\n",
    "            self.all_entries.append(entry)\n",
    "            if pn:\n",
    "                self.by_part_number[pn] = entry\n",
    "                self.by_part_number[self._normalize(pn)] = entry\n",
    "            self.by_filename[jp.stem] = entry\n",
    "            self.by_filename[self._normalize(jp.stem)] = entry\n",
    "        print(f\"Loaded {len(self.all_entries)} files\")\n",
    "\n",
    "    def lookup(self, candidate: str) -> Optional[SwPartEntry]:\n",
    "        if not candidate:\n",
    "            return None\n",
    "        norm = self._normalize(candidate)\n",
    "        return self.by_part_number.get(candidate) or self.by_part_number.get(norm) or \\\n",
    "               self.by_filename.get(candidate) or self.by_filename.get(norm)\n",
    "\n",
    "sw_library = SwJsonLibrary()\n",
    "print(\"SwJsonLibrary defined\")"
   ],
   "metadata": {
    "id": "w7qDW6w27wtE",
    "outputId": "e4fdcb09-12c7-404e-f075-658428932780",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SwJsonLibrary defined\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Part Identity Resolution (Robust Matching)\n\n@dataclass\nclass ResolvedPartIdentity:\n    partNumber: str\n    confidence: float\n    source: str\n    swJsonPath: Optional[str] = None\n    candidates_tried: List[str] = field(default_factory=list)\n\ndef clean_filename(filename: str) -> str:\n    \"\"\"Remove known suffixes like Paint, REV, etc.\"\"\"\n    cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', filename, flags=re.IGNORECASE)\n    return cleaned.strip()\n\ndef extract_pn_candidates(filename: str) -> List[str]:\n    \"\"\"\n    Extract potential part number candidates from filename.\n    Handles: 1013572_01, 101357201-03, 314884W_0, 046-935-REV-A\n    Returns list of candidates (most specific to least).\n    \"\"\"\n    name_no_ext = os.path.splitext(filename)[0]\n    # Remove duplicate markers like (1), (2)\n    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)\n    cleaned = clean_filename(name_no_ext)\n    parts = re.split(r'[\\s_]+', cleaned)\n    \n    if not parts:\n        return []\n    \n    base = parts[0]\n    candidates = []\n    \n    # 1. Base as-is\n    candidates.append(base)\n    \n    # 2. Without hyphens\n    base_no_hyphen = base.replace('-', '')\n    if base_no_hyphen != base:\n        candidates.append(base_no_hyphen)\n    \n    # 3. Remove letter suffixes (046-935A -> 046-935)\n    if base and base[-1].isalpha() and len(base) > 1:\n        candidates.append(base[:-1])\n        candidates.append(base[:-1].replace('-', ''))\n    \n    # 4. Handle revision pattern (046-935-01 -> 046-935)\n    rev_match = re.match(r'^(.+)-(\\d{1,2})$', base)\n    if rev_match:\n        main_part = rev_match.group(1)\n        candidates.append(main_part)\n        candidates.append(main_part.replace('-', ''))\n    \n    # 5. Handle REV suffix (046-935-REV-A -> 046-935)\n    rev_alpha = re.match(r'^(.+?)[-_]?REV[-_]?[A-Z0-9]*$', base, re.IGNORECASE)\n    if rev_alpha:\n        candidates.append(rev_alpha.group(1))\n        candidates.append(rev_alpha.group(1).replace('-', ''))\n    \n    # 6. Peeling - progressively remove trailing digits\n    temp = base_no_hyphen\n    while len(temp) > 5:\n        temp = temp[:-1]\n        candidates.append(temp)\n    \n    # Remove duplicates, preserve order\n    seen = set()\n    unique = []\n    for c in candidates:\n        if c and c not in seen:\n            seen.add(c)\n            unique.append(c)\n    \n    return unique\n\ndef resolve_part_identity(pdf_path: str, artifacts: List[PageArtifact], sw_lib: SwJsonLibrary) -> ResolvedPartIdentity:\n    \"\"\"Resolve part identity using robust filename matching.\"\"\"\n    filename = os.path.basename(pdf_path)\n    candidates = extract_pn_candidates(filename)\n    \n    # Try each candidate against SW library\n    for candidate in candidates:\n        entry = sw_lib.lookup(candidate)\n        if entry:\n            return ResolvedPartIdentity(\n                partNumber=entry.part_number or candidate,\n                confidence=1.0,\n                source=\"filename+sw\",\n                swJsonPath=entry.json_path,\n                candidates_tried=candidates\n            )\n    \n    # Try PDF embedded text\n    for art in artifacts:\n        if art.direct_text:\n            text_candidates = extract_pn_candidates(art.direct_text[:200])\n            for candidate in text_candidates[:5]:\n                entry = sw_lib.lookup(candidate)\n                if entry:\n                    return ResolvedPartIdentity(\n                        partNumber=entry.part_number or candidate,\n                        confidence=0.8,\n                        source=\"pdf_text+sw\",\n                        swJsonPath=entry.json_path,\n                        candidates_tried=candidates + text_candidates[:5]\n                    )\n    \n    # Fallback - use first candidate or filename stem\n    fallback_pn = candidates[0] if candidates else Path(pdf_path).stem\n    return ResolvedPartIdentity(\n        partNumber=fallback_pn,\n        confidence=0.3,\n        source=\"fallback\",\n        swJsonPath=None,\n        candidates_tried=candidates\n    )\n\nprint(\"resolve_part_identity defined (robust matching)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Load SolidWorks Library (Upload ZIP)\nfrom google.colab import files\nimport zipfile\n\nif not os.path.exists(SOLIDWORKS_JSON_DIR) or not list(Path(SOLIDWORKS_JSON_DIR).glob(\"*.json\")):\n    print(\"Upload your sw_json_library.zip file:\")\n    uploaded = files.upload()\n    \n    for filename in uploaded:\n        if filename.endswith('.zip'):\n            print(f\"Extracting {filename}...\")\n            with zipfile.ZipFile(filename, 'r') as z:\n                z.extractall(SOLIDWORKS_JSON_DIR)\n            print(f\"Extracted to {SOLIDWORKS_JSON_DIR}\")\n            break\n\nsw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\nprint(f\"Library ready: {len(sw_library.all_entries)} parts indexed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Upload and Render PDF Drawing\nfrom google.colab import files\nfrom IPython.display import display\n\nprint(\"Upload your PDF drawing:\")\nuploaded = files.upload()\n\nfor filename in uploaded:\n    if filename.lower().endswith('.pdf'):\n        DRAWING_PDF_PATH = filename\n        break\n\nprint(f\"Processing: {DRAWING_PDF_PATH}\")\nartifacts = render_pdf(DRAWING_PDF_PATH)\n\n# Display the rendered image\nif artifacts:\n    display(artifacts[0].image.resize((800, int(800 * artifacts[0].height / artifacts[0].width))))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Resolve Part Identity\npart_identity = resolve_part_identity(DRAWING_PDF_PATH, artifacts, sw_library)\n\nprint(\"=\"*50)\nprint(\"RESOLVED PART IDENTITY\")\nprint(\"=\"*50)\nprint(f\"Part Number:  {part_identity.partNumber}\")\nprint(f\"Confidence:   {part_identity.confidence}\")\nprint(f\"Source:       {part_identity.source}\")\nprint(f\"SW JSON:      {part_identity.swJsonPath or 'Not found'}\")\nprint(f\"Candidates:   {part_identity.candidates_tried[:5]}\")\n\n# Save to output\nidentity_out = os.path.join(OUTPUT_DIR, \"ResolvedPartIdentity.json\")\nwith open(identity_out, 'w') as f:\n    json.dump(asdict(part_identity), f, indent=2)\nprint(f\"\\nSaved: {identity_out}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Load LightOnOCR-2 and Run OCR\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom google.colab import userdata\n\n# Clear GPU memory\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Get HF token\ntry:\n    hf_token = userdata.get('HF_TOKEN')\nexcept:\n    hf_token = None\n\nprint(\"Loading LightOnOCR-2-1B...\")\nocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n\nocr_processor = AutoProcessor.from_pretrained(\n    \"lightonai/LightOnOCR-2-1B\",\n    token=hf_token,\n    trust_remote_code=True\n)\n\nocr_model = AutoModelForVision2Seq.from_pretrained(\n    \"lightonai/LightOnOCR-2-1B\",\n    torch_dtype=ocr_dtype,\n    token=hf_token,\n    trust_remote_code=True\n).to(ocr_device)\n\nprint(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n\ndef run_lighton_ocr(image: Image.Image) -> List[str]:\n    \"\"\"Run LightOnOCR-2 on image, return list of text lines.\"\"\"\n    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n    \n    img = image.convert(\"RGB\")\n    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n    \n    inputs = ocr_processor.apply_chat_template(\n        conversation, add_generation_prompt=True, tokenize=True,\n        return_dict=True, return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n    \n    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n    output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n    \n    return [line.strip() for line in output_text.split(\"\\n\") if line.strip()]\n\n# Run OCR on the drawing\nprint(\"Running OCR on drawing...\")\nocr_lines = run_lighton_ocr(artifacts[0].image)\nprint(f\"OCR extracted {len(ocr_lines)} lines\")\nprint(\"\\nFirst 10 lines:\")\nfor line in ocr_lines[:10]:\n    print(f\"  {line}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}