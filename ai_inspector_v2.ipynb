{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/ai_inspector_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QxQqDl17wtB"
   },
   "source": [
    "# AI Engineering Drawing Inspector v2.0\n",
    "\n",
    "**Single-File QC Pipeline**\n",
    "\n",
    "Outputs:\n",
    "1. `ResolvedPartIdentity.json`\n",
    "2. `DrawingEvidence.json`\n",
    "3. `DiffResult.json`\n",
    "4. `QCReport.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUxTZFYm7wtC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f828d6b8-8e55-40f6-f2a8-e199d7911be1"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q pymupdf opencv-python-headless jsonschema pillow pytesseract\n",
    "!pip install -q accelerate qwen-vl-utils bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q json-repair\n",
    "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import os, json, re, gc\n",
    "import torch\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DRAWING_PDF_PATH = \"\"\n",
    "SOLIDWORKS_JSON_DIR = \"sw_json_library\"\n",
    "OUTPUT_DIR = \"qc_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "metadata": {
    "id": "BywKsGSW7wtD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "71bdd93a-ff42-4206-b2a9-2e66aeed8ac8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: BOM-Robust JSON Loader\n",
    "def load_json_robust(filepath) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"Load JSON with BOM handling. Tries: utf-8-sig, utf-8, latin-1\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=enc) as f:\n",
    "                return json.load(f), None\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except json.JSONDecodeError as e:\n",
    "            if 'BOM' in str(e) and enc == 'utf-8':\n",
    "                continue\n",
    "            return None, f\"JSON error: {str(e)[:50]}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Error: {str(e)[:50]}\"\n",
    "    return None, \"Failed all encodings\"\n",
    "\n",
    "print(\"load_json_robust defined\")"
   ],
   "metadata": {
    "id": "Fip3em4Z7wtD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01b8d138-42d3-4863-89c5-8f488930e250"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: PDF Rendering\n",
    "@dataclass\n",
    "class PageArtifact:\n",
    "    pageIndex0: int\n",
    "    page: int\n",
    "    image: Image.Image\n",
    "    width: int\n",
    "    height: int\n",
    "    dpi: int\n",
    "    direct_text: Optional[str] = None\n",
    "\n",
    "def render_pdf(pdf_path: str, dpi: int = 300) -> List[PageArtifact]:\n",
    "    \"\"\"Render first page of PDF to image.\"\"\"\n",
    "    artifacts = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    zoom = dpi / 72.0\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    direct_text = page.get_text(\"text\")\n",
    "\n",
    "    artifacts.append(PageArtifact(\n",
    "        pageIndex0=0, page=1, image=img,\n",
    "        width=pix.width, height=pix.height, dpi=dpi,\n",
    "        direct_text=direct_text if len(direct_text.strip()) > 10 else None\n",
    "    ))\n",
    "    doc.close()\n",
    "    print(f\"Rendered: {pix.width}x{pix.height}px\")\n",
    "    return artifacts\n",
    "\n",
    "print(\"render_pdf defined\")"
   ],
   "metadata": {
    "id": "g5CgMeOp7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: SolidWorks JSON Library\n",
    "@dataclass\n",
    "class SwPartEntry:\n",
    "    json_path: str\n",
    "    part_number: str\n",
    "    filename_stem: str = \"\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class SwJsonLibrary:\n",
    "    def __init__(self):\n",
    "        self.by_part_number: Dict[str, SwPartEntry] = {}\n",
    "        self.by_filename: Dict[str, SwPartEntry] = {}\n",
    "        self.all_entries: List[SwPartEntry] = []\n",
    "\n",
    "    def _normalize(self, s: str) -> str:\n",
    "        return re.sub(r'[-\\s_]', '', str(s or '')).lower()\n",
    "\n",
    "    def load_from_directory(self, directory: str):\n",
    "        json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for jp in json_files:\n",
    "            data, err = load_json_robust(jp)\n",
    "            if data is None:\n",
    "                continue\n",
    "            pn = data.get('identity', {}).get('partNumber', '')\n",
    "            entry = SwPartEntry(str(jp), pn, jp.stem, data)\n",
    "            self.all_entries.append(entry)\n",
    "            if pn:\n",
    "                self.by_part_number[pn] = entry\n",
    "                self.by_part_number[self._normalize(pn)] = entry\n",
    "            self.by_filename[jp.stem] = entry\n",
    "            self.by_filename[self._normalize(jp.stem)] = entry\n",
    "        print(f\"Loaded {len(self.all_entries)} files\")\n",
    "\n",
    "    def lookup(self, candidate: str) -> Optional[SwPartEntry]:\n",
    "        if not candidate:\n",
    "            return None\n",
    "        norm = self._normalize(candidate)\n",
    "        return self.by_part_number.get(candidate) or self.by_part_number.get(norm) or \\\n",
    "               self.by_filename.get(candidate) or self.by_filename.get(norm)\n",
    "\n",
    "sw_library = SwJsonLibrary()\n",
    "print(\"SwJsonLibrary defined\")"
   ],
   "metadata": {
    "id": "w7qDW6w27wtE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6: Part Identity Resolution (Robust Matching)\n",
    "\n",
    "@dataclass\n",
    "class ResolvedPartIdentity:\n",
    "    partNumber: str\n",
    "    confidence: float\n",
    "    source: str\n",
    "    swJsonPath: Optional[str] = None\n",
    "    candidates_tried: List[str] = field(default_factory=list)\n",
    "\n",
    "def clean_filename(filename: str) -> str:\n",
    "    \"\"\"Remove known suffixes like Paint, REV, etc.\"\"\"\n",
    "    cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', filename, flags=re.IGNORECASE)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_pn_candidates(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract potential part number candidates from filename.\n",
    "    Handles: 1013572_01, 101357201-03, 314884W_0, 046-935-REV-A\n",
    "    Returns list of candidates (most specific to least).\n",
    "    \"\"\"\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    # Remove duplicate markers like (1), (2)\n",
    "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)\n",
    "    cleaned = clean_filename(name_no_ext)\n",
    "    parts = re.split(r'[\\s_]+', cleaned)\n",
    "\n",
    "    if not parts:\n",
    "        return []\n",
    "\n",
    "    base = parts[0]\n",
    "    candidates = []\n",
    "\n",
    "    # 1. Base as-is\n",
    "    candidates.append(base)\n",
    "\n",
    "    # 2. Without hyphens\n",
    "    base_no_hyphen = base.replace('-', '')\n",
    "    if base_no_hyphen != base:\n",
    "        candidates.append(base_no_hyphen)\n",
    "\n",
    "    # 3. Remove letter suffixes (046-935A -> 046-935)\n",
    "    if base and base[-1].isalpha() and len(base) > 1:\n",
    "        candidates.append(base[:-1])\n",
    "        candidates.append(base[:-1].replace('-', ''))\n",
    "\n",
    "    # 4. Handle revision pattern (046-935-01 -> 046-935)\n",
    "    rev_match = re.match(r'^(.+)-(\\d{1,2})$', base)\n",
    "    if rev_match:\n",
    "        main_part = rev_match.group(1)\n",
    "        candidates.append(main_part)\n",
    "        candidates.append(main_part.replace('-', ''))\n",
    "\n",
    "    # 5. Handle REV suffix (046-935-REV-A -> 046-935)\n",
    "    rev_alpha = re.match(r'^(.+?)[-_]?REV[-_]?[A-Z0-9]*$', base, re.IGNORECASE)\n",
    "    if rev_alpha:\n",
    "        candidates.append(rev_alpha.group(1))\n",
    "        candidates.append(rev_alpha.group(1).replace('-', ''))\n",
    "\n",
    "    # 6. Peeling - progressively remove trailing digits\n",
    "    temp = base_no_hyphen\n",
    "    while len(temp) > 5:\n",
    "        temp = temp[:-1]\n",
    "        candidates.append(temp)\n",
    "\n",
    "    # Remove duplicates, preserve order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return unique\n",
    "\n",
    "def resolve_part_identity(pdf_path: str, artifacts: List[PageArtifact], sw_lib: SwJsonLibrary) -> ResolvedPartIdentity:\n",
    "    \"\"\"Resolve part identity using robust filename matching.\"\"\"\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    candidates = extract_pn_candidates(filename)\n",
    "\n",
    "    # Try each candidate against SW library\n",
    "    for candidate in candidates:\n",
    "        entry = sw_lib.lookup(candidate)\n",
    "        if entry:\n",
    "            return ResolvedPartIdentity(\n",
    "                partNumber=entry.part_number or candidate,\n",
    "                confidence=1.0,\n",
    "                source=\"filename+sw\",\n",
    "                swJsonPath=entry.json_path,\n",
    "                candidates_tried=candidates\n",
    "            )\n",
    "\n",
    "    # Try PDF embedded text\n",
    "    for art in artifacts:\n",
    "        if art.direct_text:\n",
    "            text_candidates = extract_pn_candidates(art.direct_text[:200])\n",
    "            for candidate in text_candidates[:5]:\n",
    "                entry = sw_lib.lookup(candidate)\n",
    "                if entry:\n",
    "                    return ResolvedPartIdentity(\n",
    "                        partNumber=entry.part_number or candidate,\n",
    "                        confidence=0.8,\n",
    "                        source=\"pdf_text+sw\",\n",
    "                        swJsonPath=entry.json_path,\n",
    "                        candidates_tried=candidates + text_candidates[:5]\n",
    "                    )\n",
    "\n",
    "    # Fallback - use first candidate or filename stem\n",
    "    fallback_pn = candidates[0] if candidates else Path(pdf_path).stem\n",
    "    return ResolvedPartIdentity(\n",
    "        partNumber=fallback_pn,\n",
    "        confidence=0.3,\n",
    "        source=\"fallback\",\n",
    "        swJsonPath=None,\n",
    "        candidates_tried=candidates\n",
    "    )\n",
    "\n",
    "print(\"resolve_part_identity defined (robust matching)\")"
   ],
   "metadata": {
    "id": "U5TWtUzhJnRw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Load SolidWorks Library (Upload ZIP)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(SOLIDWORKS_JSON_DIR) or not list(Path(SOLIDWORKS_JSON_DIR).glob(\"*.json\")):\n",
    "    print(\"Upload your sw_json_library.zip file:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded:\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as z:\n",
    "                z.extractall(SOLIDWORKS_JSON_DIR)\n",
    "            print(f\"Extracted to {SOLIDWORKS_JSON_DIR}\")\n",
    "            break\n",
    "\n",
    "sw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\n",
    "print(f\"Library ready: {len(sw_library.all_entries)} parts indexed\")"
   ],
   "metadata": {
    "id": "qOgdhn3iJnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Upload and Render PDF Drawing\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Upload your PDF drawing:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded:\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        DRAWING_PDF_PATH = filename\n",
    "        break\n",
    "\n",
    "print(f\"Processing: {DRAWING_PDF_PATH}\")\n",
    "artifacts = render_pdf(DRAWING_PDF_PATH)\n",
    "\n",
    "# Display the rendered image\n",
    "if artifacts:\n",
    "    display(artifacts[0].image.resize((800, int(800 * artifacts[0].height / artifacts[0].width))))"
   ],
   "metadata": {
    "id": "ZQJrQtNzJnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 9: Resolve Part Identity\n",
    "part_identity = resolve_part_identity(DRAWING_PDF_PATH, artifacts, sw_library)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RESOLVED PART IDENTITY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Part Number:  {part_identity.partNumber}\")\n",
    "print(f\"Confidence:   {part_identity.confidence}\")\n",
    "print(f\"Source:       {part_identity.source}\")\n",
    "print(f\"SW JSON:      {part_identity.swJsonPath or 'Not found'}\")\n",
    "print(f\"Candidates:   {part_identity.candidates_tried[:5]}\")\n",
    "\n",
    "# Save to output\n",
    "identity_out = os.path.join(OUTPUT_DIR, \"ResolvedPartIdentity.json\")\n",
    "with open(identity_out, 'w') as f:\n",
    "    json.dump(asdict(part_identity), f, indent=2)\n",
    "print(f\"\\nSaved: {identity_out}\")"
   ],
   "metadata": {
    "id": "nWyBd12_JnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 10: Load LightOnOCR-2 and Run OCR\n",
    "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
    "from google.colab import userdata\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Get HF token\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    hf_token = None\n",
    "\n",
    "print(\"Loading LightOnOCR-2-1B...\")\n",
    "ocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n",
    "\n",
    "ocr_processor = LightOnOcrProcessor.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    torch_dtype=ocr_dtype,\n",
    "    token=hf_token\n",
    ").to(ocr_device)\n",
    "\n",
    "print(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "def run_lighton_ocr(image: Image.Image) -> List[str]:\n",
    "    \"\"\"Run LightOnOCR-2 on image, return list of text lines.\"\"\"\n",
    "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
    "\n",
    "    img = image.convert(\"RGB\")\n",
    "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n",
    "\n",
    "    inputs = ocr_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return [line.strip() for line in output_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "# Run OCR on the drawing\n",
    "print(\"Running OCR on drawing...\")\n",
    "ocr_lines = run_lighton_ocr(artifacts[0].image)\n",
    "print(f\"OCR extracted {len(ocr_lines)} lines\")\n",
    "print(\"\\nFirst 10 lines:\")\n",
    "for line in ocr_lines[:10]:\n",
    "    print(f\"  {line}\")"
   ],
   "metadata": {
    "id": "NQuVga-jJnRx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "6d8cb633637947beb0986365e507fb89",
      "11d6a066a09f4ab6985c9cebdc01f28c",
      "20d47e35df1b4654a1caccd298981e41",
      "4feae93817e14b79b7315bdd01fc7694",
      "ee59d6c245604de5810ca56238df624d",
      "395213106c784b698b3227e0b8411eba",
      "76a15fdc82ad45c2bd70db4305a4ead8",
      "c4e75a9efb19494c9f585ee543060497",
      "af8de84578af4d04825ca6397a1b5457",
      "17c59dd010d74fbfa92ffdbb4e926348",
      "36cb6ddc8dac4afcae53de9d93378244"
     ]
    },
    "outputId": "35dc45ce-b8d0-4101-80a5-0fcbfbf56a70"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 10b: Qwen2.5-VL Drawing Understanding (with JSON repair)\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from json_repair import repair_json\n",
    "\n",
    "# Clear some GPU memory before loading Qwen\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading Qwen2.5-VL-7B for drawing understanding...\")\n",
    "qwen_model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_model_id, trust_remote_code=True)\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    qwen_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"Qwen2.5-VL loaded: {qwen_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "def analyze_drawing_with_qwen(image: Image.Image) -> Dict[str, Any]:\n",
    "    \"\"\"Use Qwen2.5-VL to understand the engineering drawing visually.\"\"\"\n",
    "\n",
    "    prompt = \"\"\"Analyze this engineering drawing and identify all features. Return a JSON object with:\n",
    "\n",
    "{\n",
    "  \"partDescription\": \"brief description of the part\",\n",
    "  \"views\": [\"list of views shown: TOP, FRONT, SIDE, ISOMETRIC, SECTION, DETAIL\"],\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"TappedHole|ThroughHole|BlindHole|Counterbore|Countersink|Slot|Fillet|Chamfer|Thread\",\n",
    "      \"description\": \"brief description\",\n",
    "      \"callout\": \"the dimension/callout text if visible\",\n",
    "      \"quantity\": 1,\n",
    "      \"location\": \"where on the part\"\n",
    "    }\n",
    "  ],\n",
    "  \"material\": \"material if shown in title block\",\n",
    "  \"titleBlockInfo\": {\n",
    "    \"partNumber\": \"if visible\",\n",
    "    \"revision\": \"if visible\",\n",
    "    \"scale\": \"if visible\"\n",
    "  },\n",
    "  \"notes\": [\"any general notes visible on drawing\"]\n",
    "}\n",
    "\n",
    "Be thorough - identify ALL holes, threads, chamfers, fillets, and other machined features you can see.\n",
    "Only return valid JSON, no other text.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = qwen_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(qwen_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=4096, temperature=0.1)\n",
    "\n",
    "    generated_ids = output_ids[0, inputs.input_ids.shape[1]:]\n",
    "    response = qwen_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Parse JSON from response with repair\n",
    "    try:\n",
    "        # Try to extract JSON block from response\n",
    "        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "        else:\n",
    "            json_match = re.search(r'\\{[\\s\\S]*\\}', response)\n",
    "            json_str = json_match.group() if json_match else response\n",
    "\n",
    "        # Try direct parse first\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # Use json-repair to fix malformed JSON\n",
    "            print(\"  Attempting JSON repair...\")\n",
    "            repaired = repair_json(json_str)\n",
    "            return json.loads(repaired)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"raw_response\": response[:1000], \"parse_error\": str(e)}\n",
    "\n",
    "# Analyze the drawing\n",
    "print(\"Analyzing drawing with Qwen2.5-VL...\")\n",
    "qwen_understanding = analyze_drawing_with_qwen(artifacts[0].image)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"QWEN2.5-VL DRAWING UNDERSTANDING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if \"parse_error\" not in qwen_understanding:\n",
    "    print(f\"Part: {qwen_understanding.get('partDescription', 'N/A')}\")\n",
    "    print(f\"Views: {qwen_understanding.get('views', [])}\")\n",
    "    print(f\"Material: {qwen_understanding.get('material', 'N/A')}\")\n",
    "    print(f\"\\nFeatures identified: {len(qwen_understanding.get('features', []))}\")\n",
    "    for f in qwen_understanding.get('features', [])[:10]:\n",
    "        print(f\"  - {f.get('type')}: {f.get('callout', f.get('description', ''))}\")\n",
    "    if qwen_understanding.get('notes'):\n",
    "        print(f\"\\nNotes: {qwen_understanding.get('notes', [])[:3]}\")\n",
    "else:\n",
    "    print(f\"Parse error: {qwen_understanding.get('parse_error')}\")\n",
    "    print(f\"Raw response:\\n{qwen_understanding.get('raw_response', '')[:500]}\")\n",
    "\n",
    "# Save understanding\n",
    "understanding_out = os.path.join(OUTPUT_DIR, \"QwenUnderstanding.json\")\n",
    "with open(understanding_out, 'w') as f:\n",
    "    json.dump(qwen_understanding, f, indent=2)\n",
    "print(f\"\\nSaved: {understanding_out}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503,
     "referenced_widgets": [
      "f8742eb41fab497ba281655192a114ac",
      "e3f65c48467542e58565542cca30c9d5",
      "ef098d0d1aa245dca3b97ebcd632627d",
      "b1544a4969d644569fb7b97067b3d590",
      "74bdac0bd1e041538b3039e0e8c683a8",
      "5363bb466f9f4045bb35ce729d8317e6",
      "f86bf2be7cca40419a7f6b919c398a13",
      "fba7d3fa3b1d4d1ea981a0465d151f85",
      "fb6e931eed134e05b2f1f899b1c9e011",
      "ffd58f4bba81420aac5af57e13e2274d",
      "219c366a577644fb94db7eab6b876436",
      "62cd9f11297c4f77b227e5a963718ca6",
      "c24a9305fa054ab4a35277ac4ab2bb86",
      "c695847144ec4cb799f7697408cfe0b2",
      "aad9aa0b59e942658e5f570772bf1076",
      "13145392bc1c44319cec4ba35e940ac8",
      "ccd34443ff1d4a25b0a84d80e5369b63",
      "c02b293403eb4f9fac7ad10324ffdd97",
      "7044cbb302af47d890c40d227f082a7b",
      "8994eb13c8284d09b0a236fae93a3860",
      "8c49e459cf6344e79f9bf78d64610ae5",
      "cd7f7fb3ea104fb8a49565c7147991c9",
      "1bfe08e7fd0a47ec8a8c3a1407c340b0",
      "d6c7690cbd0f4b0e8192f77bdcbfca45",
      "8e0e99b5da574536aeb20d2281ce177b",
      "3c258c3522884e05b9332e90f11ca60d",
      "9f526fcbaca743648f17cd567f384975",
      "b647c233f0904a628d8c2ac27cedf304",
      "3eca00b628d84d4ea0ab22cf9d9a0e49",
      "1fbab13f6c83494cac016e8b18ffaa67",
      "9641a756a7ae47d98ab2ddcf665a4ce8",
      "9cc1cd6f1fbf4c1c929d6d3857d63b6c",
      "579efa5e128a4881bcc7f3c172d17db3",
      "39f1402a91ac4f16a539abfbe7a6d744",
      "c327d834e69a4a8093e167c888e4b631",
      "f01a8c4705b84bfc95d90a4e33ff31e4",
      "55ebd7e73d85471aa079f92c61b05bb7",
      "c944c0e378c041c5b924924c0a795ed5",
      "b0520cd694dc45499cb83555559e9877",
      "f57dd0de6cdb4f29b0aa60ba92f6f5c9",
      "341fae84019b4d398d998628f250300c",
      "8f94dc08de5247e8a6930e100f7a6d1f",
      "092726aebeb04d60812e95f055ac5b3c",
      "1cb95142cec545beb7d0b3bc5cab44cd"
     ]
    },
    "id": "IZg8-vwCYxps",
    "outputId": "3e70c417-4c08-48ac-c19c-4e341b372084"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: Merge OCR + Qwen Understanding into Enriched Evidence\n# All dimensions in INCHES (matching drawing callouts)\n\n# Regex patterns for extracting callouts from OCR\nPATTERNS = {\n    'metric_thread': r'M(\\d+(?:\\.\\d+)?)\\s*[xX]\\s*(\\d+(?:\\.\\d+)?)',\n    'imperial_thread': r'(\\d+/\\d+)\\s*-\\s*(\\d+)',\n    'thru_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*(?:mm|MM|\")?\\s*THRU',\n    'blind_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*[xX]\\s*(\\d+\\.?\\d*)\\s*(?:DEEP|DP)',\n    'fillet': r'\\bR(\\d+\\.?\\d*)\\b',\n    'chamfer': r'(\\d+\\.?\\d*)\\s*[xX]\\s*45\\s*[\u00b0]?',\n}\n\ndef parse_ocr_callouts(ocr_lines: List[str]) -> List[Dict]:\n    \"\"\"Extract callouts from OCR text. Hole diameters stored in inches (as-is from drawing).\"\"\"\n    callouts = []\n    raw_text = \"\\n\".join(ocr_lines)\n\n    # Metric threads (M6x1.0) - keep in metric\n    for match in re.finditer(PATTERNS['metric_thread'], raw_text, re.IGNORECASE):\n        callouts.append({\n            'calloutType': 'TappedHole',\n            'thread': {'standard': 'Metric', 'nominalDiameterMm': float(match.group(1)), 'pitch': float(match.group(2))},\n            'raw': match.group(0), 'source': 'ocr'\n        })\n\n    # Through holes - store diameter in inches (no conversion)\n    for match in re.finditer(PATTERNS['thru_hole'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        callouts.append({\n            'calloutType': 'Hole',\n            'diameterInches': val,  # Keep as inches\n            'isThrough': True,\n            'raw': raw,\n            'source': 'ocr'\n        })\n\n    # Fillets - store in inches\n    for match in re.finditer(PATTERNS['fillet'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        callouts.append({\n            'calloutType': 'Fillet',\n            'radiusInches': val,\n            'raw': raw,\n            'source': 'ocr'\n        })\n\n    # Chamfers - store in inches\n    for match in re.finditer(PATTERNS['chamfer'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        callouts.append({\n            'calloutType': 'Chamfer',\n            'distance1Inches': val,\n            'angleDegrees': 45,\n            'raw': raw,\n            'source': 'ocr'\n        })\n\n    return callouts\n\ndef parse_qwen_features(qwen_data: Dict) -> List[Dict]:\n    \"\"\"Convert Qwen features to callout format. Dimensions in inches.\"\"\"\n    callouts = []\n    if \"parse_error\" in qwen_data:\n        return callouts\n\n    type_map = {\n        'TappedHole': 'TappedHole', 'ThroughHole': 'Hole', 'BlindHole': 'Hole',\n        'Counterbore': 'Hole', 'Countersink': 'Hole', 'Fillet': 'Fillet',\n        'Chamfer': 'Chamfer', 'Thread': 'TappedHole', 'Slot': 'Slot'\n    }\n\n    # Filter out general notes misclassified as features\n    note_keywords = ['REMOVE ALL BURRS', 'BREAK SHARP EDGES', 'DEBURR', 'CLEAN']\n\n    for feat in qwen_data.get('features', []):\n        ftype = feat.get('type', '')\n        callout_type = type_map.get(ftype, ftype)\n        callout = feat.get('callout', '')\n        qty = feat.get('quantity', 1)\n\n        # Skip if this looks like a general note, not a feature\n        if any(kw in callout.upper() for kw in note_keywords):\n            continue\n\n        entry = {\n            'calloutType': callout_type,\n            'description': feat.get('description', ''),\n            'location': feat.get('location', ''),\n            'quantity': qty,\n            'raw': callout,\n            'source': 'qwen'\n        }\n\n        # Try to parse dimensions from Qwen's callout text\n        if callout:\n            # Metric threads\n            thread_match = re.search(r'M(\\d+(?:\\.\\d+)?)[xX](\\d+(?:\\.\\d+)?)', callout)\n            if thread_match:\n                entry['thread'] = {\n                    'standard': 'Metric',\n                    'nominalDiameterMm': float(thread_match.group(1)),\n                    'pitch': float(thread_match.group(2))\n                }\n\n            # Hole diameters - keep in inches\n            hole_match = re.search(r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*)', callout)\n            if hole_match and callout_type == 'Hole':\n                val = float(hole_match.group(1))\n                entry['diameterInches'] = val\n                entry['isThrough'] = 'THRU' in callout.upper()\n\n        callouts.append(entry)\n\n    return callouts\n\ndef merge_evidence(ocr_callouts: List[Dict], qwen_callouts: List[Dict]) -> List[Dict]:\n    \"\"\"Merge OCR and Qwen callouts, preferring OCR for dimensions but using Qwen for context.\"\"\"\n    merged = []\n    used_qwen = set()\n\n    for ocr in ocr_callouts:\n        merged_entry = ocr.copy()\n        merged_entry['sources'] = ['ocr']\n\n        # Try to find matching Qwen feature for additional context\n        for qi, qwen in enumerate(qwen_callouts):\n            if qi in used_qwen:\n                continue\n            if ocr.get('calloutType') == qwen.get('calloutType'):\n                # Check if dimensions roughly match\n                if ocr.get('thread') and qwen.get('thread'):\n                    if ocr['thread'].get('nominalDiameterMm') == qwen['thread'].get('nominalDiameterMm'):\n                        merged_entry['location'] = qwen.get('location', '')\n                        merged_entry['description'] = qwen.get('description', '')\n                        merged_entry['sources'].append('qwen')\n                        used_qwen.add(qi)\n                        break\n                elif ocr.get('diameterInches') and qwen.get('diameterInches'):\n                    # Compare in inches with 0.01\" tolerance\n                    if abs(ocr['diameterInches'] - qwen['diameterInches']) < 0.01:\n                        merged_entry['location'] = qwen.get('location', '')\n                        merged_entry['description'] = qwen.get('description', '')\n                        merged_entry['sources'].append('qwen')\n                        used_qwen.add(qi)\n                        break\n\n        merged.append(merged_entry)\n\n    # Add Qwen features not matched to OCR (may be features OCR missed)\n    for qi, qwen in enumerate(qwen_callouts):\n        if qi not in used_qwen:\n            qwen['sources'] = ['qwen_only']\n            merged.append(qwen)\n\n    return merged\n\n# Parse both sources\nocr_callouts = parse_ocr_callouts(ocr_lines)\nqwen_callouts = parse_qwen_features(qwen_understanding)\n\n# Merge\nmerged_callouts = merge_evidence(ocr_callouts, qwen_callouts)\n\n# Build enriched evidence\nevidence = {\n    'schemaVersion': '1.3.0',  # Updated: now uses inches\n    'partNumber': part_identity.partNumber,\n    'extractedAt': datetime.now().isoformat() + 'Z',\n    'units': 'inches',  # Explicit unit declaration\n    'sources': {\n        'ocr': {'model': 'LightOnOCR-2-1B', 'lineCount': len(ocr_lines)},\n        'vision': {'model': 'Qwen2.5-VL-7B', 'featureCount': len(qwen_callouts)}\n    },\n    'drawingInfo': {\n        'views': qwen_understanding.get('views', []),\n        'partDescription': qwen_understanding.get('partDescription', ''),\n        'material': qwen_understanding.get('material', ''),\n        'titleBlock': qwen_understanding.get('titleBlockInfo', {}),\n        'notes': qwen_understanding.get('notes', [])\n    },\n    'foundCallouts': merged_callouts,\n    'rawOcrSample': ocr_lines[:15]\n}\n\nprint(\"=\"*50)\nprint(\"MERGED DRAWING EVIDENCE (inches)\")\nprint(\"=\"*50)\nprint(f\"OCR callouts:   {len(ocr_callouts)}\")\nprint(f\"Qwen features:  {len(qwen_callouts)}\")\nprint(f\"Merged total:   {len(merged_callouts)}\")\nprint(f\"\\nDrawing info:\")\nprint(f\"  Views: {evidence['drawingInfo']['views']}\")\nprint(f\"  Material: {evidence['drawingInfo']['material']}\")\nprint(f\"\\nMerged callouts:\")\nfor c in merged_callouts[:10]:\n    sources = '+'.join(c.get('sources', []))\n    extra = f\" [{sources}]\"\n    if c.get('location'):\n        extra += f\" @ {c['location']}\"\n    print(f\"  {c['calloutType']}: {c.get('raw', c.get('description', ''))}{extra}\")\n\n# Save\nevidence_out = os.path.join(OUTPUT_DIR, \"DrawingEvidence.json\")\nwith open(evidence_out, 'w') as f:\n    json.dump(evidence, f, indent=2)\nprint(f\"\\nSaved: {evidence_out}\")",
   "metadata": {
    "id": "3ZlPb5FJQBHV",
    "outputId": "75fde05a-ab49-4f51-b9fa-8fd95e43541b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 12: Generate DiffResult (comparison in INCHES)\n\ndef extract_sw_requirements(sw_data: Dict) -> List[Dict]:\n    \"\"\"Extract requirements from SolidWorks JSON using comparison.holeGroups.\n    Returns hole diameters in INCHES for direct comparison with drawing callouts.\"\"\"\n    requirements = []\n\n    # Primary source: comparison.holeGroups (reconciled/canonical data)\n    comparison = sw_data.get('comparison', {})\n    hole_groups = comparison.get('holeGroups', [])\n\n    for hg in hole_groups:\n        hole_type = hg.get('holeType', '')\n        canonical = hg.get('canonical', '')\n        count = hg.get('count', 1)\n        diameters = hg.get('diameters', {})\n        thread = hg.get('thread', {})\n\n        if hole_type == 'Tapped':\n            # Tapped hole - thread info stays in metric (M6x1.0)\n            requirements.append({\n                'type': 'TappedHole',\n                'thread': {\n                    'standard': thread.get('standard', 'Metric'),\n                    'nominalDiameterMm': thread.get('majorDiameterMm') or diameters.get('threadNominalDiameterMm'),\n                    'pitch': thread.get('pitch'),\n                    'callout': thread.get('callout', canonical)\n                },\n                'count': count,\n                'canonical': canonical,\n                'source': 'sw_comparison.holeGroups'\n            })\n        elif hole_type == 'Through':\n            # Plain through hole - use INCHES for comparison\n            diameter_inches = diameters.get('pilotOrTapDrillDiameterInches')\n            requirements.append({\n                'type': 'Hole',\n                'diameterInches': diameter_inches,\n                'isThrough': True,\n                'count': count,\n                'canonical': canonical,\n                'canonicalInches': diameters.get('nearestStandardInch', ''),\n                'source': 'sw_comparison.holeGroups'\n            })\n        elif hole_type == 'Blind':\n            # Blind hole - use INCHES\n            diameter_inches = diameters.get('pilotOrTapDrillDiameterInches')\n            requirements.append({\n                'type': 'Hole',\n                'diameterInches': diameter_inches,\n                'isThrough': False,\n                'count': count,\n                'canonical': canonical,\n                'source': 'sw_comparison.holeGroups'\n            })\n\n    # Fallback: features.holeWizardHoles if no comparison data\n    if not requirements:\n        features = sw_data.get('features', {})\n        for hole in features.get('holeWizardHoles', []):\n            if hole.get('isTapped'):\n                thread_size = hole.get('threadSize', '')\n                m = re.match(r'M(\\d+(?:\\.\\d+)?)[xX](\\d+(?:\\.\\d+)?)', thread_size)\n                if m:\n                    requirements.append({\n                        'type': 'TappedHole',\n                        'thread': {\n                            'standard': 'Metric',\n                            'nominalDiameterMm': float(m.group(1)),\n                            'pitch': float(m.group(2)),\n                            'callout': thread_size\n                        },\n                        'count': hole.get('instanceCount', 1),\n                        'source': 'sw_features.holeWizardHoles'\n                    })\n            else:\n                # Convert meters to inches\n                diameter_m = hole.get('diameter', 0)\n                diameter_inches = diameter_m * 39.3701\n                requirements.append({\n                    'type': 'Hole',\n                    'diameterInches': diameter_inches,\n                    'isThrough': hole.get('isThrough', False),\n                    'count': hole.get('instanceCount', 1),\n                    'source': 'sw_features.holeWizardHoles'\n                })\n\n        # Fillets - convert to inches\n        for fillet in features.get('fillets', []):\n            radius_mm = fillet.get('radius', 0)\n            requirements.append({\n                'type': 'Fillet',\n                'radiusInches': radius_mm / 25.4 if radius_mm else 0,\n                'source': 'sw_features'\n            })\n\n        # Chamfers - convert to inches\n        for chamfer in features.get('chamfers', []):\n            dist_mm = chamfer.get('distance1', 0)\n            requirements.append({\n                'type': 'Chamfer',\n                'distance1Inches': dist_mm / 25.4 if dist_mm else 0,\n                'angleDegrees': chamfer.get('angle', 45),\n                'source': 'sw_features'\n            })\n\n    return requirements\n\ndef compare_callout_to_requirement(callout: Dict, req: Dict, tolerance_inches: float = 0.015) -> bool:\n    \"\"\"Check if a drawing callout matches a SW requirement.\n    Hole comparison done in INCHES with 0.015\\\" tolerance (~0.4mm).\"\"\"\n    ctype = callout.get('calloutType')\n    rtype = req.get('type')\n\n    if ctype != rtype:\n        return False\n\n    if ctype == 'Hole':\n        # Compare in INCHES\n        d1 = callout.get('diameterInches', 0)\n        d2 = req.get('diameterInches', 0)\n        if d1 and d2 and abs(d1 - d2) <= tolerance_inches:\n            return True\n\n    elif ctype == 'TappedHole':\n        # Metric threads - compare in mm\n        t1 = callout.get('thread', {})\n        t2 = req.get('thread', {})\n        nom1 = t1.get('nominalDiameterMm', 0)\n        nom2 = t2.get('nominalDiameterMm', 0)\n        if nom1 and nom2 and abs(nom1 - nom2) < 0.1:\n            p1 = t1.get('pitch')\n            p2 = t2.get('pitch')\n            if p1 and p2:\n                return abs(p1 - p2) < 0.01\n            return True\n\n    elif ctype == 'Fillet':\n        r1 = callout.get('radiusInches', 0)\n        r2 = req.get('radiusInches', 0)\n        if r1 and r2 and abs(r1 - r2) <= tolerance_inches:\n            return True\n\n    elif ctype == 'Chamfer':\n        d1 = callout.get('distance1Inches', 0)\n        d2 = req.get('distance1Inches', 0)\n        if d1 and d2 and abs(d1 - d2) <= tolerance_inches:\n            return True\n\n    return False\n\ndef generate_diff_result(evidence: Dict, sw_data: Dict) -> Dict:\n    \"\"\"Compare drawing evidence against SolidWorks requirements (in inches).\"\"\"\n    callouts = evidence.get('foundCallouts', [])\n    requirements = extract_sw_requirements(sw_data)\n\n    found = []\n    missing = []\n    matched_callouts = set()\n    matched_requirements = set()\n\n    # Check each requirement against callouts\n    for ri, req in enumerate(requirements):\n        match_found = False\n        for ci, callout in enumerate(callouts):\n            if ci not in matched_callouts and compare_callout_to_requirement(callout, req):\n                found.append({\n                    'status': 'FOUND',\n                    'requirement': req,\n                    'evidence': callout,\n                    'note': f\"Matched: {req.get('canonical', req.get('type'))}\"\n                })\n                matched_callouts.add(ci)\n                matched_requirements.add(ri)\n                match_found = True\n                break\n\n        if not match_found:\n            missing.append({\n                'status': 'MISSING',\n                'requirement': req,\n                'evidence': None,\n                'note': f\"Not found in drawing: {req.get('canonical', req.get('type'))}\"\n            })\n\n    # Extra callouts not matched to any requirement\n    extra = []\n    for ci, callout in enumerate(callouts):\n        if ci not in matched_callouts:\n            extra.append({\n                'status': 'EXTRA',\n                'requirement': None,\n                'evidence': callout,\n                'note': f\"In drawing but not in SW: {callout.get('raw', callout.get('calloutType'))}\"\n            })\n\n    diff_result = {\n        'partNumber': evidence.get('partNumber'),\n        'generatedAt': datetime.now().isoformat() + 'Z',\n        'units': 'inches',\n        'summary': {\n            'totalRequirements': len(requirements),\n            'found': len(found),\n            'missing': len(missing),\n            'extra': len(extra),\n            'matchRate': f\"{len(found)/len(requirements)*100:.1f}%\" if requirements else \"N/A\"\n        },\n        'details': {\n            'found': found,\n            'missing': missing,\n            'extra': extra\n        }\n    }\n\n    return diff_result\n\n# Load SW data and generate diff\nif part_identity.swJsonPath:\n    sw_data, err = load_json_robust(part_identity.swJsonPath)\n    if sw_data:\n        # Show what we're extracting\n        requirements = extract_sw_requirements(sw_data)\n        print(\"=\"*50)\n        print(\"SW REQUIREMENTS EXTRACTED (inches)\")\n        print(\"=\"*50)\n        for req in requirements:\n            if req['type'] == 'Hole':\n                print(f\"  {req['type']}: \u00f8{req.get('diameterInches', 0):.4f}\\\" ({req.get('canonical', '')})\")\n            else:\n                print(f\"  {req['type']}: {req.get('canonical', req.get('thread', {}).get('callout', ''))}\")\n\n        diff_result = generate_diff_result(evidence, sw_data)\n\n        print(\"\\n\" + \"=\"*50)\n        print(\"DIFF RESULT\")\n        print(\"=\"*50)\n        print(f\"Part: {diff_result['partNumber']}\")\n        print(f\"  Total Requirements: {diff_result['summary']['totalRequirements']}\")\n        print(f\"  FOUND:   {diff_result['summary']['found']}\")\n        print(f\"  MISSING: {diff_result['summary']['missing']}\")\n        print(f\"  EXTRA:   {diff_result['summary']['extra']}\")\n        print(f\"  Match Rate: {diff_result['summary']['matchRate']}\")\n\n        if diff_result['details']['found']:\n            print(\"\\nMatched:\")\n            for item in diff_result['details']['found']:\n                print(f\"  \u2713 {item['note']}\")\n\n        if diff_result['details']['missing']:\n            print(\"\\nMissing from drawing:\")\n            for item in diff_result['details']['missing']:\n                print(f\"  \u2717 {item['note']}\")\n\n        if diff_result['details']['extra']:\n            print(\"\\nExtra in drawing:\")\n            for item in diff_result['details']['extra']:\n                print(f\"  ? {item['note']}\")\n\n        # Save\n        diff_out = os.path.join(OUTPUT_DIR, \"DiffResult.json\")\n        with open(diff_out, 'w') as f:\n            json.dump(diff_result, f, indent=2)\n        print(f\"\\nSaved: {diff_out}\")\n    else:\n        print(f\"Error loading SW JSON: {err}\")\nelse:\n    print(\"No SW JSON path - cannot generate diff\")",
   "metadata": {
    "id": "eZuYl-gdQBHV",
    "outputId": "a92d7f7d-e8a4-4cc2-ac38-0b0ccb3fa837",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}