{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/tests/notebooks/test_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QUFTLuJZu01"
      },
      "source": [
        "# Test Detection: YOLO-OBB (M1) + OBB Cropping (M2)\n",
        "\n",
        "This notebook tests the first two stages of the AI Inspector pipeline:\n",
        "\n",
        "- **M1 -- YOLODetector**: Runs YOLO11-OBB inference on a drawing page image and\n",
        "  returns oriented bounding boxes around engineering callouts (Holes, TappedHoles,\n",
        "  Fillets, Chamfers, etc.).\n",
        "- **M2 -- OBB Cropper**: Takes each OBB detection and crops it from the source\n",
        "  image, applying padding and minimum-size constraints.\n",
        "\n",
        "**Runtime requirement:** GPU (A100 preferred, T4 acceptable).\n",
        "\n",
        "**Outputs:**\n",
        "- Detection summary (class counts, confidence statistics)\n",
        "- Cropped callout images saved to `debug/crops/`\n",
        "- Visual grid of all crops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1aHFVSqZu02"
      },
      "source": [
        "# Cell 1: Install dependencies\n",
        "# NOTE: Set your runtime to GPU before running (Runtime > Change runtime type > A100)\n",
        "%pip install ultralytics pillow matplotlib --quiet\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git /content/AI-Drawing-Inspector 2>/dev/null || \\\n",
        "    (cd /content/AI-Drawing-Inspector && git pull)\n",
        "\n",
        "print(\"Dependencies installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgmeHGISZu02"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Skip mounting Drive since it is failing and we can use local upload.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Add repo to Python path\n",
        "sys.path.insert(0, '/content/AI-Drawing-Inspector')\n",
        "\n",
        "# ---- Configure these paths ----\n",
        "# Model from HuggingFace (finetuned YOLO11s-OBB)\n",
        "MODEL_PATH = 'hf://shadrack20s/ai-inspector-callout-detection/callout_v2_yolo11s-obb_best.pt'\n",
        "\n",
        "# Output directory (crops will be saved here)\n",
        "OUTPUT_DIR = '/content/debug/crops'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f'Model path:  {MODEL_PATH}')\n",
        "print(f'Output dir:   {OUTPUT_DIR}')\n",
        "print('Google Drive mount skipped. Proceeding with local file upload.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibTeJcSuZu02"
      },
      "source": [
        "# Cell 3: Import ai_inspector detection modules\n",
        "from ai_inspector.detection.yolo_detector import YOLODetector\n",
        "from ai_inspector.detection.classes import IDX_TO_CLASS, YOLO_CLASSES, FUTURE_TYPES\n",
        "from ai_inspector.extractors.cropper import crop_obb, crop_detections\n",
        "from ai_inspector.contracts import DetectionResult, CropResult\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "print(f'YOLO classes ({len(YOLO_CLASSES)}): {YOLO_CLASSES}')\n",
        "print(f'Future types (skipped in matching): {FUTURE_TYPES}')\n",
        "print('Imports OK.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print('Please upload your sample image now.')\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Assuming only one file is uploaded for simplicity\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    # Move the uploaded file to a known location if needed, or use its current path\n",
        "    global SAMPLE_IMAGE\n",
        "    SAMPLE_IMAGE = os.path.join('/content/', uploaded_filename)\n",
        "    print(f'Uploaded file: {SAMPLE_IMAGE}')\n",
        "    print(f'Image exists: {os.path.exists(SAMPLE_IMAGE)}')\n",
        "else:\n",
        "    raise FileNotFoundError(\"No file was uploaded. Please upload a sample image.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GELg8VnbZu03"
      },
      "source": [
        "# Cell 4: Load YOLO model and run detection on sample image\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "# Download model if it's a HuggingFace path\n",
        "if MODEL_PATH.startswith('hf://'):\n",
        "    print(f\"Downloading model from {MODEL_PATH}...\")\n",
        "    # Manual parsing for the known path structure\n",
        "    # MODEL_PATH = 'hf://shadrack20s/ai-inspector-callout-detection/callout_v2_yolo11s-obb_best.pt'\n",
        "    repo_id = \"shadrack20s/ai-inspector-callout-detection\"\n",
        "    filename = \"callout_v2_yolo11s-obb_best.pt\"\n",
        "\n",
        "    local_model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    print(f\"Model downloaded to: {local_model_path}\")\n",
        "else:\n",
        "    local_model_path = MODEL_PATH\n",
        "\n",
        "detector = YOLODetector(\n",
        "    model_path=local_model_path,\n",
        "    confidence_threshold=0.25,\n",
        "    device='cuda',\n",
        ")\n",
        "detector.load()\n",
        "print(f'Model loaded: {detector.is_loaded}')\n",
        "\n",
        "# Load test image\n",
        "image = Image.open(SAMPLE_IMAGE).convert('RGB')\n",
        "print(f'Image size: {image.size}')\n",
        "\n",
        "# Run detection\n",
        "detections = detector.detect(image, page_id='test_page_0')\n",
        "print(f'\\nDetections found: {len(detections)}')\n",
        "if not detections:\n",
        "    print('WARNING: No detections found. Try a lower confidence threshold (e.g., 0.15) or verify model/image path.')\n",
        "\n",
        "# Show first 5 detections\n",
        "for i, det in enumerate(detections[:5]):\n",
        "    print(f'  [{i}] {det.class_name:20s} conf={det.confidence:.3f} id={det.det_id}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hls6cTzlZu03"
      },
      "source": [
        "# Cell 5: Run OBB cropping on detections, save crops to debug/crops/\n",
        "crops = crop_detections(image, detections, pad_ratio=0.15)\n",
        "print(f'Crops generated: {len(crops)}')\n",
        "\n",
        "# Save each crop\n",
        "for i, (det, crop) in enumerate(zip(detections, crops)):\n",
        "    crop_path = f'{OUTPUT_DIR}/{det.det_id}_{det.class_name}.png'\n",
        "    crop.image.save(crop_path)\n",
        "    print(f'  [{i}] {det.class_name:20s} crop_size={crop.image.size}  '\n",
        "          f'angle={crop.meta.get(\"rotation_angle\", 0):.1f}deg  '\n",
        "          f'saved={crop_path}')\n",
        "\n",
        "print(f'\\nAll crops saved to {OUTPUT_DIR}/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCo-g4nhZu03"
      },
      "source": [
        "# Cell 6: Display crop results in a grid\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "n_crops = min(len(crops), 20)  # Show at most 20 crops\n",
        "if n_crops == 0:\n",
        "    print('No crops to display.')\n",
        "else:\n",
        "    cols = 5\n",
        "    rows = math.ceil(n_crops / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
        "    axes = np.array(axes).flatten() if n_crops > 1 else [axes]\n",
        "\n",
        "    for i in range(n_crops):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(crops[i].image)\n",
        "        ax.set_title(\n",
        "            f'{detections[i].class_name}\\n'\n",
        "            f'conf={detections[i].confidence:.2f}',\n",
        "            fontsize=9,\n",
        "        )\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide unused axes\n",
        "    for j in range(n_crops, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.suptitle(f'OBB Crops ({n_crops} of {len(crops)})', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEUJRJM4Zu03"
      },
      "source": [
        "# Cell 7: Print detection summary (class counts, confidence stats)\n",
        "from collections import Counter\n",
        "\n",
        "summary = detector.summary(detections)\n",
        "print('=== Detection Summary ===')\n",
        "print(f'Total detections: {summary[\"total\"]}')\n",
        "print(f'Average confidence: {summary[\"avg_confidence\"]:.3f}')\n",
        "print()\n",
        "\n",
        "# Per-class breakdown\n",
        "print(f'{\"Class\":25s} {\"Count\":>6s} {\"Avg Conf\":>10s} {\"Min Conf\":>10s} {\"Max Conf\":>10s}')\n",
        "print('-' * 65)\n",
        "\n",
        "class_groups = {}\n",
        "for det in detections:\n",
        "    cls = det.class_name\n",
        "    if cls not in class_groups:\n",
        "        class_groups[cls] = []\n",
        "    class_groups[cls].append(det.confidence)\n",
        "\n",
        "for cls in sorted(class_groups.keys()):\n",
        "    confs = class_groups[cls]\n",
        "    print(f'{cls:25s} {len(confs):6d} {sum(confs)/len(confs):10.3f} '\n",
        "          f'{min(confs):10.3f} {max(confs):10.3f}')\n",
        "\n",
        "# Confidence histogram\n",
        "all_confs = [d.confidence for d in detections]\n",
        "if all_confs:\n",
        "    print(f'\\nConfidence distribution:')\n",
        "    for threshold in [0.9, 0.75, 0.5, 0.25]:\n",
        "        count = sum(1 for c in all_confs if c >= threshold)\n",
        "        print(f'  >= {threshold:.2f}: {count}/{len(all_confs)}')\n",
        "\n",
        "# Cleanup\n",
        "detector.unload()\n",
        "print('\\nDetector unloaded.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}