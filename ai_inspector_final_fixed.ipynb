{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFrLGYPswHHZ"
   },
   "source": [
    "# AI Engineering Drawing Inspector (Final Version)\n",
    "\n",
    "A context-aware GD&T checker that uses:\n",
    "- Part context from BOM structure\n",
    "- RAG retrieval from ASME Y14.5 standard\n",
    "- Qwen2-VL for visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTvRp4rwHHa"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmkO97ziwHHa",
    "outputId": "3731f36c-5925-45bb-98e0-36c6a6a7bb32"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 1A: Install Dependencies\n# ============================================================\n!pip install -q transformers accelerate\n!pip install -q qwen-vl-utils\n!pip install -q pdf2image\n!pip install -q faiss-cpu sentence-transformers\n!pip install -q bitsandbytes\n!apt-get install -y poppler-utils > /dev/null 2>&1\n\n# Production Pipeline Dependencies\n!pip install -q pymupdf opencv-python-headless\n\n# Tesseract OCR (replaces PaddleOCR - more stable)\n!sudo apt-get install -y tesseract-ocr > /dev/null 2>&1\n!pip install -q pytesseract\n\nprint(\"‚úÖ All packages installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_VXcblCwHHa",
    "outputId": "2cc92830-8741-41d8-a68e-08f0130fcbf7"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 1B: Import Libraries\n# ============================================================\nimport os\nimport json\nimport re\nimport pickle\nimport torch\nfrom pathlib import Path\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# Production Pipeline Imports\nimport fitz  # PyMuPDF\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Dict, Any\n\n# Tesseract OCR\nimport pytesseract\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmqDweB4wHHa"
   },
   "source": [
    "## 2. Load Model (Qwen2-VL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "2cb373474b194b8da3ef1dc6753df2e3",
      "da6c3848895042bb9985d5febae6250a",
      "f2c44e9a7b7744e682ae9b1959b31764",
      "40681f3c555c4b7c8151e328ebafcbb3",
      "93130fc6f2204866813a448860e0b8e3",
      "47c9fcd759d749e9866e8c5e4a42908b",
      "07ee67ba714d4787b7c3a8650421445f",
      "8ca9810080244d0aafac09c78ab027ea",
      "97f20662d5754c30818216f746845a52",
      "0d3c5bc3a43f40f595a4ecfc1da171a5",
      "4998675f0ff244c8a4309534a2fe7e09"
     ]
    },
    "id": "8fokt7vLwHHa",
    "outputId": "6d24f93f-89c6-4f8f-ecb4-6e15ccd20e62"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Load Qwen2-VL Model (4-bit Quantized)\n# ============================================================\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n\nMODEL_ID = \"Qwen/Qwen2-VL-72B-Instruct\"\n\nprint(f\"Loading {MODEL_ID} in 4-bit (NF4)...\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    attn_implementation=\"sdpa\",\n    trust_remote_code=True\n)\n\nprocessor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n\nprint(\"‚úÖ Qwen2-VL-72B (4-bit) Loaded Successfully!\")\nprint(f\"Memory Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkOM8aOwHHa"
   },
   "source": [
    "## 3. Load Context Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK5ZAuphwHHa",
    "outputId": "c5530898-6106-4299-dfe9-2dc717d3cb41"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3A: Upload Configuration Files\n# ============================================================\nimport os\nimport zipfile\nfrom google.colab import files\n\nMAPPING_FILE = \"400S_file_part_mapping.json\"\nSTRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\nRAG_INDEX_FILE = \"asme_visual_index.pkl\"\n\nprint(\"=\"*60)\nprint(\"STEP 1: Upload Configuration Files\")\nprint(\"=\"*60)\n\ndef locate_file(filename):\n    if os.path.exists(filename):\n        return os.path.abspath(filename)\n    nested_path = os.path.join(\"rag_data\", filename)\n    if os.path.exists(nested_path):\n        return os.path.abspath(nested_path)\n    return None\n\nFILE_MAPPING_PATH = locate_file(MAPPING_FILE)\nSTRUCTURE_PATH = locate_file(STRUCTURE_FILE)\nRAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n\nmissing_files = []\nif not FILE_MAPPING_PATH:\n    missing_files.append(MAPPING_FILE)\nif not STRUCTURE_PATH:\n    missing_files.append(STRUCTURE_FILE)\nif not RAG_INDEX_PATH:\n    missing_files.append(RAG_INDEX_FILE)\n\nif missing_files:\n    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n    uploaded = files.upload()\n\n    for filename in uploaded:\n        if filename.lower().endswith('.zip'):\n            print(f\"\\nExtracting {filename}...\")\n            with zipfile.ZipFile(filename, 'r') as zip_ref:\n                zip_ref.extractall(\"rag_data\")\n            print(\"Extraction complete.\")\n            break\n\n    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n\nif FILE_MAPPING_PATH:\n    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\nelse:\n    DATA_DIR = \"/content\"\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILE STATUS:\")\nprint(\"=\"*60)\nprint(f\"File Mapping:  {'‚úÖ OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else '‚ùå MISSING'}\")\nprint(f\"Structure:     {'‚úÖ OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else '‚ùå MISSING'}\")\nprint(f\"RAG Index:     {'‚úÖ OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else '‚ö†Ô∏è MISSING'}\")\nprint(f\"\\nData directory: {DATA_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykrHElKlwHHb",
    "outputId": "7504d772-a2b4-4a52-fab3-839da9f64bf5"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3B: Load Part Context Databases\n# ============================================================\n\ndef normalize_pn(pn):\n    \"\"\"Normalize part number for lookup.\"\"\"\n    return re.sub(r'[-\\s]', '', str(pn)).lower()\n\ndef load_context_databases():\n    \"\"\"Load and build all context databases.\"\"\"\n    print(\"Loading file mapping...\")\n    with open(FILE_MAPPING_PATH, 'r') as f:\n        file_mapping_list = json.load(f)\n\n    filename_to_pn = {}\n    for entry in file_mapping_list:\n        filename = entry['file']\n        pn = entry['pn']\n        if pn:\n            filename_to_pn[filename] = pn\n            filename_to_pn[filename + '.pdf'] = pn\n            filename_to_pn[filename + '.PDF'] = pn\n\n    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n\n    print(\"Loading part structure...\")\n    with open(STRUCTURE_PATH, 'r') as f:\n        structure_data = json.load(f)\n\n    print(\"Building part context database...\")\n    part_context_db = {}\n\n    for assembly_name, parts_list in structure_data.items():\n        for part in parts_list:\n            pn = part['pn']\n            desc = part['desc']\n\n            siblings_list = []\n            siblings_pns = []\n\n            for p_sibling in parts_list:\n                if p_sibling['pn'] != pn:\n                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n                    siblings_pns.append(p_sibling['pn'])\n\n            siblings_str = \"; \".join(siblings_list[:12])\n            if len(siblings_list) > 12:\n                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n\n            lookup_key = normalize_pn(pn)\n\n            part_context_db[lookup_key] = {\n                'pn': pn,\n                'description': desc,\n                'assembly': assembly_name,\n                'siblings': siblings_str,\n                'siblings_list': siblings_pns\n            }\n            part_context_db[pn] = part_context_db[lookup_key]\n\n    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n    return filename_to_pn, part_context_db\n\nfilename_to_pn, part_context_db = load_context_databases()\nprint(\"\\n‚úÖ Context databases loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fecfcc5a",
    "outputId": "e7da488e-fa00-4f26-c64f-434887def839"
   },
   "source": "# ============================================================\n# CELL 3C-PREP: Verify Tesseract Installation\n# ============================================================\nimport shutil\n\n# Check if Tesseract is installed\ntesseract_path = shutil.which(\"tesseract\")\nif tesseract_path:\n    print(f\"‚úÖ Tesseract found: {tesseract_path}\")\nelse:\n    print(\"‚ö†Ô∏è Tesseract not found. Installing...\")\n    !sudo apt-get install -y tesseract-ocr\n    print(\"‚úÖ Tesseract installed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d605a04",
    "outputId": "8a3ae866-14bc-4842-fa30-3c1c1de21f1f"
   },
   "source": "# ============================================================\n# CELL 3C: Initialize Tesseract OCR\n# ============================================================\nimport pytesseract\nfrom PIL import Image\nimport re\n\nprint(\"Loading Tesseract OCR Engine...\")\n\ndef get_drawing_text_ocr(image_input):\n    \"\"\"\n    Runs Tesseract OCR on the drawing and returns a clean list of found text.\n    Uses PSM 11 (Sparse Text) mode which is best for engineering drawings.\n    \n    Args:\n        image_input: PIL Image or numpy array\n        \n    Returns:\n        List of unique text strings found\n    \"\"\"\n    try:\n        # Convert numpy array to PIL Image if needed\n        if isinstance(image_input, np.ndarray):\n            img = Image.fromarray(image_input)\n        else:\n            img = image_input\n        \n        # Run Tesseract with sparse text mode (PSM 11)\n        # PSM 11: Sparse text - Find as much text as possible in no particular order\n        raw_text = pytesseract.image_to_string(img, config='--psm 11')\n        \n        # Split by newlines and clean\n        lines = raw_text.split('\\n')\n        \n        text_set = set()\n        for line in lines:\n            # Strip whitespace\n            cleaned = line.strip()\n            \n            # Skip empty lines\n            if not cleaned:\n                continue\n            \n            # Skip garbage (less than 2 alphanumeric characters)\n            alphanumeric_count = sum(1 for c in cleaned if c.isalnum())\n            if alphanumeric_count < 2:\n                continue\n            \n            # Normalize engineering symbols\n            cleaned = cleaned.replace(\"√ò\", \"DIA \")\n            cleaned = cleaned.replace(\"√∏\", \"DIA \")\n            \n            text_set.add(cleaned)\n        \n        return sorted(list(text_set))\n    \n    except Exception as e:\n        print(f\"‚ö†Ô∏è OCR Warning: {e}\")\n        return []\n\nprint(\"‚úÖ Tesseract OCR Engine Ready!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "AsY8eejBwHHb",
    "outputId": "8d9e785d-d9b1-4f59-dbeb-23446b1a41b7"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3E: Load RAG Index & Visual Database\n# ============================================================\nimport os\nimport pickle\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nrag_data = []\nrag_embeddings = None\nrag_available = False\nRAG_IMAGE_DIR = None\n\nprint(\"=\"*60)\nprint(\"RAG SYSTEM SETUP\")\nprint(\"=\"*60)\n\nprint(\"\\n[STEP 1/3] Loading CLIP model...\")\nsearch_model = SentenceTransformer('clip-ViT-B-32')\nprint(\"  ‚úÖ CLIP model loaded!\")\n\nprint(\"\\n[STEP 2/3] Loading RAG Index...\")\nindex_loaded = False\n\n# Check multiple locations for the index file\nindex_locations = [\n    \"/content/asme_visual_index.pkl\",\n    \"/content/rag_data/asme_visual_index.pkl\",\n    \"asme_visual_index.pkl\",\n]\nif 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH:\n    index_locations.insert(0, RAG_INDEX_PATH)\n\nfor idx_path in index_locations:\n    if idx_path and os.path.exists(idx_path):\n        print(f\"  ‚úÖ Found: {idx_path}\")\n        with open(idx_path, 'rb') as f:\n            rag_data = pickle.load(f)\n        RAG_INDEX_PATH = idx_path\n        index_loaded = True\n        break\n\nif not index_loaded:\n    print(\"  ‚ùå No index found. Please upload asme_visual_index.pkl:\")\n    from google.colab import files\n    try:\n        uploaded = files.upload()\n        for filename in uploaded:\n            if filename.endswith('.pkl'):\n                with open(filename, 'rb') as f:\n                    rag_data = pickle.load(f)\n                index_loaded = True\n                break\n    except:\n        pass\n\nprint(\"\\n[STEP 3/3] Looking for RAG Visual Database...\")\n\n# Check multiple locations for the image folder\nimage_locations = [\n    \"/content/rag_visual_db\",\n    \"/content/rag_data/rag_visual_db\",\n    \"rag_visual_db\",\n]\nif 'DATA_DIR' in dir() and DATA_DIR:\n    image_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n\nfound_images = False\nfor loc in image_locations:\n    if loc and os.path.exists(loc) and os.path.isdir(loc):\n        # Count images\n        img_files = [f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        if len(img_files) > 0:\n            RAG_IMAGE_DIR = os.path.abspath(loc)\n            found_images = True\n            print(f\"  ‚úÖ Found: {RAG_IMAGE_DIR} ({len(img_files)} images)\")\n            break\n\nif not found_images:\n    print(\"  ‚ùå No images found. Please upload rag_visual_db.zip:\")\n    from google.colab import files\n    import zipfile, shutil\n    try:\n        uploaded = files.upload()\n        for filename in uploaded:\n            if filename.lower().endswith('.zip'):\n                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n                if os.path.exists(RAG_IMAGE_DIR):\n                    shutil.rmtree(RAG_IMAGE_DIR)\n                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n                with zipfile.ZipFile(filename, 'r') as zf:\n                    zf.extractall(RAG_IMAGE_DIR)\n                found_images = True\n                print(f\"  ‚úÖ Extracted to {RAG_IMAGE_DIR}\")\n                break\n    except:\n        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n\n# Build search index\nprint(\"\\n\" + \"=\"*60)\nif index_loaded and len(rag_data) > 0:\n    embeddings_list = [item['embedding'] for item in rag_data]\n    rag_embeddings = np.array(embeddings_list).astype('float32')\n    rag_available = True\n    print(\"‚úÖ RAG SYSTEM: READY\")\n    print(f\"  Index: {len(rag_data)} entries\")\n    print(f\"  Images: {RAG_IMAGE_DIR}\")\nelse:\n    print(\"‚ùå RAG SYSTEM: NOT READY\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD4nCWywHHb"
   },
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4Y7DliIwHHb",
    "outputId": "78791372-51b2-467d-fb01-ca749c867593"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4A: Core Helper Functions\n# ============================================================\nimport os\nimport re\nfrom pdf2image import convert_from_path\n\ndef extract_filename_key(filepath):\n    \"\"\"Extract filename key for lookup.\"\"\"\n    filename = os.path.basename(filepath)\n    name_no_ext = os.path.splitext(filename)[0]\n    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)  # Remove (1), (2) etc\n    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n    return name_cleaned.strip()\n\ndef get_part_context(filepath):\n    \"\"\"Look up part context from filename.\"\"\"\n    filename_key = extract_filename_key(filepath)\n\n    if filename_key in filename_to_pn:\n        pn = filename_to_pn[filename_key]\n        lookup_key = normalize_pn(pn)\n        if lookup_key in part_context_db:\n            return pn, part_context_db[lookup_key]\n\n    for ext in ['.pdf', '.PDF']:\n        key = filename_key + ext\n        if key in filename_to_pn:\n            pn = filename_to_pn[key]\n            lookup_key = normalize_pn(pn)\n            if lookup_key in part_context_db:\n                return pn, part_context_db[lookup_key]\n\n    return None, None\n\ndef build_context_string(pn, context):\n    \"\"\"Build the context string for inspection prompt.\"\"\"\n    if context is None:\n        return \"CONTEXT: Unknown Part (General Syntax Check Only).\"\n\n    desc = context.get('description', 'Unknown')\n    assembly = context.get('assembly', 'Unknown Assembly')\n    siblings = context.get('siblings', 'None listed')\n\n    return f\"\"\"CONTEXT: This is Part {pn} ({desc}).\nIt belongs to the {assembly}.\nIt must assemble with these mating parts: {siblings}.\nCRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n\ndef pdf_to_image(pdf_path, dpi=150):\n    \"\"\"Convert first page of PDF to PIL Image.\"\"\"\n    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n    return pages[0] if pages else None\n\nprint(\"‚úÖ Core helper functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3aLheGBwHHb",
    "outputId": "d8a213ee-421f-4122-ec0d-79dc267a9bfe"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4B: Model Query Function\n# ============================================================\nimport torch\nfrom qwen_vl_utils import process_vision_info\n\ndef query_model(messages, max_tokens=1024):\n    \"\"\"Send a query to Qwen2-VL and get response.\"\"\"\n    if 'model' not in globals() or 'processor' not in globals():\n        raise RuntimeError(\"‚ö†Ô∏è Model not loaded. Run Cell 2 first.\")\n\n    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n\n    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n    response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    return response.strip()\n\nprint(\"‚úÖ Model query function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gETA6XD5wHHb",
    "outputId": "09461b92-e283-4f86-8a6a-40452499cefb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4C: RAG Retrieval Function\n# ============================================================\n\ndef retrieve_asme_pages(keywords, top_k=2):\n    \"\"\"Retrieve relevant ASME standard pages based on keywords.\"\"\"\n    global RAG_IMAGE_DIR\n\n    if not rag_available or rag_embeddings is None:\n        print(\"  ‚ö†Ô∏è RAG system not available\")\n        return []\n\n    if RAG_IMAGE_DIR is None:\n        print(\"  ‚ö†Ô∏è RAG_IMAGE_DIR not set\")\n        return []\n\n    try:\n        query_vector = search_model.encode([keywords])\n        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n        top_indices = np.argsort(scores)[-top_k:][::-1]\n\n        retrieved_images = []\n        print(f\"  RAG Search: '{keywords[:50]}...'\")\n\n        for idx in top_indices:\n            item = rag_data[idx]\n            rel_path = item['path'].replace('\\\\', '/')\n\n            paths_to_try = [\n                os.path.join(RAG_IMAGE_DIR, rel_path),\n                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)),\n            ]\n\n            path_parts = rel_path.split('/')\n            if len(path_parts) > 1:\n                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n\n            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n\n            for try_path in paths_to_try:\n                if os.path.exists(try_path):\n                    try:\n                        img = Image.open(try_path).convert('RGB')\n                        retrieved_images.append(img)\n                        break\n                    except Exception as e:\n                        print(f\"      Error: {e}\")\n\n        return retrieved_images\n\n    except Exception as e:\n        print(f\"  RAG error: {e}\")\n        return []\n\nprint(\"‚úÖ RAG retrieval function defined.\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 4D: Production Pipeline Helpers (Tesseract OCR + Tiling)\n# ============================================================\nimport fitz  # PyMuPDF\nfrom PIL import Image\nimport numpy as np\nfrom typing import List, Tuple\nimport pytesseract\n\nprint(\"‚öôÔ∏è Initializing Production Pipeline...\")\n\ndef render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n    \"\"\"Renders the first page of a PDF to a High-Res PIL Image using PyMuPDF.\"\"\"\n    try:\n        doc = fitz.open(pdf_path)\n        page = doc.load_page(0)\n        zoom = dpi / 72.0\n        mat = fitz.Matrix(zoom, zoom)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        doc.close()\n        return img\n    except Exception as e:\n        print(f\"‚ùå Rendering Error: {e}\")\n        return None\n\ndef run_tesseract_ocr(img: Image.Image) -> List[str]:\n    \"\"\"\n    Runs Tesseract OCR on the image and returns a sorted, unique list of text found.\n    Uses PSM 11 (Sparse Text) mode which is best for engineering drawings.\n    Normalizes common engineering symbols (√ò -> DIA).\n    \n    Args:\n        img: PIL Image\n        \n    Returns:\n        List of unique text strings found, sorted alphabetically\n    \"\"\"\n    try:\n        # Run Tesseract with sparse text mode (PSM 11)\n        raw_text = pytesseract.image_to_string(img, config='--psm 11')\n        \n        # Split by newlines and clean\n        lines = raw_text.split('\\n')\n        \n        texts = []\n        for line in lines:\n            # Strip whitespace\n            cleaned = line.strip()\n            \n            # Skip empty lines\n            if not cleaned:\n                continue\n            \n            # Skip garbage (less than 2 alphanumeric characters)\n            alphanumeric_count = sum(1 for c in cleaned if c.isalnum())\n            if alphanumeric_count < 2:\n                continue\n            \n            # Normalize engineering symbols\n            cleaned = cleaned.replace(\"√ò\", \"DIA \")\n            cleaned = cleaned.replace(\"√∏\", \"DIA \")\n            \n            texts.append(cleaned)\n        \n        # Deduplicate and sort\n        return sorted(list(set(texts)))\n    \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Tesseract OCR Error: {e}\")\n        return []\n\ndef make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n    \"\"\"Splits the image into 4 overlapping quadrants for better resolution.\"\"\"\n    w, h = full_img.size\n    tile_w, tile_h = w // 2, h // 2\n    overlap = int(min(w, h) * 0.15)\n\n    boxes = {\n        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n    }\n\n    tiles = []\n    for name, box in boxes.items():\n        tiles.append((name, full_img.crop(box)))\n    return tiles\n\nprint(\"‚úÖ Production Pipeline Helpers Loaded (Tesseract OCR).\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJGxTszX-2XY",
    "outputId": "bd619b37-a123-45d7-86bc-05f66cad0437"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmd8w7bcwHHb"
   },
   "source": [
    "## 5. Main Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BcVNR3hwHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 5A: Main Inspection Function (RAG + OCR Hybrid)\n# ============================================================\nimport numpy as np\nimport os\n\ndef inspect_drawing_rag(drawing_path, verbose=True):\n    \"\"\"Main inspection function using Vision + OCR + RAG.\"\"\"\n\n    if verbose:\n        print(f\"\\n{'='*60}\")\n        print(f\"INSPECTING: {os.path.basename(drawing_path)}\")\n        print('='*60)\n\n    # Phase 0: Identify Part\n    if verbose:\n        print(\"\\n[1/5] Identifying part...\")\n\n    pn, context = get_part_context(drawing_path)\n\n    if not context:\n        if verbose:\n            print(f\"  ‚ùå Could not find context for '{drawing_path}'\")\n        return {'result': 'FAIL', 'part_number': None, 'description': None, 'details': 'Identity Unknown'}\n\n    context_str = build_context_string(pn, context)\n\n    if pn and verbose:\n        print(f\"  Part: {pn}\")\n        print(f\"  Description: {context.get('description', 'N/A')}\")\n        print(f\"  Assembly: {context.get('assembly', 'N/A')}\")\n\n    # Phase 1: Load Image + OCR\n    if verbose:\n        print(\"\\n[2/5] Loading drawing & OCR scan...\")\n\n    try:\n        drawing_image = pdf_to_image(drawing_path)\n        if drawing_image is None:\n            return {'result': 'ERROR', 'part_number': pn, 'details': 'Failed to load PDF'}\n        if verbose:\n            print(f\"  Drawing loaded: {drawing_image.size}\")\n    except Exception as e:\n        return {'result': 'ERROR', 'part_number': pn, 'details': f'PDF Error: {e}'}\n\n    ocr_text_list = []\n    ocr_text_block = \"\"\n    try:\n        ocr_input = np.array(drawing_image)\n        ocr_text_list = get_drawing_text_ocr(ocr_input)\n        ocr_text_block = \"\\n\".join(ocr_text_list)\n        if verbose:\n            print(f\"  OCR Found {len(ocr_text_list)} elements: {ocr_text_list[:5]}...\")\n    except Exception as e:\n        if verbose:\n            print(f\"  OCR Warning: {e}\")\n\n    # Phase 2: Vision + OCR Extraction\n    if verbose:\n        print(\"\\n[3/5] CoT Step 1: Extraction...\")\n\n    if ocr_text_block:\n        extraction_prompt = f\"\"\"You are an Expert Engineering Drawing Scanner.\n\nOCR Data found:\n--- OCR DATA ---\n{ocr_text_block}\n--- END ---\n\nExtract: 1. Thread Callouts 2. Bore/Hole Dimensions 3. Material Note 4. GD&T Symbols\nTrust the OCR data. Output a clean list.\"\"\"\n    else:\n        extraction_prompt = \"\"\"Scan this drawing and extract:\n1. Thread Callouts (e.g., '1/4-20 UNC')\n2. Bore/Hole Dimensions with tolerances\n3. Material Note\n4. GD&T Symbols\nList them exactly as written.\"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": drawing_image}, {\"type\": \"text\", \"text\": extraction_prompt}]}]\n    extraction_text = query_model(messages, max_tokens=512)\n    messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": extraction_text}]})\n\n    if verbose:\n        print(f\"  Extracted:\\n{extraction_text[:300]}...\")\n\n    # Phase 3: RAG Retrieval\n    if verbose:\n        print(\"\\n[4/5] Retrieving ASME references...\")\n\n    asme_images = []\n    if rag_available:\n        asme_images = retrieve_asme_pages(extraction_text, top_k=2)\n    if verbose:\n        print(f\"  ASME pages: {len(asme_images)}\")\n\n    # Phase 4: Audit\n    if verbose:\n        print(\"\\n[5/5] CoT Step 2: Audit...\")\n\n    mating_parts_str = context.get('siblings', 'None') if context else 'None'\n\n    audit_prompt = f\"\"\"You are a Strict Logic Comparator.\n\nREQUIREMENTS: {context_str}\nACTUALS: {extraction_text}\n\nRULES:\n- Verify dimensions match mating parts\n- If Mating Part is '3/4-16' and ACTUALS shows 'M10' -> FAIL\n- Missing features -> CANNOT VERIFY\n\nOUTPUT: Line 1: PASS or FAIL\nThen: Tier 1 (General), Tier 2 (GD&T), Tier 3 (Assembly Fit), Recommendations\"\"\"\n\n    content_2 = [{\"type\": \"image\", \"image\": img} for img in asme_images]\n    content_2.append({\"type\": \"text\", \"text\": audit_prompt})\n    messages.append({\"role\": \"user\", \"content\": content_2})\n\n    audit_response = query_model(messages, max_tokens=1500)\n\n    # Parse result\n    first_line = audit_response.split('\\n')[0].upper()\n    if 'PASS' in first_line and 'FAIL' not in first_line:\n        result = 'PASS'\n    elif 'FAIL' in first_line:\n        result = 'FAIL'\n    else:\n        result = 'REVIEW'\n\n    if verbose:\n        print(f\"\\n{'='*60}\\nRESULT: {result}\\n{'='*60}\")\n        print(audit_response)\n\n    return {\n        'result': result, 'part_number': pn,\n        'description': context.get('description') if context else None,\n        'assembly': context.get('assembly') if context else None,\n        'mating_parts': mating_parts_str,\n        'ocr_text_count': len(ocr_text_list),\n        'asme_pages_used': len(asme_images),\n        'details': audit_response\n    }\n\nprint(\"‚úÖ inspect_drawing_rag() defined.\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5B: Production Inspection (Strict Mismatch Mode - Optimized)\n# ============================================================\n\ndef inspect_drawing_production(pdf_path, context_str=None, use_tiles=False):\n    \"\"\"\n    Production inspection using Tesseract OCR + Optional Tiling.\n    Uses STRICT MISMATCH MODE - aggressively finds contradictions.\n    \n    Args:\n        pdf_path: Path to PDF file\n        context_str: Optional mating parts context (auto-fetched if None)\n        use_tiles: If True, includes 4 tiles (uses more memory). Default False.\n    \"\"\"\n    print(f\"\\n{'='*60}\\nINSPECTING (Strict Mismatch Mode): {pdf_path}\\n{'='*60}\")\n\n    # --- Auto-Context Logic ---\n    pn = None\n    ctx = None\n    if not context_str:\n        pn, ctx = get_part_context(pdf_path)\n        if ctx:\n            context_str = ctx.get('siblings', 'No mating parts listed')\n            print(f\"  Part: {pn} ({ctx.get('description', 'N/A')})\")\n            print(f\"  Assembly: {ctx.get('assembly', 'N/A')}\")\n        else:\n            context_str = \"Unknown part - no mating context available\"\n            print(\"  ‚ö†Ô∏è Part not found in database\")\n    \n    # --- Phase 1: Render High-Res Image ---\n    print(\"\\n[1/4] Rendering High-Res Image...\")\n    full_img = render_pdf_page(pdf_path, dpi=200)  # Reduced from 300 to 200 DPI\n    if not full_img:\n        return {\"error\": \"FAIL: Rendering Failed\"}\n    print(f\"  Size: {full_img.size}\")\n\n    # --- Phase 2: Tesseract OCR ---\n    print(\"\\n[2/4] Extracting Tesseract OCR Evidence...\")\n    ocr_texts = run_tesseract_ocr(full_img)\n    # LIMIT to 30 elements to reduce token count\n    ocr_limited = ocr_texts[:30]\n    ocr_block = \"\\n\".join([f\"- {t}\" for t in ocr_limited])\n    print(f\"  OCR Found {len(ocr_texts)} elements (using top 30)\")\n    if ocr_texts[:5]:\n        print(f\"  Preview: {ocr_texts[:5]}\")\n\n    # --- Phase 3: Optional Tiles ---\n    tiles = []\n    if use_tiles:\n        print(\"\\n[3/4] Generating Tiles...\")\n        tiles = make_overlapping_tiles(full_img)\n        print(f\"  Created {len(tiles)} tiles\")\n    else:\n        print(\"\\n[3/4] Skipping tiles (memory optimization)\")\n\n    # --- Phase 4: STRICT MISMATCH INFERENCE ---\n    print(\"\\n[4/4] Running STRICT MISMATCH Analysis...\")\n    \n    # Compact system prompt\n    system_prompt = \"\"\"You are a STRICT MISMATCH DETECTOR. Find FAULTS, not compatibility.\n\nRULES:\n1. Imperial vs Metric = AUTOMATIC FAIL (e.g., 3/4-16 UNC vs M10x1.5)\n2. Missing feature = FAIL\n3. No assumptions - specs must match EXACTLY\n4. Different thread pitch = FAIL\"\"\"\n\n    # Compact user prompt\n    user_text = f\"\"\"REQUIREMENTS (mating parts):\n{context_str[:500]}\n\nOCR EVIDENCE (from drawing):\n{ocr_block}\n\nTASK: Check if drawing specs match mating part requirements.\n\nOUTPUT FORMAT:\n**[Part Name]**\n- Requirement: [needed spec]\n- Drawing Shows: [OCR finding or NOT FOUND]\n- Status: PASS/FAIL\n- Reason: [brief]\n\nFINAL VERDICT: PASS or FAIL\nISSUES: [list conflicts]\"\"\"\n\n    # Build payload - only full image (no tiles by default)\n    content = [\n        {'type': 'image', 'image': full_img}, \n        {'type': 'text', 'text': user_text}\n    ]\n    \n    # Add tiles only if requested\n    if use_tiles and tiles:\n        for name, tile in tiles:\n            content.insert(-1, {'type': 'image', 'image': tile})\n            content.insert(-1, {'type': 'text', 'text': f\"TILE: {name}\"})\n\n    messages = [\n        {'role': 'system', 'content': system_prompt}, \n        {'role': 'user', 'content': content}\n    ]\n\n    # Inference\n    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \n    if use_tiles and tiles:\n        image_inputs = [full_img] + [t[1] for t in tiles]\n    else:\n        image_inputs = [full_img]\n\n    inputs = processor(\n        text=[text_input], \n        images=image_inputs, \n        return_tensors=\"pt\", \n        padding=True\n    ).to(model.device)\n    \n    # Check token count\n    token_count = inputs.input_ids.shape[1]\n    print(f\"  Token count: {token_count}\")\n    if token_count > 30000:\n        print(f\"  ‚ö†Ô∏è Warning: High token count, may cause issues\")\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=800)\n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n    \n    print(f\"\\n{'='*60}\\nSTRICT MISMATCH RESULT:\\n{'='*60}\\n{response}\")\n    \n    return {\n        'response': response,\n        'part_number': pn,\n        'ocr_count': len(ocr_texts),\n        'token_count': token_count\n    }\n\nprint(\"‚úÖ inspect_drawing_production() defined (Optimized for memory).\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peirUbTi-2XY",
    "outputId": "d1159ea1-eef7-4ab7-9d6d-b5205e36ae5b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc7S5QTzwHHb"
   },
   "source": [
    "## 6. Batch Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-tlYl9GwHHb",
    "outputId": "4dbabcb7-94d7-4361-9f09-8e9640283cc4"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 6: Batch Inspection Function\n# ============================================================\nimport glob\nimport json\n\ndef inspect_batch(drawing_folder, output_file=\"inspection_results.json\", limit=None):\n    \"\"\"Inspect all PDFs in a folder.\"\"\"\n    from tqdm.notebook import tqdm\n\n    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n    pdf_files = list(set(pdf_files))\n\n    if limit:\n        pdf_files = pdf_files[:limit]\n\n    print(f\"Found {len(pdf_files)} PDF files\")\n\n    results = []\n    pass_count = fail_count = error_count = 0\n\n    for pdf_path in tqdm(pdf_files, desc=\"Inspecting\"):\n        try:\n            result = inspect_drawing_rag(pdf_path, verbose=False)\n            result['file'] = os.path.basename(pdf_path)\n            results.append(result)\n\n            if result['result'] == 'PASS': pass_count += 1\n            elif result['result'] == 'FAIL': fail_count += 1\n            else: error_count += 1\n        except Exception as e:\n            results.append({'file': os.path.basename(pdf_path), 'result': 'ERROR', 'details': str(e)})\n            error_count += 1\n\n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n\n    print(f\"\\n{'='*60}\\nSUMMARY\\n{'='*60}\")\n    print(f\"Total: {len(results)} | PASS: {pass_count} | FAIL: {fail_count} | ERROR: {error_count}\")\n    print(f\"Saved to: {output_file}\")\n    return results\n\nprint(\"‚úÖ Batch inspection function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM1_6wuZwHHb"
   },
   "source": [
    "## 7. Test the Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0hzx2gEywHHb",
    "outputId": "936081db-a3db-47f2-ac53-240786c51f40"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7A: Single File Test (STRICT MISMATCH MODE)\n# ============================================================\nfrom google.colab import files\n\nprint(\"Upload a PDF drawing to inspect (Strict Mismatch Mode):\")\nuploaded = files.upload()\n\nif uploaded:\n    test_drawing = list(uploaded.keys())[0]\n    print(f\"\\nRunning STRICT MISMATCH inspection on {test_drawing}...\")\n    result = inspect_drawing_production(test_drawing)  # <-- Uses Strict Mismatch Mode\nelse:\n    print(\"No file uploaded.\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00c88766",
    "outputId": "4a0bd8e0-43eb-4730-ce5f-5474b39601da"
   },
   "source": "# ============================================================\n# CELL 7B: Recovery - Reload Context (After Restart)\n# ============================================================\nimport os, json, re\n\nif 'filename_to_pn' not in globals() or 'part_context_db' not in globals():\n    print(\"üîÑ Reloading Context Databases...\")\n\n    MAPPING_FILE = \"400S_file_part_mapping.json\"\n    STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n\n    def locate_file(filename):\n        if os.path.exists(filename): return os.path.abspath(filename)\n        if os.path.exists(os.path.join(\"rag_data\", filename)): return os.path.abspath(os.path.join(\"rag_data\", filename))\n        return None\n\n    FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n    STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n\n    if not FILE_MAPPING_PATH or not STRUCTURE_PATH:\n        print(\"‚ùå Config files not found. Please re-upload.\")\n    else:\n        with open(FILE_MAPPING_PATH, 'r') as f:\n            file_mapping_list = json.load(f)\n\n        filename_to_pn = {}\n        for entry in file_mapping_list:\n            pn, fname = entry.get('pn'), entry.get('file')\n            if pn and fname:\n                filename_to_pn[fname] = pn\n                filename_to_pn[fname + '.pdf'] = pn\n                filename_to_pn[fname + '.PDF'] = pn\n\n        with open(STRUCTURE_PATH, 'r') as f:\n            structure_data = json.load(f)\n\n        part_context_db = {}\n        def normalize_pn(pn): return re.sub(r'[-\\s]', '', str(pn)).lower()\n\n        for assembly_name, parts_list in structure_data.items():\n            for part in parts_list:\n                pn, desc = part['pn'], part['desc']\n                siblings = [f\"{p['pn']} ({p['desc']})\" for p in parts_list if p['pn'] != pn]\n                key = normalize_pn(pn)\n                ctx = {'pn': pn, 'description': desc, 'assembly': assembly_name, 'siblings': \"; \".join(siblings[:12])}\n                part_context_db[key] = ctx\n                part_context_db[pn] = ctx\n\n        print(f\"‚úÖ Restored: {len(filename_to_pn)} mappings, {len(part_context_db)//2} parts\")\nelse:\n    print(\"‚úÖ Context already loaded.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc691493",
    "outputId": "f5f28246-2e27-46c7-806d-2ed5f175c633"
   },
   "source": "# ============================================================\n# CELL 7C: OCR Libraries (Backup Install)\n# ============================================================\n!sudo apt-get install -y tesseract-ocr > /dev/null 2>&1\n!pip install -q pytesseract\nprint(\"‚úÖ Tesseract OCR Libraries Installed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "666f60f1",
    "outputId": "1d289435-1a92-4882-f6cb-d325c04fb89e"
   },
   "source": "# ============================================================\n# CELL 7D: Verify RAG Database\n# ============================================================\nimport os, glob\n\nif 'DATA_DIR' not in globals():\n    DATA_DIR = \"/content\"\n\nrag_db_path = os.path.join(DATA_DIR, \"rag_visual_db\")\nprint(f\"DATA_DIR: {DATA_DIR}\")\nprint(f\"RAG path: {rag_db_path}\")\n\nif os.path.exists(rag_db_path):\n    images = glob.glob(os.path.join(rag_db_path, \"**/*.png\"), recursive=True)\n    images += glob.glob(os.path.join(rag_db_path, \"**/*.jpg\"), recursive=True)\n    print(f\"‚úÖ Found {len(images)} images\")\nelse:\n    print(\"‚ùå Folder NOT found\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JB5KJmE_wHHb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "outputId": "2a899988-d821-4c99-99b1-aa88f9eb9f65"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7E: Batch Test (Upload ZIP)\n# ============================================================\nfrom google.colab import files\nimport shutil, zipfile, glob\n\nprint(\"Upload a ZIP file with PDF drawings:\")\nuploaded = files.upload()\n\nif uploaded:\n    zip_file = next((f for f in uploaded if f.lower().endswith('.zip')), None)\n    if zip_file:\n        batch_dir = \"batch_drawings\"\n        if os.path.exists(batch_dir): shutil.rmtree(batch_dir)\n        os.makedirs(batch_dir, exist_ok=True)\n\n        print(f\"Extracting {zip_file}...\")\n        with zipfile.ZipFile(zip_file, 'r') as zf:\n            zf.extractall(batch_dir)\n\n        pdfs = glob.glob(os.path.join(batch_dir, \"**/*.pdf\"), recursive=True)\n        print(f\"Found {len(pdfs)} PDFs\")\n\n        results = inspect_batch(batch_dir, \"inspection_results.json\")\n    else:\n        print(\"No ZIP file found\")\nelse:\n    print(\"No files uploaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG8mlYNMwHHb"
   },
   "source": [
    "## 8. View Failed Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DgLE5trwHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 8: View Failed Inspections\n# ============================================================\n\ndef show_failures(results):\n    \"\"\"Display failed inspections.\"\"\"\n    failures = [r for r in results if r.get('result') == 'FAIL']\n    print(f\"\\nFAILED: {len(failures)}\")\n    print('='*60)\n\n    for i, fail in enumerate(failures, 1):\n        print(f\"\\n[{i}] {fail.get('file', 'Unknown')}\")\n        print(f\"    Part: {fail.get('part_number', 'N/A')} - {fail.get('description', 'N/A')}\")\n        print(f\"    Details: {fail.get('details', 'N/A')[:300]}...\")\n\n# Usage: show_failures(results)\nprint(\"‚úÖ show_failures() defined.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2cb373474b194b8da3ef1dc6753df2e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da6c3848895042bb9985d5febae6250a",
       "IPY_MODEL_f2c44e9a7b7744e682ae9b1959b31764",
       "IPY_MODEL_40681f3c555c4b7c8151e328ebafcbb3"
      ],
      "layout": "IPY_MODEL_93130fc6f2204866813a448860e0b8e3"
     }
    },
    "da6c3848895042bb9985d5febae6250a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47c9fcd759d749e9866e8c5e4a42908b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_07ee67ba714d4787b7c3a8650421445f",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "f2c44e9a7b7744e682ae9b1959b31764": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ca9810080244d0aafac09c78ab027ea",
      "max": 38,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_97f20662d5754c30818216f746845a52",
      "value": 38
     }
    },
    "40681f3c555c4b7c8151e328ebafcbb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d3c5bc3a43f40f595a4ecfc1da171a5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4998675f0ff244c8a4309534a2fe7e09",
      "value": "‚Äá38/38‚Äá[13:08&lt;00:00,‚Äá16.04s/it]"
     }
    },
    "93130fc6f2204866813a448860e0b8e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47c9fcd759d749e9866e8c5e4a42908b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07ee67ba714d4787b7c3a8650421445f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ca9810080244d0aafac09c78ab027ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97f20662d5754c30818216f746845a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0d3c5bc3a43f40f595a4ecfc1da171a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4998675f0ff244c8a4309534a2fe7e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}