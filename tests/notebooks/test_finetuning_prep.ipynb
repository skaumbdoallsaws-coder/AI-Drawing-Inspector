{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Finetuning Prep: Evaluation (M12) + YOLO Training Setup\n",
        "\n",
        "This notebook tests the evaluation harness and prepares for YOLO-OBB finetuning:\n",
        "\n",
        "- **M12 -- Evaluator**: Compares pipeline predictions against ground truth sidecar\n",
        "  annotations. Provides stage-by-stage metrics (detection F1, transcription CER/WER,\n",
        "  parsing accuracy) and identifies the bottleneck stage.\n",
        "- **YOLO Finetuning Config**: Placeholder for A100-optimized YOLO training setup\n",
        "  using ultralytics.\n",
        "\n",
        "**Runtime requirement:** GPU (A100 preferred). The evaluation itself is CPU-based,\n",
        "but the finetuning configuration cell requires A100 to be meaningful.\n",
        "\n",
        "**Required files:**\n",
        "- Ground truth sidecar YAML/JSON files\n",
        "- Pipeline prediction outputs (or run pipeline inline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install dependencies\n",
        "# NOTE: Set your runtime to GPU (A100) for the finetuning config cell.\n",
        "%pip install ultralytics pillow pyyaml --quiet\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git /content/AI-Drawing-Inspector 2>/dev/null || \\\n",
        "    (cd /content/AI-Drawing-Inspector && git pull)\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/AI-Drawing-Inspector')\n",
        "\n",
        "print('Dependencies installed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Check GPU info (should be A100)\n",
        "import torch\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f'GPU name: {gpu_name}')\n",
        "    print(f'Compute capability: {props.major}.{props.minor}')\n",
        "    print(f'Total VRAM: {props.total_mem / 1e9:.1f} GB')\n",
        "    print(f'SM count: {props.multi_processor_count}')\n",
        "\n",
        "    if 'A100' in gpu_name:\n",
        "        print('\\n[OK] A100 GPU detected. Full finetuning is supported.')\n",
        "    elif 'T4' in gpu_name:\n",
        "        print('\\n[WARN] T4 GPU detected. Finetuning will be slower; consider A100.')\n",
        "    else:\n",
        "        print(f'\\n[INFO] GPU: {gpu_name}. Check VRAM is sufficient for finetuning.')\n",
        "else:\n",
        "    print('\\n[ERROR] No GPU detected. Set Runtime > Change runtime type > GPU.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Import evaluation modules\n",
        "from ai_inspector.fine_tuning.evaluate import (\n",
        "    evaluate_page,\n",
        "    evaluate_batch,\n",
        "    print_evaluation_table,\n",
        "    load_sidecar,\n",
        "    load_sidecars,\n",
        "    pair_detections_iou,\n",
        "    compute_cer,\n",
        "    compute_wer,\n",
        "    compute_parsing_accuracy,\n",
        ")\n",
        "\n",
        "print('Evaluation modules imported successfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Load ground truth sidecar annotations\n",
        "# Option A: Load from Drive\n",
        "# GT_DIR = '/content/drive/MyDrive/ai_inspector_data/ground_truth/'\n",
        "# sidecars = load_sidecars(GT_DIR)\n",
        "\n",
        "# Option B: Use synthetic ground truth for testing\n",
        "print('Creating synthetic ground truth for evaluation testing...')\n",
        "\n",
        "ground_truth = [\n",
        "    {\n",
        "        'class': 'Hole',\n",
        "        'obb_points': [[100, 100], [300, 100], [300, 180], [100, 180]],\n",
        "        'text': '\\u2300.500 THRU',\n",
        "        'parsed': {\n",
        "            'calloutType': 'Hole',\n",
        "            'diameter': '.500',\n",
        "            'depth': 'THRU',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'class': 'Hole',\n",
        "        'obb_points': [[400, 200], [600, 200], [600, 280], [400, 280]],\n",
        "        'text': '\\u2300.250 DEEP .500',\n",
        "        'parsed': {\n",
        "            'calloutType': 'Hole',\n",
        "            'diameter': '.250',\n",
        "            'depth': '.500',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'class': 'TappedHole',\n",
        "        'obb_points': [[700, 300], [950, 300], [950, 380], [700, 380]],\n",
        "        'text': 'M10x1.5 THRU',\n",
        "        'parsed': {\n",
        "            'calloutType': 'TappedHole',\n",
        "            'threadSize': 'M10x1.5',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'class': 'Fillet',\n",
        "        'obb_points': [[100, 400], [250, 400], [250, 460], [100, 460]],\n",
        "        'text': 'R.125 TYP.',\n",
        "        'parsed': {\n",
        "            'calloutType': 'Fillet',\n",
        "            'radius': '.125',\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        'class': 'Chamfer',\n",
        "        'obb_points': [[300, 500], [500, 500], [500, 560], [300, 560]],\n",
        "        'text': '.045 x 45\\u00b0',\n",
        "        'parsed': {\n",
        "            'calloutType': 'Chamfer',\n",
        "            'size': '.045',\n",
        "            'angle': '45',\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "# Simulated predictions (imperfect -- some errors for realistic eval)\n",
        "predictions = [\n",
        "    {   # Correct detection + correct OCR\n",
        "        'class': 'Hole',\n",
        "        'obb_points': [[102, 98], [298, 102], [296, 182], [100, 178]],\n",
        "        'text': '\\u2300.500 THRU',\n",
        "        'parsed': {'calloutType': 'Hole', 'diameter': '.500', 'depth': 'THRU'},\n",
        "    },\n",
        "    {   # Correct detection, slight OCR error\n",
        "        'class': 'Hole',\n",
        "        'obb_points': [[402, 198], [598, 202], [596, 282], [400, 278]],\n",
        "        'text': '\\u2300.250 DEEP .S00',  # OCR error: S instead of 5\n",
        "        'parsed': {'calloutType': 'Hole', 'diameter': '.250', 'depth': '.S00'},\n",
        "    },\n",
        "    {   # Correct detection + correct OCR\n",
        "        'class': 'TappedHole',\n",
        "        'obb_points': [[698, 302], [952, 298], [954, 378], [700, 382]],\n",
        "        'text': 'M10x1.5 THRU',\n",
        "        'parsed': {'calloutType': 'TappedHole', 'threadSize': 'M10x1.5'},\n",
        "    },\n",
        "    {   # Correct detection, correct OCR\n",
        "        'class': 'Fillet',\n",
        "        'obb_points': [[98, 402], [252, 398], [254, 458], [100, 462]],\n",
        "        'text': 'R.125 TYP.',\n",
        "        'parsed': {'calloutType': 'Fillet', 'radius': '.125'},\n",
        "    },\n",
        "    {   # False positive (no matching GT)\n",
        "        'class': 'Dimension',\n",
        "        'obb_points': [[800, 600], [1000, 600], [1000, 660], [800, 660]],\n",
        "        'text': '3.500',\n",
        "        'parsed': {'calloutType': 'Dimension'},\n",
        "    },\n",
        "    # Missing: Chamfer (false negative -- not detected)\n",
        "]\n",
        "\n",
        "print(f'Ground truth annotations: {len(ground_truth)}')\n",
        "print(f'Predictions: {len(predictions)}')\n",
        "print(f'Expected: 4 TP, 1 FP (Dimension), 1 FN (Chamfer)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Run evaluate_page() on sample\n",
        "eval_results = evaluate_page(\n",
        "    predictions=predictions,\n",
        "    ground_truth=ground_truth,\n",
        "    iou_threshold=0.3,\n",
        ")\n",
        "\n",
        "import json\n",
        "print('Raw evaluation results:')\n",
        "print(json.dumps(eval_results, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Print evaluation table\n",
        "print_evaluation_table(eval_results)\n",
        "\n",
        "# Additional detail\n",
        "print('\\n--- Detection Detail ---')\n",
        "det = eval_results['detection']\n",
        "print(f'True positives:  {det[\"true_positives\"]}')\n",
        "print(f'False positives: {det[\"false_positives\"]}')\n",
        "print(f'False negatives: {det[\"false_negatives\"]}')\n",
        "print(f'Mean IoU:        {det[\"mean_iou\"]}')\n",
        "\n",
        "print('\\n--- Transcription Detail ---')\n",
        "trans = eval_results['transcription']\n",
        "print(f'Evaluated pairs: {trans[\"evaluated_count\"]}')\n",
        "print(f'Mean CER:        {trans[\"mean_cer\"]}')\n",
        "print(f'Mean WER:        {trans[\"mean_wer\"]}')\n",
        "\n",
        "print('\\n--- Parsing Detail ---')\n",
        "parse = eval_results['parsing']\n",
        "print(f'Fields correct:  {parse[\"fields_correct\"]}')\n",
        "print(f'Fields total:    {parse[\"fields_total\"]}')\n",
        "print(f'Accuracy:        {parse[\"accuracy\"]}')\n",
        "\n",
        "print('\\n--- Class Breakdown ---')\n",
        "for cls, info in eval_results.get('class_breakdown', {}).items():\n",
        "    print(f'  {cls}: TP={info[\"tp\"]}, class_match={info[\"class_match\"]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Identify bottleneck stage\n",
        "summary = eval_results['summary']\n",
        "\n",
        "print('=== Pipeline Bottleneck Analysis ===')\n",
        "print()\n",
        "print(f'Detection F1:        {summary[\"detection_f1\"]:.4f}')\n",
        "print(f'Mean CER:            {summary[\"mean_cer\"]:.4f}  (lower is better)')\n",
        "print(f'Parsing accuracy:    {summary[\"parsing_accuracy\"]:.4f}')\n",
        "print()\n",
        "print(f'>>> Bottleneck stage: {summary[\"bottleneck\"].upper()}')\n",
        "print()\n",
        "\n",
        "# Recommendations based on bottleneck\n",
        "bottleneck = summary['bottleneck']\n",
        "if bottleneck == 'detection':\n",
        "    print('Recommendation: Finetune YOLO-OBB model on more annotated drawings.')\n",
        "    print('  - Collect more OBB annotations (CVAT or Label Studio)')\n",
        "    print('  - Increase training epochs or adjust augmentation')\n",
        "    print('  - See Cell 8 for A100 finetuning configuration')\n",
        "elif bottleneck == 'transcription':\n",
        "    print('Recommendation: Improve OCR stage.')\n",
        "    print('  - Check rotation selector quality scores')\n",
        "    print('  - Consider finetuning LightOnOCR-2 on engineering text')\n",
        "    print('  - Add more canonicalization rules for common OCR errors')\n",
        "elif bottleneck == 'parsing':\n",
        "    print('Recommendation: Improve regex patterns or add VLM fallback.')\n",
        "    print('  - Review missed parses in crop_reader debug output')\n",
        "    print('  - Add regex patterns for new callout formats')\n",
        "    print('  - Enable VLM fallback (Qwen) for low-confidence OCR results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Placeholder for YOLO finetuning configuration (A100 optimized)\n",
        "print('=== YOLO-OBB Finetuning Configuration (A100) ===')\n",
        "print()\n",
        "print('This cell provides a ready-to-run finetuning setup for YOLO11-OBB.')\n",
        "print('Uncomment and run when you have annotated training data.\\n')\n",
        "\n",
        "# ---------- YOLO Finetuning Config (uncomment to run) ----------\n",
        "\n",
        "# from ultralytics import YOLO\n",
        "#\n",
        "# # Load pretrained model\n",
        "# model = YOLO('yolo11n-obb.pt')  # or your current best.pt\n",
        "#\n",
        "# # A100-optimized training configuration\n",
        "# results = model.train(\n",
        "#     # Dataset\n",
        "#     data='/content/drive/MyDrive/ai_inspector_data/dataset.yaml',\n",
        "#     task='obb',\n",
        "#\n",
        "#     # Training params (A100 optimized)\n",
        "#     epochs=100,\n",
        "#     batch=32,              # A100 can handle large batch with OBB\n",
        "#     imgsz=1024,            # High res for engineering drawings\n",
        "#     device='cuda',\n",
        "#\n",
        "#     # Optimization\n",
        "#     optimizer='AdamW',\n",
        "#     lr0=0.001,\n",
        "#     lrf=0.01,              # Final LR = lr0 * lrf\n",
        "#     warmup_epochs=3,\n",
        "#     weight_decay=0.0005,\n",
        "#\n",
        "#     # Augmentation (conservative for technical drawings)\n",
        "#     hsv_h=0.0,            # No hue shift (drawings are B&W)\n",
        "#     hsv_s=0.0,            # No saturation shift\n",
        "#     hsv_v=0.2,            # Slight brightness variation\n",
        "#     degrees=5.0,          # Small rotation (callouts have orientation)\n",
        "#     translate=0.1,\n",
        "#     scale=0.3,\n",
        "#     flipud=0.0,           # No vertical flip (text would be upside down)\n",
        "#     fliplr=0.0,           # No horizontal flip (text would be mirrored)\n",
        "#     mosaic=0.5,           # Reduced mosaic (preserve drawing context)\n",
        "#     mixup=0.0,            # No mixup (drawings don't blend well)\n",
        "#\n",
        "#     # Output\n",
        "#     project='/content/drive/MyDrive/ai_inspector_models/finetune',\n",
        "#     name='yolo11_obb_v1',\n",
        "#     save=True,\n",
        "#     save_period=10,\n",
        "#     plots=True,\n",
        "#     verbose=True,\n",
        "# )\n",
        "#\n",
        "# print('Training complete.')\n",
        "# print(f'Best model: {results.save_dir}/weights/best.pt')\n",
        "\n",
        "print('----------')\n",
        "print('dataset.yaml format for OBB training:')\n",
        "print('''\n",
        "# dataset.yaml\n",
        "path: /content/drive/MyDrive/ai_inspector_data/obb_dataset\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "names:\n",
        "  0: Hole\n",
        "  1: TappedHole\n",
        "  2: CounterboreHole\n",
        "  3: CountersinkHole\n",
        "  4: Fillet\n",
        "  5: Chamfer\n",
        "  6: Thread\n",
        "  7: Slot\n",
        "  8: Bend\n",
        "  9: GDT\n",
        "  10: SurfaceFinish\n",
        "  11: Dimension\n",
        "  12: Tolerance\n",
        "  13: Note\n",
        "''')\n",
        "\n",
        "print('OBB label format (per image .txt):')\n",
        "print('  class_id x1 y1 x2 y2 x3 y3 x4 y4')\n",
        "print('  (all values normalized 0-1)')\n",
        "print()\n",
        "print('A100 training estimates:')\n",
        "print('  - 100 epochs, batch=32, imgsz=1024: ~2-4 hours')\n",
        "print('  - VRAM usage: ~15-20 GB (plenty of headroom on 40 GB A100)')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}