{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsQ04WlL3Qt5"
   },
   "source": [
    "# AI Engineering Drawing Inspector v2.0\n",
    "\n",
    "**Single-File QC Pipeline with Production Upgrades**\n",
    "\n",
    "This notebook processes one drawing PDF at a time and produces 4 artifacts:\n",
    "1. `ResolvedPartIdentity.json` - Part identification with confidence\n",
    "2. `DrawingEvidence.json` - Extracted callouts (validates against schema v1.1.1)\n",
    "3. `DiffResult.json` - Comparison with SolidWorks truth\n",
    "4. `QCReport.md` - Human-readable QC report\n",
    "\n",
    "**Production Upgrades Implemented:**\n",
    "- **Upgrade 1**: Real OCR bounding boxes (Tesseract + LightOnOCR hybrid)\n",
    "- **Upgrade 2**: Title-block / notes-region ROI OCR for identity extraction\n",
    "- **Upgrade 3**: Explicit unit detection + normalization (INCH/MM)\n",
    "- **Upgrade 4**: Deterministic canonicalization post-processor (regex-based)\n",
    "\n",
    "**Models:**\n",
    "- OCR: Tesseract (bboxes) + LightOnOCR-2 (text quality)\n",
    "- VLM: Qwen2-VL-7B\n",
    "- Judge: Text LLM\n",
    "\n",
    "**Version:** 2.0 | **Schema:** DrawingEvidence v1.1.1"
   ],
   "id": "vsQ04WlL3Qt5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6us5h8l53Qt6"
   },
   "source": [
    "---\n",
    "## Section 0: Configuration"
   ],
   "id": "6us5h8l53Qt6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlcIEWPG3Qt6"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 0A: Install Dependencies\n",
    "# ============================================================\n",
    "!pip install -q accelerate qwen-vl-utils pdf2image bitsandbytes\n",
    "!pip install -q pymupdf opencv-python-headless jsonschema\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q pillow pypdfium2 huggingface_hub\n",
    "!pip install -q pytesseract  # For real OCR bounding boxes\n",
    "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ],
   "outputs": [],
   "id": "JlcIEWPG3Qt6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcO9Z7J63Qt6"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 0B: Configuration Variables\n",
    "# ============================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# USER CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# Input: Single drawing PDF path\n",
    "DRAWING_PDF_PATH = \"\"  # Set this to your PDF path\n",
    "\n",
    "# SolidWorks JSON library folder\n",
    "SOLIDWORKS_JSON_DIR = \"sw_json_library\"  # Folder containing part JSONs\n",
    "\n",
    "# Output directory for artifacts\n",
    "OUTPUT_DIR = \"qc_output\"\n",
    "\n",
    "# Mode: \"fast\" (first page only) or \"full\" (all pages)\n",
    "MODE = \"fast\"\n",
    "\n",
    "# Schema file path\n",
    "SCHEMA_PATH = \"schemas/drawing_evidence_v1.1.1.schema.json\"\n",
    "\n",
    "# =========================\n",
    "# DERIVED PATHS\n",
    "# =========================\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Schema version we're using\n",
    "SCHEMA_VERSION = \"1.1.1\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  DRAWING_PDF_PATH: {DRAWING_PDF_PATH or '(not set)'}\")\n",
    "print(f\"  SOLIDWORKS_JSON_DIR: {SOLIDWORKS_JSON_DIR}\")\n",
    "print(f\"  OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"  MODE: {MODE}\")\n",
    "print(f\"  SCHEMA_VERSION: {SCHEMA_VERSION}\")"
   ],
   "outputs": [],
   "id": "DcO9Z7J63Qt6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SoBkVc03Qt6"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 0C: Import Libraries & GPU Setup\n",
    "# ============================================================\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import jsonschema\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [],
   "id": "2SoBkVc03Qt6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ooiu1Vxj3Qt6"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 0D: Load Schema for Validation\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Check if schema exists, otherwise upload\n",
    "if not os.path.exists(SCHEMA_PATH):\n",
    "    os.makedirs(os.path.dirname(SCHEMA_PATH), exist_ok=True)\n",
    "    print(f\"Schema not found at {SCHEMA_PATH}\")\n",
    "    print(\"Please upload drawing_evidence_v1.1.1.schema.json:\")\n",
    "    uploaded = files.upload()\n",
    "    for filename in uploaded:\n",
    "        if 'schema' in filename.lower():\n",
    "            with open(SCHEMA_PATH, 'wb') as f:\n",
    "                f.write(uploaded[filename])\n",
    "            print(f\"Schema saved to {SCHEMA_PATH}\")\n",
    "            break\n",
    "\n",
    "# Load schema\n",
    "try:\n",
    "    with open(SCHEMA_PATH, 'r') as f:\n",
    "        DRAWING_EVIDENCE_SCHEMA = json.load(f)\n",
    "    print(f\"Schema loaded: {DRAWING_EVIDENCE_SCHEMA.get('title', 'Unknown')}\")\n",
    "    print(f\"Schema version: {DRAWING_EVIDENCE_SCHEMA.get('properties', {}).get('schemaVersion', {}).get('const', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load schema: {e}\")\n",
    "    DRAWING_EVIDENCE_SCHEMA = None"
   ],
   "outputs": [],
   "id": "Ooiu1Vxj3Qt6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1fE7grP3Qt7"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 0E: HuggingFace Authentication\n",
    "# ============================================================\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in to HuggingFace!\")\n",
    "except Exception as e:\n",
    "    print(f\"HF Login failed: {e}\")\n",
    "    print(\"Set HF_TOKEN in Colab Secrets (key icon in left sidebar)\")\n",
    "    hf_token = None"
   ],
   "outputs": [],
   "id": "Z1fE7grP3Qt7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJBU5fIP3Qt7"
   },
   "source": [
    "---\n",
    "## Section 1: PDF Ingestion -> PageArtifacts"
   ],
   "id": "ZJBU5fIP3Qt7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2D4PxAg3Qt7"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 1A: PageArtifacts Data Structure\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class PageArtifact:\n",
    "    \"\"\"Rendered page from PDF with associated data.\"\"\"\n",
    "    pageIndex0: int  # 0-indexed page number\n",
    "    page: int  # 1-indexed page number (for schema)\n",
    "    image: Image.Image  # Rendered page image\n",
    "    width: int\n",
    "    height: int\n",
    "    dpi: int\n",
    "    direct_text: Optional[str] = None  # PDF text layer if available\n",
    "\n",
    "\n",
    "def render_pdf_to_artifacts(pdf_path: str, mode: str = \"fast\", dpi: int = 300) -> List[PageArtifact]:\n",
    "    \"\"\"\n",
    "    Render PDF pages to PageArtifacts.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        mode: \"fast\" (first page only) or \"full\" (all pages)\n",
    "        dpi: Resolution for rendering\n",
    "\n",
    "    Returns:\n",
    "        List of PageArtifact objects\n",
    "    \"\"\"\n",
    "    artifacts = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "\n",
    "        # Determine which pages to render\n",
    "        if mode == \"fast\":\n",
    "            pages_to_render = [0]  # First page only\n",
    "        else:\n",
    "            pages_to_render = list(range(total_pages))\n",
    "\n",
    "        print(f\"PDF: {os.path.basename(pdf_path)}\")\n",
    "        print(f\"  Total pages: {total_pages}\")\n",
    "        print(f\"  Rendering: {len(pages_to_render)} page(s) at {dpi} DPI\")\n",
    "\n",
    "        for page_idx in pages_to_render:\n",
    "            page = doc.load_page(page_idx)\n",
    "\n",
    "            # Render to image\n",
    "            zoom = dpi / 72.0\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "            # Extract direct text if available\n",
    "            direct_text = page.get_text(\"text\")\n",
    "            if direct_text and len(direct_text.strip()) < 10:\n",
    "                direct_text = None  # Probably just whitespace\n",
    "\n",
    "            artifact = PageArtifact(\n",
    "                pageIndex0=page_idx,\n",
    "                page=page_idx + 1,  # 1-indexed\n",
    "                image=img,\n",
    "                width=pix.width,\n",
    "                height=pix.height,\n",
    "                dpi=dpi,\n",
    "                direct_text=direct_text\n",
    "            )\n",
    "            artifacts.append(artifact)\n",
    "            print(f\"    Page {page_idx + 1}: {pix.width}x{pix.height}px\")\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "    return artifacts\n",
    "\n",
    "print(\"PageArtifact structure defined.\")\n",
    "print(\"render_pdf_to_artifacts() ready.\")"
   ],
   "outputs": [],
   "id": "q2D4PxAg3Qt7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviOnjfV3Qt7"
   },
   "source": [
    "---\n",
    "## Section 2: SolidWorks JSON Library Indexing"
   ],
   "id": "nviOnjfV3Qt7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-YyDPUn3Qt7"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 2A: SolidWorks JSON Library Index (BOM-Robust)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class SwPartEntry:\n",
    "    \"\"\"Entry in the SolidWorks parts index.\"\"\"\n",
    "    json_path: str\n",
    "    part_number: str\n",
    "    custom_id: Optional[str] = None\n",
    "    custom_part_number: Optional[str] = None\n",
    "    filename_stem: str = \"\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LoadResult:\n",
    "    \"\"\"Result of loading JSON files.\"\"\"\n",
    "    loaded: int\n",
    "    skipped: int\n",
    "    skipped_files: List[Tuple[str, str]]  # (filename, reason)\n",
    "\n",
    "\n",
    "def load_json_robust(filepath: Path) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Load a JSON file with robust encoding handling.\n",
    "\n",
    "    Tries encodings in order:\n",
    "    1. utf-8-sig (handles BOM automatically)\n",
    "    2. utf-8 (standard)\n",
    "    3. latin-1 (fallback for weird encodings)\n",
    "\n",
    "    Returns:\n",
    "        (data, error_message) - data is None if failed, error_message is None if success\n",
    "    \"\"\"\n",
    "    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin-1']\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=encoding) as f:\n",
    "                data = json.load(f)\n",
    "            return data, None\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except json.JSONDecodeError as e:\n",
    "            # If it's a BOM error on utf-8, try utf-8-sig\n",
    "            if 'BOM' in str(e) and encoding == 'utf-8':\n",
    "                continue\n",
    "            # For other JSON errors, return the error\n",
    "            return None, f\"JSON parse error: {str(e)[:50]}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Unexpected error: {str(e)[:50]}\"\n",
    "\n",
    "    return None, \"Failed all encoding attempts\"\n",
    "\n",
    "\n",
    "def normalize_json_to_utf8(filepath: Path, dry_run: bool = False) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Normalize a JSON file to plain UTF-8 (no BOM).\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to JSON file\n",
    "        dry_run: If True, only report what would be done\n",
    "\n",
    "    Returns:\n",
    "        (success, message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read with BOM-aware encoding\n",
    "        with open(filepath, 'rb') as f:\n",
    "            raw = f.read()\n",
    "\n",
    "        # Check for BOM\n",
    "        has_bom = raw.startswith(b'\\xef\\xbb\\xbf')\n",
    "\n",
    "        if not has_bom:\n",
    "            return True, \"Already clean UTF-8\"\n",
    "\n",
    "        if dry_run:\n",
    "            return True, \"Would remove BOM\"\n",
    "\n",
    "        # Remove BOM and re-encode\n",
    "        content = raw[3:].decode('utf-8')\n",
    "\n",
    "        # Validate it's valid JSON\n",
    "        data = json.loads(content)\n",
    "\n",
    "        # Write back as clean UTF-8\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return True, \"BOM removed, file normalized\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {str(e)[:50]}\"\n",
    "\n",
    "\n",
    "def normalize_json_directory(directory: str, dry_run: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize all JSON files in a directory to plain UTF-8.\n",
    "\n",
    "    Args:\n",
    "        directory: Path to directory\n",
    "        dry_run: If True, only report what would be done\n",
    "\n",
    "    Returns:\n",
    "        Summary dict with counts and details\n",
    "    \"\"\"\n",
    "    json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "\n",
    "    results = {\n",
    "        \"total\": len(json_files),\n",
    "        \"already_clean\": 0,\n",
    "        \"would_fix\": 0,\n",
    "        \"fixed\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"details\": []\n",
    "    }\n",
    "\n",
    "    print(f\"{'[DRY RUN] ' if dry_run else ''}Normalizing JSON files in {directory}...\")\n",
    "    print(f\"  Found {len(json_files)} JSON files\")\n",
    "\n",
    "    for fp in json_files:\n",
    "        success, msg = normalize_json_to_utf8(fp, dry_run=dry_run)\n",
    "\n",
    "        if \"Already clean\" in msg:\n",
    "            results[\"already_clean\"] += 1\n",
    "        elif \"Would remove\" in msg:\n",
    "            results[\"would_fix\"] += 1\n",
    "            results[\"details\"].append((fp.name, msg))\n",
    "        elif \"BOM removed\" in msg:\n",
    "            results[\"fixed\"] += 1\n",
    "            results[\"details\"].append((fp.name, msg))\n",
    "        else:\n",
    "            results[\"errors\"] += 1\n",
    "            results[\"details\"].append((fp.name, msg))\n",
    "\n",
    "    print(f\"  Already clean: {results['already_clean']}\")\n",
    "    print(f\"  {'Would fix' if dry_run else 'Fixed'}: {results['would_fix'] if dry_run else results['fixed']}\")\n",
    "    print(f\"  Errors: {results['errors']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class SwJsonLibrary:\n",
    "    \"\"\"Index of SolidWorks JSON files for fast lookup (BOM-robust).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.by_part_number: Dict[str, SwPartEntry] = {}\n",
    "        self.by_custom_id: Dict[str, SwPartEntry] = {}\n",
    "        self.by_custom_pn: Dict[str, SwPartEntry] = {}\n",
    "        self.by_filename: Dict[str, SwPartEntry] = {}\n",
    "        self.all_entries: List[SwPartEntry] = []\n",
    "        self.load_result: Optional[LoadResult] = None\n",
    "\n",
    "    def _normalize(self, s: str) -> str:\n",
    "        \"\"\"Normalize string for lookup.\"\"\"\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        return re.sub(r'[-\\s_]', '', str(s)).lower()\n",
    "\n",
    "    def load_from_directory(self, directory: str) -> LoadResult:\n",
    "        \"\"\"\n",
    "        Load all JSON files from directory into index.\n",
    "\n",
    "        Handles UTF-8 BOM and other encoding issues gracefully.\n",
    "        \"\"\"\n",
    "        loaded = 0\n",
    "        skipped = 0\n",
    "        skipped_files = []\n",
    "\n",
    "        json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "\n",
    "        print(f\"Indexing SolidWorks JSON library...\")\n",
    "        print(f\"  Directory: {directory}\")\n",
    "        print(f\"  Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for json_path in json_files:\n",
    "            # Use robust JSON loader\n",
    "            data, error = load_json_robust(json_path)\n",
    "\n",
    "            if data is None:\n",
    "                skipped += 1\n",
    "                skipped_files.append((json_path.name, error or \"Unknown error\"))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract identifiers\n",
    "                identity = data.get('identity', {})\n",
    "                custom_props = data.get('customProperties', {})\n",
    "\n",
    "                part_number = identity.get('partNumber', '')\n",
    "                custom_id = custom_props.get('ID', '')\n",
    "                custom_pn = custom_props.get('PART_NUMBER', custom_props.get('Part_Number', ''))\n",
    "                filename_stem = json_path.stem\n",
    "\n",
    "                entry = SwPartEntry(\n",
    "                    json_path=str(json_path),\n",
    "                    part_number=part_number,\n",
    "                    custom_id=custom_id,\n",
    "                    custom_part_number=custom_pn,\n",
    "                    filename_stem=filename_stem,\n",
    "                    data=data\n",
    "                )\n",
    "\n",
    "                self.all_entries.append(entry)\n",
    "\n",
    "                # Index by various keys\n",
    "                if part_number:\n",
    "                    self.by_part_number[part_number] = entry\n",
    "                    self.by_part_number[self._normalize(part_number)] = entry\n",
    "\n",
    "                if custom_id:\n",
    "                    self.by_custom_id[custom_id] = entry\n",
    "                    self.by_custom_id[self._normalize(custom_id)] = entry\n",
    "\n",
    "                if custom_pn:\n",
    "                    self.by_custom_pn[custom_pn] = entry\n",
    "                    self.by_custom_pn[self._normalize(custom_pn)] = entry\n",
    "\n",
    "                if filename_stem:\n",
    "                    self.by_filename[filename_stem] = entry\n",
    "                    self.by_filename[self._normalize(filename_stem)] = entry\n",
    "\n",
    "                loaded += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "                skipped_files.append((json_path.name, f\"Processing error: {str(e)[:50]}\"))\n",
    "\n",
    "        # Store result\n",
    "        self.load_result = LoadResult(\n",
    "            loaded=loaded,\n",
    "            skipped=skipped,\n",
    "            skipped_files=skipped_files\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"  Loaded: {loaded} files\")\n",
    "        print(f\"  Skipped: {skipped} files\")\n",
    "\n",
    "        if skipped_files:\n",
    "            print(f\"  Skipped file details:\")\n",
    "            for fname, reason in skipped_files[:10]:  # Show first 10\n",
    "                print(f\"    - {fname}: {reason}\")\n",
    "            if len(skipped_files) > 10:\n",
    "                print(f\"    ... and {len(skipped_files) - 10} more\")\n",
    "\n",
    "        print(f\"  Index sizes: by_part_number={len(self.by_part_number)}, by_filename={len(self.by_filename)}\")\n",
    "\n",
    "        return self.load_result\n",
    "\n",
    "    def lookup(self, candidate: str) -> Optional[SwPartEntry]:\n",
    "        \"\"\"Look up a part by any identifier.\"\"\"\n",
    "        if not candidate:\n",
    "            return None\n",
    "\n",
    "        norm = self._normalize(candidate)\n",
    "\n",
    "        # Try exact matches first\n",
    "        if candidate in self.by_part_number:\n",
    "            return self.by_part_number[candidate]\n",
    "        if candidate in self.by_custom_id:\n",
    "            return self.by_custom_id[candidate]\n",
    "        if candidate in self.by_custom_pn:\n",
    "            return self.by_custom_pn[candidate]\n",
    "        if candidate in self.by_filename:\n",
    "            return self.by_filename[candidate]\n",
    "\n",
    "        # Try normalized\n",
    "        if norm in self.by_part_number:\n",
    "            return self.by_part_number[norm]\n",
    "        if norm in self.by_custom_id:\n",
    "            return self.by_custom_id[norm]\n",
    "        if norm in self.by_custom_pn:\n",
    "            return self.by_custom_pn[norm]\n",
    "        if norm in self.by_filename:\n",
    "            return self.by_filename[norm]\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Global library instance\n",
    "sw_library: Optional[SwJsonLibrary] = None\n",
    "\n",
    "print(\"SwJsonLibrary class defined (BOM-robust).\")"
   ],
   "outputs": [],
   "id": "E-YyDPUn3Qt7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQGQn_Gh3Qt7"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 2B: Load SolidWorks Library\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(SOLIDWORKS_JSON_DIR):\n",
    "    print(f\"SolidWorks JSON directory not found: {SOLIDWORKS_JSON_DIR}\")\n",
    "    print(\"Please upload a ZIP file containing your SolidWorks JSON files:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded:\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(SOLIDWORKS_JSON_DIR)\n",
    "            print(f\"Extracted to {SOLIDWORKS_JSON_DIR}\")\n",
    "            break\n",
    "\n",
    "# Initialize and load library\n",
    "sw_library = SwJsonLibrary()\n",
    "if os.path.exists(SOLIDWORKS_JSON_DIR):\n",
    "    sw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\n",
    "else:\n",
    "    print(f\"Warning: Could not find {SOLIDWORKS_JSON_DIR}\")"
   ],
   "outputs": [],
   "id": "SQGQn_Gh3Qt7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ5vUY-o3Qt7"
   },
   "source": [
    "---\n",
    "## Section 3: ResolvePartIdentity"
   ],
   "id": "AQ5vUY-o3Qt7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTuU_zs03Qt7"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 3A: Part Identity Resolution\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class IdentityCandidate:\n",
    "    \"\"\"A candidate part number with source info.\"\"\"\n",
    "    value: str\n",
    "    source: str  # \"filename\", \"ocr_title_block\", \"ocr_near_label\"\n",
    "    confidence: float\n",
    "    match_found: bool = False\n",
    "    json_path: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResolvedPartIdentity:\n",
    "    \"\"\"Result of part identity resolution.\"\"\"\n",
    "    resolvedPartNumber: Optional[str]\n",
    "    jsonPath: Optional[str]\n",
    "    confidence: float\n",
    "    candidates: Dict[str, List[Dict]]\n",
    "    notes: List[str]\n",
    "    needsReview: bool = False\n",
    "\n",
    "\n",
    "def extract_pn_from_filename(filepath: str) -> List[IdentityCandidate]:\n",
    "    \"\"\"\n",
    "    Extract part number candidates from filename.\n",
    "    Handles patterns like:\n",
    "    - 1013572_01.pdf -> 1013572\n",
    "    - 101357201-03.pdf -> 1013572\n",
    "    - 314884W_0.pdf -> 314884\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    filename = os.path.basename(filepath)\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Remove common suffixes\n",
    "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)  # Remove (1), (2) etc\n",
    "    name_no_ext = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n",
    "\n",
    "    # Split by common separators\n",
    "    parts = re.split(r'[\\s_]+', name_no_ext)\n",
    "    if not parts:\n",
    "        return candidates\n",
    "\n",
    "    base = parts[0]\n",
    "    tried = set()\n",
    "\n",
    "    def add_candidate(val: str, conf: float):\n",
    "        if val and val not in tried and len(val) >= 4:\n",
    "            tried.add(val)\n",
    "            match = sw_library.lookup(val) if sw_library else None\n",
    "            candidates.append(IdentityCandidate(\n",
    "                value=val,\n",
    "                source=\"filename\",\n",
    "                confidence=conf,\n",
    "                match_found=match is not None,\n",
    "                json_path=match.json_path if match else None\n",
    "            ))\n",
    "\n",
    "    # Try various transformations\n",
    "    add_candidate(base, 0.9)\n",
    "    add_candidate(base.replace('-', ''), 0.85)\n",
    "\n",
    "    # Remove letter suffix\n",
    "    if base and base[-1].isalpha():\n",
    "        add_candidate(base[:-1], 0.8)\n",
    "        add_candidate(base[:-1].replace('-', ''), 0.75)\n",
    "\n",
    "    # Handle revision pattern (1013572-01)\n",
    "    rev_match = re.match(r'^(.+)-(\\d{1,2})$', base)\n",
    "    if rev_match:\n",
    "        main_part = rev_match.group(1)\n",
    "        add_candidate(main_part, 0.85)\n",
    "        add_candidate(main_part.replace('-', ''), 0.8)\n",
    "\n",
    "    # Progressive digit stripping\n",
    "    temp = base.replace('-', '')\n",
    "    conf = 0.7\n",
    "    while len(temp) > 5 and conf > 0.3:\n",
    "        temp = temp[:-1]\n",
    "        add_candidate(temp, conf)\n",
    "        conf -= 0.05\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def extract_pn_from_ocr(ocr_lines: List[str]) -> List[IdentityCandidate]:\n",
    "    \"\"\"\n",
    "    Extract part number candidates from OCR text.\n",
    "    Looks for text near labels like PART NO, DWG NO, ID, etc.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    tried = set()\n",
    "\n",
    "    # Labels that typically precede part numbers\n",
    "    pn_labels = [\n",
    "        r'PART\\s*(?:NO|NUMBER|#)[.:]?\\s*',\n",
    "        r'DWG\\s*(?:NO|NUMBER|#)[.:]?\\s*',\n",
    "        r'DRAWING\\s*(?:NO|NUMBER|#)[.:]?\\s*',\n",
    "        r'P/?N[.:]?\\s*',\n",
    "        r'ID[.:]?\\s*',\n",
    "        r'ITEM\\s*(?:NO|#)[.:]?\\s*',\n",
    "    ]\n",
    "\n",
    "    def add_candidate(val: str, source: str, conf: float):\n",
    "        val = val.strip()\n",
    "        if val and val not in tried and len(val) >= 4:\n",
    "            # Check if it looks like a part number (has digits)\n",
    "            if not any(c.isdigit() for c in val):\n",
    "                return\n",
    "            tried.add(val)\n",
    "            match = sw_library.lookup(val) if sw_library else None\n",
    "            candidates.append(IdentityCandidate(\n",
    "                value=val,\n",
    "                source=source,\n",
    "                confidence=conf,\n",
    "                match_found=match is not None,\n",
    "                json_path=match.json_path if match else None\n",
    "            ))\n",
    "\n",
    "    full_text = \"\\n\".join(ocr_lines)\n",
    "\n",
    "    # Search for labeled part numbers\n",
    "    for pattern in pn_labels:\n",
    "        matches = re.finditer(pattern + r'([A-Z0-9][-A-Z0-9]{3,20})', full_text, re.IGNORECASE)\n",
    "        for m in matches:\n",
    "            add_candidate(m.group(1), \"ocr_near_label\", 0.85)\n",
    "\n",
    "    # Look for standalone part number patterns\n",
    "    pn_patterns = [\n",
    "        r'\\b(\\d{6,8})\\b',  # 6-8 digit numbers\n",
    "        r'\\b(\\d{5,7}[A-Z])\\b',  # Digits + letter suffix\n",
    "        r'\\b([A-Z]{1,3}\\d{5,8})\\b',  # Letter prefix + digits\n",
    "    ]\n",
    "\n",
    "    for pattern in pn_patterns:\n",
    "        matches = re.finditer(pattern, full_text)\n",
    "        for m in matches:\n",
    "            add_candidate(m.group(1), \"ocr_title_block\", 0.6)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def resolve_part_identity(\n",
    "    pdf_path: str,\n",
    "    ocr_lines: Optional[List[str]] = None\n",
    ") -> ResolvedPartIdentity:\n",
    "    \"\"\"\n",
    "    Resolve part identity from filename and OCR.\n",
    "\n",
    "    Returns ResolvedPartIdentity with best match and alternatives.\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    # Get candidates from filename\n",
    "    filename_candidates = extract_pn_from_filename(pdf_path)\n",
    "\n",
    "    # Get candidates from OCR\n",
    "    ocr_candidates = extract_pn_from_ocr(ocr_lines) if ocr_lines else []\n",
    "\n",
    "    # Combine all candidates\n",
    "    all_candidates = filename_candidates + ocr_candidates\n",
    "\n",
    "    # Find best match (prefer matches found in library)\n",
    "    matched = [c for c in all_candidates if c.match_found]\n",
    "    unmatched = [c for c in all_candidates if not c.match_found]\n",
    "\n",
    "    # Sort by confidence\n",
    "    matched.sort(key=lambda x: x.confidence, reverse=True)\n",
    "    unmatched.sort(key=lambda x: x.confidence, reverse=True)\n",
    "\n",
    "    best_candidate = None\n",
    "    confidence = 0.0\n",
    "    needs_review = False\n",
    "\n",
    "    if matched:\n",
    "        best_candidate = matched[0]\n",
    "        confidence = best_candidate.confidence\n",
    "        notes.append(f\"Matched '{best_candidate.value}' from {best_candidate.source}\")\n",
    "\n",
    "        # Check for conflicting matches\n",
    "        if len(matched) > 1:\n",
    "            unique_pns = set(c.value for c in matched)\n",
    "            if len(unique_pns) > 1:\n",
    "                notes.append(f\"Multiple matches found: {unique_pns}\")\n",
    "                needs_review = True\n",
    "                confidence *= 0.7\n",
    "    else:\n",
    "        notes.append(\"No match found in SolidWorks library\")\n",
    "        needs_review = True\n",
    "        if unmatched:\n",
    "            best_candidate = unmatched[0]\n",
    "            confidence = best_candidate.confidence * 0.5  # Reduce confidence\n",
    "            notes.append(f\"Best unmatched candidate: '{best_candidate.value}'\")\n",
    "\n",
    "    # Low confidence threshold\n",
    "    if confidence < 0.5:\n",
    "        needs_review = True\n",
    "        notes.append(\"Low confidence - needs manual review\")\n",
    "\n",
    "    return ResolvedPartIdentity(\n",
    "        resolvedPartNumber=best_candidate.value if best_candidate else None,\n",
    "        jsonPath=best_candidate.json_path if best_candidate else None,\n",
    "        confidence=confidence,\n",
    "        candidates={\n",
    "            \"fromFilename\": [asdict(c) for c in filename_candidates],\n",
    "            \"fromOcrTitleBlock\": [asdict(c) for c in ocr_candidates],\n",
    "            \"matchesFound\": [asdict(c) for c in matched]\n",
    "        },\n",
    "        notes=notes,\n",
    "        needsReview=needs_review\n",
    "    )\n",
    "\n",
    "print(\"Part identity resolution functions defined.\")"
   ],
   "outputs": [],
   "id": "pTuU_zs03Qt7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8651OOFB3Qt7"
   },
   "source": [
    "---\n",
    "## Section 4: OCR Evidence Extraction"
   ],
   "id": "8651OOFB3Qt7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkxhNzvI3Qt8"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 4A: Load LightOnOCR-2 Model\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Loading LightOnOCR-2-1B...\")\n",
    "\n",
    "ocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n",
    "\n",
    "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    torch_dtype=ocr_dtype,\n",
    "    token=hf_token\n",
    ").to(ocr_device)\n",
    "\n",
    "ocr_processor = LightOnOcrProcessor.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "id": "LkxhNzvI3Qt8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GHBtfng3Qt8"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 4B: OCR Block Extraction with Real Bounding Boxes\n",
    "# ============================================================\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class OcrBlock:\n",
    "    \"\"\"Single OCR text block with precise location.\"\"\"\n",
    "    text: str\n",
    "    confidence: float\n",
    "    pageIndex0: int\n",
    "    bbox: Optional[Dict[str, float]] = None  # Normalized 0-1 coords: x, y, width, height\n",
    "    line_index: int = 0\n",
    "    rawBbox: Optional[Dict[str, int]] = None  # Pixel coordinates for debugging\n",
    "\n",
    "\n",
    "def run_tesseract_ocr(image: Image.Image, page_index: int) -> List[OcrBlock]:\n",
    "    \"\"\"\n",
    "    Run Tesseract OCR to get text with real bounding boxes.\n",
    "    Returns OcrBlocks with precise x,y,width,height coordinates.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "\n",
    "    # Convert to RGB if needed\n",
    "    img = image.convert(\"RGB\")\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    # Get OCR data with bounding boxes\n",
    "    # Output format: dict with 'text', 'conf', 'left', 'top', 'width', 'height', 'level'\n",
    "    try:\n",
    "        ocr_data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n",
    "    except Exception as e:\n",
    "        print(f\"    Tesseract error: {e}\")\n",
    "        return blocks\n",
    "\n",
    "    n_boxes = len(ocr_data['text'])\n",
    "\n",
    "    # Group words into lines (level 4 = word)\n",
    "    current_line_y = -1\n",
    "    current_line_blocks = []\n",
    "    line_index = 0\n",
    "\n",
    "    for i in range(n_boxes):\n",
    "        text = ocr_data['text'][i].strip()\n",
    "        conf = int(ocr_data['conf'][i])\n",
    "        level = ocr_data['level'][i]\n",
    "\n",
    "        # Skip empty text or very low confidence\n",
    "        if not text or conf < 10:\n",
    "            continue\n",
    "\n",
    "        # Get pixel coordinates\n",
    "        x_px = ocr_data['left'][i]\n",
    "        y_px = ocr_data['top'][i]\n",
    "        w_px = ocr_data['width'][i]\n",
    "        h_px = ocr_data['height'][i]\n",
    "\n",
    "        # Normalize to 0-1 range\n",
    "        x_norm = x_px / img_width\n",
    "        y_norm = y_px / img_height\n",
    "        w_norm = w_px / img_width\n",
    "        h_norm = h_px / img_height\n",
    "\n",
    "        # Detect line breaks (significant Y change = new line)\n",
    "        if current_line_y < 0:\n",
    "            current_line_y = y_px\n",
    "        elif abs(y_px - current_line_y) > h_px * 0.5:\n",
    "            # New line detected - flush current line\n",
    "            if current_line_blocks:\n",
    "                line_index += 1\n",
    "            current_line_y = y_px\n",
    "\n",
    "        block = OcrBlock(\n",
    "            text=text,\n",
    "            confidence=conf / 100.0,  # Normalize to 0-1\n",
    "            pageIndex0=page_index,\n",
    "            bbox={\n",
    "                \"x\": round(x_norm, 4),\n",
    "                \"y\": round(y_norm, 4),\n",
    "                \"width\": round(w_norm, 4),\n",
    "                \"height\": round(h_norm, 4)\n",
    "            },\n",
    "            rawBbox={\n",
    "                \"x\": x_px,\n",
    "                \"y\": y_px,\n",
    "                \"width\": w_px,\n",
    "                \"height\": h_px\n",
    "            },\n",
    "            line_index=line_index\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def merge_words_to_lines(blocks: List[OcrBlock], y_tolerance: float = 0.015) -> List[OcrBlock]:\n",
    "    \"\"\"\n",
    "    Merge individual word blocks into line blocks based on Y proximity.\n",
    "    Words on the same line (similar Y) are combined.\n",
    "    \"\"\"\n",
    "    if not blocks:\n",
    "        return []\n",
    "\n",
    "    # Sort by Y then X\n",
    "    sorted_blocks = sorted(blocks, key=lambda b: (b.bbox['y'], b.bbox['x']))\n",
    "\n",
    "    merged_lines = []\n",
    "    current_line_words = [sorted_blocks[0]]\n",
    "    current_y = sorted_blocks[0].bbox['y']\n",
    "\n",
    "    for block in sorted_blocks[1:]:\n",
    "        if abs(block.bbox['y'] - current_y) <= y_tolerance:\n",
    "            # Same line\n",
    "            current_line_words.append(block)\n",
    "        else:\n",
    "            # New line - merge current line\n",
    "            if current_line_words:\n",
    "                merged_lines.append(merge_line_words(current_line_words, len(merged_lines)))\n",
    "            current_line_words = [block]\n",
    "            current_y = block.bbox['y']\n",
    "\n",
    "    # Don't forget last line\n",
    "    if current_line_words:\n",
    "        merged_lines.append(merge_line_words(current_line_words, len(merged_lines)))\n",
    "\n",
    "    return merged_lines\n",
    "\n",
    "\n",
    "def merge_line_words(words: List[OcrBlock], line_idx: int) -> OcrBlock:\n",
    "    \"\"\"Merge multiple word blocks into a single line block.\"\"\"\n",
    "    if not words:\n",
    "        return None\n",
    "\n",
    "    # Sort words left to right\n",
    "    words = sorted(words, key=lambda w: w.bbox['x'])\n",
    "\n",
    "    # Combine text\n",
    "    text = \" \".join(w.text for w in words)\n",
    "\n",
    "    # Average confidence\n",
    "    avg_conf = sum(w.confidence for w in words) / len(words)\n",
    "\n",
    "    # Calculate bounding box that encompasses all words\n",
    "    min_x = min(w.bbox['x'] for w in words)\n",
    "    min_y = min(w.bbox['y'] for w in words)\n",
    "    max_x = max(w.bbox['x'] + w.bbox['width'] for w in words)\n",
    "    max_y = max(w.bbox['y'] + w.bbox['height'] for w in words)\n",
    "\n",
    "    return OcrBlock(\n",
    "        text=text,\n",
    "        confidence=avg_conf,\n",
    "        pageIndex0=words[0].pageIndex0,\n",
    "        bbox={\n",
    "            \"x\": round(min_x, 4),\n",
    "            \"y\": round(min_y, 4),\n",
    "            \"width\": round(max_x - min_x, 4),\n",
    "            \"height\": round(max_y - min_y, 4)\n",
    "        },\n",
    "        rawBbox=None,  # Not preserved after merge\n",
    "        line_index=line_idx\n",
    "    )\n",
    "\n",
    "\n",
    "def run_ocr_on_page(page_artifact: PageArtifact, use_lighton: bool = True) -> List[OcrBlock]:\n",
    "    \"\"\"\n",
    "    Run OCR on a page and return OcrBlocks with real bounding boxes.\n",
    "\n",
    "    Uses Tesseract for bounding boxes. Optionally uses LightOnOCR-2\n",
    "    for enhanced text recognition (hybrid approach).\n",
    "    \"\"\"\n",
    "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
    "\n",
    "    # Step 1: Run Tesseract to get bounding boxes\n",
    "    word_blocks = run_tesseract_ocr(page_artifact.image, page_artifact.pageIndex0)\n",
    "\n",
    "    if not word_blocks:\n",
    "        print(f\"    No text found by Tesseract on page {page_artifact.page}\")\n",
    "        return []\n",
    "\n",
    "    # Step 2: Merge words into lines\n",
    "    line_blocks = merge_words_to_lines(word_blocks, y_tolerance=0.012)\n",
    "\n",
    "    # Step 3 (Optional): Enhance with LightOnOCR-2 for better text quality\n",
    "    if use_lighton and ocr_model is not None:\n",
    "        try:\n",
    "            img = page_artifact.image.convert(\"RGB\")\n",
    "\n",
    "            conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n",
    "            inputs = ocr_processor.apply_chat_template(\n",
    "                conversation, add_generation_prompt=True, tokenize=True,\n",
    "                return_dict=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {\n",
    "                k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device)\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "            generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "            lighton_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "            lighton_lines = [line.strip() for line in lighton_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "            # Try to match LightOnOCR text to Tesseract bboxes\n",
    "            line_blocks = align_lighton_with_tesseract(line_blocks, lighton_lines)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    LightOnOCR enhancement failed: {e}\")\n",
    "            # Fall back to Tesseract-only results\n",
    "\n",
    "    return line_blocks\n",
    "\n",
    "\n",
    "def align_lighton_with_tesseract(\n",
    "    tesseract_blocks: List[OcrBlock],\n",
    "    lighton_lines: List[str]\n",
    ") -> List[OcrBlock]:\n",
    "    \"\"\"\n",
    "    Align LightOnOCR text with Tesseract bounding boxes.\n",
    "    LightOnOCR often has better text quality, Tesseract has bboxes.\n",
    "\n",
    "    Strategy: fuzzy match LightOnOCR lines to Tesseract lines,\n",
    "    use LightOnOCR text with Tesseract bbox when confident.\n",
    "    \"\"\"\n",
    "    if not lighton_lines or not tesseract_blocks:\n",
    "        return tesseract_blocks\n",
    "\n",
    "    result = []\n",
    "    used_lighton = set()\n",
    "\n",
    "    for tess_block in tesseract_blocks:\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        best_idx = -1\n",
    "\n",
    "        tess_lower = tess_block.text.lower().replace(\" \", \"\")\n",
    "\n",
    "        for i, lo_line in enumerate(lighton_lines):\n",
    "            if i in used_lighton:\n",
    "                continue\n",
    "\n",
    "            lo_lower = lo_line.lower().replace(\" \", \"\")\n",
    "\n",
    "            # Calculate similarity (simple character overlap)\n",
    "            if len(tess_lower) < 3 or len(lo_lower) < 3:\n",
    "                continue\n",
    "\n",
    "            # Check character overlap ratio\n",
    "            common = sum(1 for c in tess_lower if c in lo_lower)\n",
    "            score = common / max(len(tess_lower), len(lo_lower))\n",
    "\n",
    "            if score > best_score and score > 0.6:\n",
    "                best_score = score\n",
    "                best_match = lo_line\n",
    "                best_idx = i\n",
    "\n",
    "        if best_match and best_score > 0.7:\n",
    "            # Use LightOnOCR text with Tesseract bbox\n",
    "            used_lighton.add(best_idx)\n",
    "            result.append(OcrBlock(\n",
    "                text=best_match,\n",
    "                confidence=max(tess_block.confidence, 0.9),  # Boost confidence for LightOnOCR\n",
    "                pageIndex0=tess_block.pageIndex0,\n",
    "                bbox=tess_block.bbox,\n",
    "                rawBbox=tess_block.rawBbox,\n",
    "                line_index=tess_block.line_index\n",
    "            ))\n",
    "        else:\n",
    "            # Keep original Tesseract result\n",
    "            result.append(tess_block)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_ocr_on_all_pages(page_artifacts: List[PageArtifact]) -> List[OcrBlock]:\n",
    "    \"\"\"Run OCR on all pages and return combined blocks with real bounding boxes.\"\"\"\n",
    "    all_blocks = []\n",
    "\n",
    "    for artifact in page_artifacts:\n",
    "        print(f\"  Running OCR on page {artifact.page}...\")\n",
    "        blocks = run_ocr_on_page(artifact)\n",
    "        all_blocks.extend(blocks)\n",
    "\n",
    "        # Stats\n",
    "        avg_conf = sum(b.confidence for b in blocks) / len(blocks) if blocks else 0\n",
    "        print(f\"    Found {len(blocks)} text lines, avg confidence: {avg_conf:.0%}\")\n",
    "\n",
    "    return all_blocks\n",
    "\n",
    "print(\"OCR extraction with real bounding boxes defined.\")"
   ],
   "outputs": [],
   "id": "3GHBtfng3Qt8"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# SECTION 4C: Title Block & Notes Region ROI OCR\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Engineering drawings typically have:\n",
    "- Title block: bottom-right corner (20-25% width, 15-20% height)\n",
    "- General notes: left side or top-left (varies)\n",
    "- Revision block: top-right or adjacent to title block\n",
    "\n",
    "This section detects these regions and runs targeted OCR for:\n",
    "- Part number, description, material, finish\n",
    "- Drawing units (INCHES/MM)\n",
    "- Revision info\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class DrawingRegion:\n",
    "    \"\"\"A detected region of interest on a drawing.\"\"\"\n",
    "    region_type: str  # \"title_block\", \"notes\", \"revision_block\"\n",
    "    bbox_norm: Dict[str, float]  # Normalized 0-1 coords\n",
    "    bbox_px: Dict[str, int]  # Pixel coords\n",
    "    confidence: float\n",
    "    ocr_text: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "# Common title block locations (normalized coordinates)\n",
    "# Format: (x_start, y_start, width, height) as fractions of image size\n",
    "TITLE_BLOCK_REGIONS = [\n",
    "    # Standard bottom-right title block (most common)\n",
    "    {\"name\": \"bottom_right_large\", \"x\": 0.60, \"y\": 0.75, \"w\": 0.40, \"h\": 0.25},\n",
    "    {\"name\": \"bottom_right_medium\", \"x\": 0.65, \"y\": 0.80, \"w\": 0.35, \"h\": 0.20},\n",
    "    {\"name\": \"bottom_right_small\", \"x\": 0.70, \"y\": 0.85, \"w\": 0.30, \"h\": 0.15},\n",
    "    # Full bottom strip (for wide title blocks)\n",
    "    {\"name\": \"bottom_strip\", \"x\": 0.0, \"y\": 0.85, \"w\": 1.0, \"h\": 0.15},\n",
    "]\n",
    "\n",
    "NOTES_REGIONS = [\n",
    "    # Left side notes column\n",
    "    {\"name\": \"left_notes\", \"x\": 0.0, \"y\": 0.0, \"w\": 0.25, \"h\": 0.70},\n",
    "    # Top-left notes area\n",
    "    {\"name\": \"top_left_notes\", \"x\": 0.0, \"y\": 0.0, \"w\": 0.40, \"h\": 0.25},\n",
    "    # Right side notes (less common)\n",
    "    {\"name\": \"right_notes\", \"x\": 0.75, \"y\": 0.0, \"w\": 0.25, \"h\": 0.60},\n",
    "]\n",
    "\n",
    "\n",
    "def detect_title_block_region(\n",
    "    page_artifact: PageArtifact,\n",
    "    ocr_blocks: List[OcrBlock]\n",
    ") -> Optional[DrawingRegion]:\n",
    "    \"\"\"\n",
    "    Detect title block region by looking for keyword clusters.\n",
    "\n",
    "    Title blocks typically contain: PART NO, DWG, MATERIAL, SCALE, etc.\n",
    "    \"\"\"\n",
    "    title_keywords = [\n",
    "        'PART', 'DWG', 'DRAWING', 'MATERIAL', 'SCALE', 'SHEET',\n",
    "        'FINISH', 'TITLE', 'REV', 'DATE', 'DRAWN', 'CHECKED',\n",
    "        'APPROVED', 'SIZE', 'CAGE', 'FSCM', 'P/N', 'UNLESS'\n",
    "    ]\n",
    "\n",
    "    img_w, img_h = page_artifact.width, page_artifact.height\n",
    "\n",
    "    # Try each predefined region\n",
    "    best_region = None\n",
    "    best_score = 0\n",
    "\n",
    "    for region_def in TITLE_BLOCK_REGIONS:\n",
    "        # Count keyword matches in this region\n",
    "        keyword_count = 0\n",
    "        text_blocks_in_region = []\n",
    "\n",
    "        for block in ocr_blocks:\n",
    "            if block.pageIndex0 != page_artifact.pageIndex0:\n",
    "                continue\n",
    "\n",
    "            bx = block.bbox.get('x', 0)\n",
    "            by = block.bbox.get('y', 0)\n",
    "\n",
    "            # Check if block is within region\n",
    "            if (region_def['x'] <= bx <= region_def['x'] + region_def['w'] and\n",
    "                region_def['y'] <= by <= region_def['y'] + region_def['h']):\n",
    "\n",
    "                text_blocks_in_region.append(block.text)\n",
    "                text_upper = block.text.upper()\n",
    "\n",
    "                for kw in title_keywords:\n",
    "                    if kw in text_upper:\n",
    "                        keyword_count += 1\n",
    "\n",
    "        # Score based on keyword density\n",
    "        if text_blocks_in_region:\n",
    "            score = keyword_count / len(text_blocks_in_region)\n",
    "            if keyword_count >= 3 and score > best_score:\n",
    "                best_score = score\n",
    "                best_region = DrawingRegion(\n",
    "                    region_type=\"title_block\",\n",
    "                    bbox_norm={\n",
    "                        \"x\": region_def['x'],\n",
    "                        \"y\": region_def['y'],\n",
    "                        \"width\": region_def['w'],\n",
    "                        \"height\": region_def['h']\n",
    "                    },\n",
    "                    bbox_px={\n",
    "                        \"x\": int(region_def['x'] * img_w),\n",
    "                        \"y\": int(region_def['y'] * img_h),\n",
    "                        \"width\": int(region_def['w'] * img_w),\n",
    "                        \"height\": int(region_def['h'] * img_h)\n",
    "                    },\n",
    "                    confidence=min(score, 0.95),\n",
    "                    ocr_text=text_blocks_in_region\n",
    "                )\n",
    "\n",
    "    return best_region\n",
    "\n",
    "\n",
    "def detect_notes_region(\n",
    "    page_artifact: PageArtifact,\n",
    "    ocr_blocks: List[OcrBlock]\n",
    ") -> Optional[DrawingRegion]:\n",
    "    \"\"\"\n",
    "    Detect general notes region by looking for note patterns.\n",
    "\n",
    "    Notes sections typically contain: NOTES:, GENERAL NOTES, 1., 2., etc.\n",
    "    \"\"\"\n",
    "    notes_indicators = [\n",
    "        'NOTES', 'GENERAL NOTES', 'UNLESS OTHERWISE', 'ALL DIMENSIONS',\n",
    "        'REMOVE BURRS', 'BREAK EDGES', 'TOLERANCES', 'INTERPRET'\n",
    "    ]\n",
    "\n",
    "    img_w, img_h = page_artifact.width, page_artifact.height\n",
    "\n",
    "    best_region = None\n",
    "    best_score = 0\n",
    "\n",
    "    for region_def in NOTES_REGIONS:\n",
    "        indicator_count = 0\n",
    "        numbered_lines = 0\n",
    "        text_blocks_in_region = []\n",
    "\n",
    "        for block in ocr_blocks:\n",
    "            if block.pageIndex0 != page_artifact.pageIndex0:\n",
    "                continue\n",
    "\n",
    "            bx = block.bbox.get('x', 0)\n",
    "            by = block.bbox.get('y', 0)\n",
    "\n",
    "            if (region_def['x'] <= bx <= region_def['x'] + region_def['w'] and\n",
    "                region_def['y'] <= by <= region_def['y'] + region_def['h']):\n",
    "\n",
    "                text_blocks_in_region.append(block.text)\n",
    "                text_upper = block.text.upper()\n",
    "\n",
    "                for ind in notes_indicators:\n",
    "                    if ind in text_upper:\n",
    "                        indicator_count += 1\n",
    "\n",
    "                # Check for numbered lists (1., 2., A., B.)\n",
    "                if re.match(r'^[\\d]{1,2}[.\\)]', block.text.strip()):\n",
    "                    numbered_lines += 1\n",
    "\n",
    "        score = indicator_count + (numbered_lines * 0.5)\n",
    "        if text_blocks_in_region and score > best_score and indicator_count >= 1:\n",
    "            best_score = score\n",
    "            best_region = DrawingRegion(\n",
    "                region_type=\"notes\",\n",
    "                bbox_norm={\n",
    "                    \"x\": region_def['x'],\n",
    "                    \"y\": region_def['y'],\n",
    "                    \"width\": region_def['w'],\n",
    "                    \"height\": region_def['h']\n",
    "                },\n",
    "                bbox_px={\n",
    "                    \"x\": int(region_def['x'] * img_w),\n",
    "                    \"y\": int(region_def['y'] * img_h),\n",
    "                    \"width\": int(region_def['w'] * img_w),\n",
    "                    \"height\": int(region_def['h'] * img_h)\n",
    "                },\n",
    "                confidence=min(score / 5, 0.9),\n",
    "                ocr_text=text_blocks_in_region\n",
    "            )\n",
    "\n",
    "    return best_region\n",
    "\n",
    "\n",
    "def run_roi_ocr(\n",
    "    page_artifact: PageArtifact,\n",
    "    region: DrawingRegion\n",
    ") -> List[OcrBlock]:\n",
    "    \"\"\"\n",
    "    Run targeted OCR on a specific region (cropped image).\n",
    "    Higher resolution OCR on smaller region for better accuracy.\n",
    "    \"\"\"\n",
    "    img = page_artifact.image\n",
    "\n",
    "    # Crop region (with small padding)\n",
    "    px = region.bbox_px\n",
    "    pad = 10\n",
    "    x1 = max(0, px['x'] - pad)\n",
    "    y1 = max(0, px['y'] - pad)\n",
    "    x2 = min(img.width, px['x'] + px['width'] + pad)\n",
    "    y2 = min(img.height, px['y'] + px['height'] + pad)\n",
    "\n",
    "    cropped = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # Run Tesseract on cropped region with better config for small text\n",
    "    try:\n",
    "        # Use PSM 6 (uniform block of text) for better small text recognition\n",
    "        custom_config = r'--oem 3 --psm 6'\n",
    "        ocr_data = pytesseract.image_to_data(\n",
    "            cropped,\n",
    "            output_type=pytesseract.Output.DICT,\n",
    "            config=custom_config\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"    ROI OCR error: {e}\")\n",
    "        return []\n",
    "\n",
    "    blocks = []\n",
    "    crop_w, crop_h = cropped.size\n",
    "\n",
    "    for i in range(len(ocr_data['text'])):\n",
    "        text = ocr_data['text'][i].strip()\n",
    "        conf = int(ocr_data['conf'][i])\n",
    "\n",
    "        if not text or conf < 20:\n",
    "            continue\n",
    "\n",
    "        # Convert crop-relative coords back to full image coords\n",
    "        local_x = ocr_data['left'][i]\n",
    "        local_y = ocr_data['top'][i]\n",
    "        w = ocr_data['width'][i]\n",
    "        h = ocr_data['height'][i]\n",
    "\n",
    "        global_x = x1 + local_x\n",
    "        global_y = y1 + local_y\n",
    "\n",
    "        # Normalize to full image\n",
    "        x_norm = global_x / img.width\n",
    "        y_norm = global_y / img.height\n",
    "        w_norm = w / img.width\n",
    "        h_norm = h / img.height\n",
    "\n",
    "        blocks.append(OcrBlock(\n",
    "            text=text,\n",
    "            confidence=conf / 100.0,\n",
    "            pageIndex0=page_artifact.pageIndex0,\n",
    "            bbox={\n",
    "                \"x\": round(x_norm, 4),\n",
    "                \"y\": round(y_norm, 4),\n",
    "                \"width\": round(w_norm, 4),\n",
    "                \"height\": round(h_norm, 4)\n",
    "            },\n",
    "            rawBbox={\n",
    "                \"x\": global_x,\n",
    "                \"y\": global_y,\n",
    "                \"width\": w,\n",
    "                \"height\": h\n",
    "            },\n",
    "            line_index=i\n",
    "        ))\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def extract_identity_from_title_block(\n",
    "    title_block_region: Optional[DrawingRegion],\n",
    "    all_ocr_blocks: List[OcrBlock]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured identity info from title block region.\n",
    "\n",
    "    Returns dict with: partNumber, description, material, finish, scale, etc.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"partNumber\": None,\n",
    "        \"description\": None,\n",
    "        \"material\": None,\n",
    "        \"finish\": None,\n",
    "        \"scale\": None,\n",
    "        \"revision\": None,\n",
    "        \"sheetInfo\": None,\n",
    "        \"rawTitleBlockText\": []\n",
    "    }\n",
    "\n",
    "    if not title_block_region:\n",
    "        return result\n",
    "\n",
    "    # Get text from title block region\n",
    "    title_text_lines = title_block_region.ocr_text or []\n",
    "    result[\"rawTitleBlockText\"] = title_text_lines\n",
    "\n",
    "    full_text = \" \".join(title_text_lines).upper()\n",
    "\n",
    "    # Part number patterns\n",
    "    pn_patterns = [\n",
    "        r'PART\\s*(?:NO|NUMBER|#)[.:)]*\\s*([A-Z0-9][-A-Z0-9]{3,20})',\n",
    "        r'P/?N[.:)]*\\s*([A-Z0-9][-A-Z0-9]{3,20})',\n",
    "        r'DWG\\s*(?:NO|NUMBER|#)?[.:)]*\\s*([A-Z0-9][-A-Z0-9]{3,20})',\n",
    "        r'DRAWING\\s*(?:NO|#)?[.:)]*\\s*([A-Z0-9][-A-Z0-9]{3,20})',\n",
    "        r'ID[.:)]*\\s*(\\d{5,10})',\n",
    "    ]\n",
    "\n",
    "    for pattern in pn_patterns:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            result[\"partNumber\"] = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # Material patterns\n",
    "    material_patterns = [\n",
    "        r'MATERIAL[.:)]*\\s*([A-Z0-9][A-Z0-9\\s\\-]{2,30}?)(?=\\s*(?:FINISH|SCALE|SHEET|$))',\n",
    "        r'MAT[\\'L]*[.:)]*\\s*([A-Z0-9][A-Z0-9\\s\\-]{2,30})',\n",
    "        r'(?:AISI|SAE|ASTM)\\s*[A-Z]?\\d{3,5}',\n",
    "        r'(?:STEEL|ALUMINUM|BRASS|BRONZE|COPPER|PLASTIC|NYLON|DELRIN)',\n",
    "    ]\n",
    "\n",
    "    for pattern in material_patterns:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            result[\"material\"] = match.group(1).strip() if match.lastindex else match.group(0).strip()\n",
    "            break\n",
    "\n",
    "    # Finish patterns\n",
    "    finish_patterns = [\n",
    "        r'FINISH[.:)]*\\s*([A-Z][A-Z\\s\\-]{2,30}?)(?=\\s*(?:MATERIAL|SCALE|SHEET|$))',\n",
    "        r'(PAINT\\s*\\w+)',\n",
    "        r'(ZINC\\s*PLATE)',\n",
    "        r'(ANODIZE[D]*)',\n",
    "        r'(POWDER\\s*COAT)',\n",
    "        r'(BLACK\\s*OXIDE)',\n",
    "    ]\n",
    "\n",
    "    for pattern in finish_patterns:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            result[\"finish\"] = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # Scale pattern\n",
    "    scale_match = re.search(r'SCALE[.:)]*\\s*(\\d+[.:]\\d+|\\d+/\\d+|FULL|NTS)', full_text)\n",
    "    if scale_match:\n",
    "        result[\"scale\"] = scale_match.group(1)\n",
    "\n",
    "    # Revision pattern\n",
    "    rev_match = re.search(r'REV(?:ISION)?[.:)]*\\s*([A-Z0-9]{1,3})', full_text)\n",
    "    if rev_match:\n",
    "        result[\"revision\"] = rev_match.group(1)\n",
    "\n",
    "    # Sheet info\n",
    "    sheet_match = re.search(r'SHEET\\s*(\\d+)\\s*(?:OF|/)\\s*(\\d+)', full_text)\n",
    "    if sheet_match:\n",
    "        result[\"sheetInfo\"] = f\"{sheet_match.group(1)} of {sheet_match.group(2)}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_notes_content(\n",
    "    notes_region: Optional[DrawingRegion],\n",
    "    all_ocr_blocks: List[OcrBlock]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract structured notes from notes region.\n",
    "\n",
    "    Returns list of note dicts with type and content.\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    if not notes_region:\n",
    "        return notes\n",
    "\n",
    "    text_lines = notes_region.ocr_text or []\n",
    "\n",
    "    # Common note patterns\n",
    "    general_tolerance_pattern = r'UNLESS\\s+OTHERWISE\\s+SPECIFIED'\n",
    "    material_note_pattern = r'MATERIAL[.:]*\\s*(.+)'\n",
    "    finish_note_pattern = r'FINISH[.:]*\\s*(.+)'\n",
    "\n",
    "    full_text = \" \".join(text_lines).upper()\n",
    "\n",
    "    # Check for tolerance block\n",
    "    if re.search(general_tolerance_pattern, full_text):\n",
    "        notes.append({\n",
    "            \"noteType\": \"GeneralTolerance\",\n",
    "            \"rawText\": full_text[:200],\n",
    "            \"confidence\": 0.8\n",
    "        })\n",
    "\n",
    "    # Extract numbered notes\n",
    "    for line in text_lines:\n",
    "        line = line.strip()\n",
    "        numbered_match = re.match(r'^(\\d{1,2})[.\\)]\\s*(.+)', line)\n",
    "        if numbered_match:\n",
    "            notes.append({\n",
    "                \"noteType\": \"Numbered\",\n",
    "                \"number\": int(numbered_match.group(1)),\n",
    "                \"rawText\": numbered_match.group(2),\n",
    "                \"confidence\": 0.75\n",
    "            })\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "print(\"Title block & notes ROI detection defined.\")"
   ],
   "metadata": {
    "id": "JLQpqZdy3Qt8"
   },
   "execution_count": null,
   "outputs": [],
   "id": "JLQpqZdy3Qt8"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# SECTION 4D: Unit Detection & Normalization\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Engineering drawings specify units in:\n",
    "- Title block: \"UNLESS OTHERWISE SPECIFIED, DIMENSIONS ARE IN INCHES\"\n",
    "- Notes: \"ALL DIMENSIONS IN MM\"\n",
    "- Scale annotation: \"1:1 METRIC\" or \"SCALE: 1=1 INCH\"\n",
    "\n",
    "This section:\n",
    "1. Detects the drawing's native unit system (INCH or MM)\n",
    "2. Provides conversion functions to normalize all values to MM\n",
    "\"\"\"\n",
    "\n",
    "from enum import Enum\n",
    "from typing import NamedTuple\n",
    "\n",
    "class DrawingUnits(Enum):\n",
    "    UNKNOWN = \"unknown\"\n",
    "    INCHES = \"inches\"\n",
    "    MILLIMETERS = \"mm\"\n",
    "    MIXED = \"mixed\"  # Some drawings have both\n",
    "\n",
    "\n",
    "class UnitDetectionResult(NamedTuple):\n",
    "    detected_unit: DrawingUnits\n",
    "    confidence: float\n",
    "    evidence: List[str]\n",
    "    conversion_factor: float  # Multiply raw values by this to get MM\n",
    "\n",
    "\n",
    "# Conversion constant\n",
    "INCH_TO_MM = 25.4\n",
    "\n",
    "# Patterns indicating INCH units\n",
    "INCH_PATTERNS = [\n",
    "    r'DIMENSIONS\\s+(?:ARE\\s+)?IN\\s+INCH(?:ES)?',\n",
    "    r'ALL\\s+DIMENSIONS\\s+IN\\s+INCH(?:ES)?',\n",
    "    r'UNLESS\\s+OTHERWISE\\s+SPECIFIED[^.]*INCH(?:ES)?',\n",
    "    r'SCALE[:\\s]+\\d+[:\\s]*\\d*\\s*INCH',\n",
    "    r'\\.XXX\\s*=\\s*[\u00b1]?\\s*\\.00[1-5]',  # Tolerance format typical for inches\n",
    "    r'FRACTIONAL[:\\s]+[\u00b1]?\\s*1/\\d+',   # Fractional tolerances\n",
    "    r'INTERPRET\\s+(?:PER|IAW)\\s+ASME\\s+Y14\\.5',  # US standard usually means inches\n",
    "]\n",
    "\n",
    "# Patterns indicating MM units\n",
    "MM_PATTERNS = [\n",
    "    r'DIMENSIONS\\s+(?:ARE\\s+)?IN\\s+(?:MM|MILLIMETER)',\n",
    "    r'ALL\\s+DIMENSIONS\\s+IN\\s+(?:MM|MILLIMETER)',\n",
    "    r'UNLESS\\s+OTHERWISE\\s+SPECIFIED[^.]*(?:MM|MILLIMETER)',\n",
    "    r'SCALE[:\\s]+\\d+[:\\s]*\\d*\\s*(?:MM|METRIC)',\n",
    "    r'\\.X\\s*=\\s*[\u00b1]?\\s*0\\.[1-5]',  # Tolerance format typical for mm\n",
    "    r'ISO\\s+\\d{4}',  # ISO standards usually mean metric\n",
    "    r'DIN\\s+\\d{4}',  # German standard = metric\n",
    "]\n",
    "\n",
    "# Dimension value heuristics\n",
    "# Typical dimension ranges help distinguish units\n",
    "# Hole diameters: 0.1\" - 2\" (2.54mm - 50.8mm)\n",
    "# Common metric holes: M3=3mm, M4=4mm, M5=5mm, M6=6mm, M8=8mm, M10=10mm\n",
    "# Imperial drills: #1-#80 (0.228\" - 0.0135\"), Letter A-Z (0.234\" - 0.413\")\n",
    "\n",
    "\n",
    "def detect_units_from_text(ocr_text: List[str]) -> UnitDetectionResult:\n",
    "    \"\"\"\n",
    "    Detect drawing units from OCR text.\n",
    "\n",
    "    Searches for explicit unit declarations and tolerance patterns.\n",
    "    \"\"\"\n",
    "    full_text = \" \".join(ocr_text).upper()\n",
    "\n",
    "    inch_evidence = []\n",
    "    mm_evidence = []\n",
    "\n",
    "    # Check explicit patterns\n",
    "    for pattern in INCH_PATTERNS:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            inch_evidence.append(f\"Pattern match: {match.group(0)[:50]}\")\n",
    "\n",
    "    for pattern in MM_PATTERNS:\n",
    "        match = re.search(pattern, full_text)\n",
    "        if match:\n",
    "            mm_evidence.append(f\"Pattern match: {match.group(0)[:50]}\")\n",
    "\n",
    "    # Check for tolerance blocks (common format indicators)\n",
    "    # Inch format: .XX = \u00b1.01, .XXX = \u00b1.005\n",
    "    if re.search(r'\\.XX\\s*=\\s*[\u00b1]?\\s*\\.0[1-3]', full_text):\n",
    "        inch_evidence.append(\"Inch-style decimal tolerance\")\n",
    "\n",
    "    # MM format: .X = \u00b10.5, .XX = \u00b10.1\n",
    "    if re.search(r'\\.X\\s*=\\s*[\u00b1]?\\s*0\\.[1-5]', full_text):\n",
    "        mm_evidence.append(\"MM-style decimal tolerance\")\n",
    "\n",
    "    # Look for explicit \"INCH\" or \"MM\" anywhere\n",
    "    if re.search(r'\\bINCH(?:ES)?\\b', full_text) and 'MILLIMETER' not in full_text:\n",
    "        inch_evidence.append(\"Explicit INCH keyword\")\n",
    "\n",
    "    if re.search(r'\\b(?:MM|MILLIMETER(?:S)?)\\b', full_text) and 'INCH' not in full_text:\n",
    "        mm_evidence.append(\"Explicit MM keyword\")\n",
    "\n",
    "    # Determine result\n",
    "    inch_score = len(inch_evidence)\n",
    "    mm_score = len(mm_evidence)\n",
    "\n",
    "    if inch_score > 0 and mm_score == 0:\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.INCHES,\n",
    "            confidence=min(0.5 + inch_score * 0.15, 0.95),\n",
    "            evidence=inch_evidence,\n",
    "            conversion_factor=INCH_TO_MM\n",
    "        )\n",
    "    elif mm_score > 0 and inch_score == 0:\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.MILLIMETERS,\n",
    "            confidence=min(0.5 + mm_score * 0.15, 0.95),\n",
    "            evidence=mm_evidence,\n",
    "            conversion_factor=1.0\n",
    "        )\n",
    "    elif inch_score > 0 and mm_score > 0:\n",
    "        # Mixed evidence - go with stronger signal\n",
    "        if inch_score > mm_score:\n",
    "            return UnitDetectionResult(\n",
    "                detected_unit=DrawingUnits.INCHES,\n",
    "                confidence=0.5 * inch_score / (inch_score + mm_score),\n",
    "                evidence=inch_evidence + [\"Mixed signals detected\"],\n",
    "                conversion_factor=INCH_TO_MM\n",
    "            )\n",
    "        else:\n",
    "            return UnitDetectionResult(\n",
    "                detected_unit=DrawingUnits.MILLIMETERS,\n",
    "                confidence=0.5 * mm_score / (inch_score + mm_score),\n",
    "                evidence=mm_evidence + [\"Mixed signals detected\"],\n",
    "                conversion_factor=1.0\n",
    "            )\n",
    "    else:\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.UNKNOWN,\n",
    "            confidence=0.0,\n",
    "            evidence=[\"No explicit unit indicators found\"],\n",
    "            conversion_factor=1.0  # Default to no conversion\n",
    "        )\n",
    "\n",
    "\n",
    "def detect_units_from_dimensions(dimensions: List[float]) -> UnitDetectionResult:\n",
    "    \"\"\"\n",
    "    Heuristic unit detection based on dimension values.\n",
    "\n",
    "    Large values (>100) are likely MM, small values with many decimals likely INCH.\n",
    "    \"\"\"\n",
    "    if not dimensions:\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.UNKNOWN,\n",
    "            confidence=0.0,\n",
    "            evidence=[\"No dimensions to analyze\"],\n",
    "            conversion_factor=1.0\n",
    "        )\n",
    "\n",
    "    # Analyze dimension characteristics\n",
    "    large_values = sum(1 for d in dimensions if d > 50)\n",
    "    small_values = sum(1 for d in dimensions if d < 10)\n",
    "    very_small = sum(1 for d in dimensions if d < 1)\n",
    "\n",
    "    # Typical metric ranges\n",
    "    metric_hole_sizes = sum(1 for d in dimensions if d in [3, 4, 5, 6, 8, 10, 12, 14, 16, 20])\n",
    "\n",
    "    # Typical imperial (converted to decimal inches)\n",
    "    # Common fractions as decimals: 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875\n",
    "    imperial_fractions = sum(1 for d in dimensions\n",
    "                            if any(abs(d - f) < 0.01 for f in\n",
    "                                   [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875,\n",
    "                                    0.0625, 0.1875, 0.3125, 0.4375, 0.5625, 0.6875, 0.8125]))\n",
    "\n",
    "    evidence = []\n",
    "\n",
    "    # If many values > 50, likely MM\n",
    "    if large_values > len(dimensions) * 0.5:\n",
    "        evidence.append(f\"{large_values}/{len(dimensions)} values > 50 (suggests MM)\")\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.MILLIMETERS,\n",
    "            confidence=0.6,\n",
    "            evidence=evidence,\n",
    "            conversion_factor=1.0\n",
    "        )\n",
    "\n",
    "    # If values look like common imperial fractions\n",
    "    if imperial_fractions > len(dimensions) * 0.3:\n",
    "        evidence.append(f\"{imperial_fractions}/{len(dimensions)} match imperial fractions\")\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.INCHES,\n",
    "            confidence=0.6,\n",
    "            evidence=evidence,\n",
    "            conversion_factor=INCH_TO_MM\n",
    "        )\n",
    "\n",
    "    # If many very small values (< 1) and small values, likely INCH\n",
    "    if very_small > 0 and small_values > len(dimensions) * 0.7:\n",
    "        evidence.append(f\"Many small values (< 10) suggest INCH\")\n",
    "        return UnitDetectionResult(\n",
    "            detected_unit=DrawingUnits.INCHES,\n",
    "            confidence=0.5,\n",
    "            evidence=evidence,\n",
    "            conversion_factor=INCH_TO_MM\n",
    "        )\n",
    "\n",
    "    return UnitDetectionResult(\n",
    "        detected_unit=DrawingUnits.UNKNOWN,\n",
    "        confidence=0.3,\n",
    "        evidence=[\"Dimension analysis inconclusive\"],\n",
    "        conversion_factor=1.0\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_to_mm(value: float, units: DrawingUnits) -> float:\n",
    "    \"\"\"Convert a dimension value to millimeters.\"\"\"\n",
    "    if units == DrawingUnits.INCHES:\n",
    "        return value * INCH_TO_MM\n",
    "    return value\n",
    "\n",
    "\n",
    "def normalize_callout_to_mm(callout: Dict, units: DrawingUnits) -> Dict:\n",
    "    \"\"\"\n",
    "    Normalize all dimension values in a callout to millimeters.\n",
    "\n",
    "    Updates diameterMm, depthMm, radiusMm, etc. based on detected units.\n",
    "    \"\"\"\n",
    "    if units == DrawingUnits.MILLIMETERS or units == DrawingUnits.UNKNOWN:\n",
    "        return callout  # Already in MM or unknown, don't convert\n",
    "\n",
    "    callout = callout.copy()  # Don't modify original\n",
    "\n",
    "    # Convert hole dimensions\n",
    "    if 'hole' in callout and callout['hole']:\n",
    "        hole = callout['hole'].copy()\n",
    "        if hole.get('diameterInches') and not hole.get('diameterMm'):\n",
    "            hole['diameterMm'] = hole['diameterInches'] * INCH_TO_MM\n",
    "        elif hole.get('diameterMm') and units == DrawingUnits.INCHES:\n",
    "            # Value was parsed as MM but drawing is in inches\n",
    "            # This is a common VLM confusion - assume it's actually inches\n",
    "            hole['diameterInches'] = hole['diameterMm']\n",
    "            hole['diameterMm'] = hole['diameterMm'] * INCH_TO_MM\n",
    "        if hole.get('depthMm') and units == DrawingUnits.INCHES:\n",
    "            hole['depthInches'] = hole['depthMm']\n",
    "            hole['depthMm'] = hole['depthMm'] * INCH_TO_MM\n",
    "        callout['hole'] = hole\n",
    "\n",
    "    # Convert fillet\n",
    "    if 'fillet' in callout and callout['fillet']:\n",
    "        fillet = callout['fillet'].copy()\n",
    "        if fillet.get('radiusMm') and units == DrawingUnits.INCHES:\n",
    "            fillet['radiusInches'] = fillet['radiusMm']\n",
    "            fillet['radiusMm'] = fillet['radiusMm'] * INCH_TO_MM\n",
    "        callout['fillet'] = fillet\n",
    "\n",
    "    # Convert chamfer\n",
    "    if 'chamfer' in callout and callout['chamfer']:\n",
    "        chamfer = callout['chamfer'].copy()\n",
    "        if chamfer.get('distance1Mm') and units == DrawingUnits.INCHES:\n",
    "            chamfer['distance1Inches'] = chamfer['distance1Mm']\n",
    "            chamfer['distance1Mm'] = chamfer['distance1Mm'] * INCH_TO_MM\n",
    "        if chamfer.get('distance2Mm') and units == DrawingUnits.INCHES:\n",
    "            chamfer['distance2Inches'] = chamfer['distance2Mm']\n",
    "            chamfer['distance2Mm'] = chamfer['distance2Mm'] * INCH_TO_MM\n",
    "        callout['chamfer'] = chamfer\n",
    "\n",
    "    # Convert linear dimension\n",
    "    if 'dimension' in callout and callout['dimension']:\n",
    "        dim = callout['dimension'].copy()\n",
    "        if dim.get('valueMm') and units == DrawingUnits.INCHES:\n",
    "            dim['valueInches'] = dim['valueMm']\n",
    "            dim['valueMm'] = dim['valueMm'] * INCH_TO_MM\n",
    "        callout['dimension'] = dim\n",
    "\n",
    "    return callout\n",
    "\n",
    "\n",
    "def parse_dimension_with_unit(raw_text: str) -> Tuple[Optional[float], DrawingUnits]:\n",
    "    \"\"\"\n",
    "    Parse a dimension string and detect its unit.\n",
    "\n",
    "    Examples:\n",
    "        \"12.7mm\" -> (12.7, MM)\n",
    "        \"0.500\"\" or \"0.500 IN\" -> (0.5, INCHES)\n",
    "        \"1/2\" -> (0.5, INCHES)  # Fractions are imperial\n",
    "        \"\u00d812.70\" -> (12.7, UNKNOWN)  # Need context\n",
    "    \"\"\"\n",
    "    text = raw_text.strip().upper()\n",
    "\n",
    "    # Remove diameter symbol\n",
    "    text = re.sub(r'[\u00d8\u00f8\u2205\u2300]', '', text)\n",
    "\n",
    "    # Check for explicit unit suffix\n",
    "    mm_match = re.match(r'([\\d.]+)\\s*MM', text)\n",
    "    if mm_match:\n",
    "        return float(mm_match.group(1)), DrawingUnits.MILLIMETERS\n",
    "\n",
    "    inch_match = re.match(r'([\\d.]+)\\s*(?:IN(?:CH)?|\")', text)\n",
    "    if inch_match:\n",
    "        return float(inch_match.group(1)), DrawingUnits.INCHES\n",
    "\n",
    "    # Check for fractions (imperial)\n",
    "    frac_match = re.match(r'(\\d+)?[\\s-]*(\\d+)/(\\d+)', text)\n",
    "    if frac_match:\n",
    "        whole = int(frac_match.group(1) or 0)\n",
    "        numer = int(frac_match.group(2))\n",
    "        denom = int(frac_match.group(3))\n",
    "        value = whole + (numer / denom)\n",
    "        return value, DrawingUnits.INCHES\n",
    "\n",
    "    # Plain number - unit unknown\n",
    "    num_match = re.match(r'([\\d.]+)', text)\n",
    "    if num_match:\n",
    "        return float(num_match.group(1)), DrawingUnits.UNKNOWN\n",
    "\n",
    "    return None, DrawingUnits.UNKNOWN\n",
    "\n",
    "\n",
    "print(\"Unit detection and normalization defined.\")"
   ],
   "metadata": {
    "id": "cBsggFOS3Qt8"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cBsggFOS3Qt8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEwC55r33Qt8"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 5B: VLM Evidence Extraction with Upgrades\n",
    "# ============================================================\n",
    "\n",
    "VLM_EXTRACTION_PROMPT = '''You are an engineering drawing analyzer. Extract all callouts and dimensions from this drawing.\n",
    "\n",
    "**OUTPUT FORMAT (JSON):**\n",
    "```json\n",
    "{\n",
    "  \"foundCallouts\": [\n",
    "    {\n",
    "      \"calloutType\": \"Hole|TappedHole|Fillet|Chamfer|LinearDimension|DiameterDimension|Note\",\n",
    "      \"rawText\": \"exact text as shown\",\n",
    "      \"quantity\": 1,\n",
    "      \"quantityRaw\": \"(2X) or 4 PLCS or null\",\n",
    "      \"hole\": {\"diameterMm\": 12.7, \"isThrough\": true, \"depthMm\": null},\n",
    "      \"thread\": {\"standard\": \"Metric\", \"nominalMm\": 6, \"pitch\": 1.0},\n",
    "      \"fillet\": {\"radiusMm\": 3.0},\n",
    "      \"chamfer\": {\"distance1Mm\": 0.5, \"angleDegrees\": 45}\n",
    "    }\n",
    "  ],\n",
    "  \"foundNotes\": [\n",
    "    {\"rawText\": \"note text\", \"noteType\": \"General|Material|Finish\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**RULES:**\n",
    "1. Only extract what you can SEE with evidence\n",
    "2. For dimensions: convert fractions to decimal (1/2 = 0.5 inches = 12.7mm)\n",
    "3. For holes: note if THRU or BLIND with depth\n",
    "4. For threads: identify M6x1.0, 1/4-20 UNC, etc.\n",
    "5. Include quantity (2X, 4 PLCS, etc.)\n",
    "6. Do NOT say \"missing\" - only report what you FIND\n",
    "\n",
    "**OCR TEXT FROM DRAWING:**\n",
    "{ocr_text}\n",
    "\n",
    "Extract all callouts and dimensions from the drawing image.\n",
    "'''\n",
    "\n",
    "\n",
    "def extract_evidence_with_vlm(\n",
    "    page_artifacts: List[PageArtifact],\n",
    "    ocr_blocks: List[OcrBlock],\n",
    "    resolved_identity: ResolvedPartIdentity,\n",
    "    detected_units: Optional[UnitDetectionResult] = None,\n",
    "    title_block_identity: Optional[Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use VLM to extract structured evidence from drawing.\n",
    "    \n",
    "    Integrates:\n",
    "    - ROI-extracted title block identity\n",
    "    - Unit detection for normalization\n",
    "    - Deterministic post-processing canonicalization\n",
    "    \n",
    "    Returns DrawingEvidence-compatible dict.\n",
    "    \"\"\"\n",
    "    global vlm_model, vlm_processor\n",
    "    \n",
    "    # Prepare OCR text for prompt\n",
    "    ocr_lines = [b.text for b in ocr_blocks if any(c.isdigit() for c in b.text)]\n",
    "    ocr_text = \"\\n\".join([f\"- {line}\" for line in ocr_lines[:100]])\n",
    "    \n",
    "    # Get first page image\n",
    "    if not page_artifacts:\n",
    "        return {\"error\": \"No page artifacts\"}\n",
    "    \n",
    "    img = page_artifacts[0].image\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = VLM_EXTRACTION_PROMPT.format(ocr_text=ocr_text)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process with VLM\n",
    "    text_input = vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    \n",
    "    inputs = vlm_processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(vlm_model.device)\n",
    "    \n",
    "    print(f\"  VLM input tokens: {inputs.input_ids.shape[1]}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = vlm_model.generate(**inputs, max_new_tokens=2000, do_sample=False)\n",
    "    \n",
    "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "    response = vlm_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    vlm_data = parse_vlm_json_response(response)\n",
    "    \n",
    "    # Build DrawingEvidence structure with all upgrades\n",
    "    evidence = build_drawing_evidence(\n",
    "        vlm_data=vlm_data,\n",
    "        ocr_blocks=ocr_blocks,\n",
    "        resolved_identity=resolved_identity,\n",
    "        page_artifacts=page_artifacts,\n",
    "        detected_units=detected_units,\n",
    "        title_block_identity=title_block_identity\n",
    "    )\n",
    "    \n",
    "    return evidence\n",
    "\n",
    "\n",
    "def parse_vlm_json_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract JSON from VLM response.\"\"\"\n",
    "    # Try to find JSON block\n",
    "    json_match = re.search(r'```json\\s*(.+?)\\s*```', response, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Try to find raw JSON\n",
    "    json_match = re.search(r'\\{[^{}]*\"foundCallouts\"[^{}]*\\}', response, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Return empty structure\n",
    "    print(f\"  Warning: Could not parse VLM JSON response\")\n",
    "    return {\"foundCallouts\": [], \"foundNotes\": []}\n",
    "\n",
    "\n",
    "def build_drawing_evidence(\n",
    "    vlm_data: Dict[str, Any],\n",
    "    ocr_blocks: List[OcrBlock],\n",
    "    resolved_identity: ResolvedPartIdentity,\n",
    "    page_artifacts: List[PageArtifact],\n",
    "    detected_units: Optional[UnitDetectionResult] = None,\n",
    "    title_block_identity: Optional[Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build DrawingEvidence JSON conforming to schema v1.1.1.\n",
    "    \n",
    "    Applies all upgrades:\n",
    "    1. Real bounding boxes from OCR\n",
    "    2. Title block identity extraction\n",
    "    3. Unit detection and normalization\n",
    "    4. Deterministic canonicalization\n",
    "    \"\"\"\n",
    "    # Get SW data for fallback (using BOM-robust loader)\n",
    "    sw_data = {}\n",
    "    if resolved_identity.jsonPath and os.path.exists(resolved_identity.jsonPath):\n",
    "        sw_data, load_error = load_json_robust(Path(resolved_identity.jsonPath))\n",
    "        if not sw_data:\n",
    "            print(f\"  Warning: Could not load SW data: {load_error}\")\n",
    "            sw_data = {}\n",
    "    \n",
    "    sw_identity = sw_data.get('identity', {})\n",
    "    \n",
    "    # Merge identity sources: OCR title block > filename resolution > SW fallback\n",
    "    tb = title_block_identity or {}\n",
    "    \n",
    "    identity = {\n",
    "        \"partNumber\": {\n",
    "            \"ocrValue\": tb.get(\"partNumber\") or resolved_identity.resolvedPartNumber,\n",
    "            \"swFallback\": sw_identity.get('partNumber'),\n",
    "            \"resolved\": tb.get(\"partNumber\") or resolved_identity.resolvedPartNumber or sw_identity.get('partNumber'),\n",
    "            \"ocrConfidence\": resolved_identity.confidence,\n",
    "            \"source\": \"ocr_title_block\" if tb.get(\"partNumber\") else (\"ocr_filename\" if resolved_identity.resolvedPartNumber else \"sw_fallback\")\n",
    "        },\n",
    "        \"description\": {\n",
    "            \"ocrValue\": tb.get(\"description\"),\n",
    "            \"swFallback\": sw_identity.get('description'),\n",
    "            \"resolved\": tb.get(\"description\") or sw_identity.get('description'),\n",
    "            \"source\": \"ocr_title_block\" if tb.get(\"description\") else \"sw_fallback\"\n",
    "        },\n",
    "        \"material\": {\n",
    "            \"ocrValue\": tb.get(\"material\"),\n",
    "            \"swFallback\": sw_identity.get('material'),\n",
    "            \"resolved\": tb.get(\"material\") or sw_identity.get('material'),\n",
    "            \"source\": \"ocr_title_block\" if tb.get(\"material\") else \"sw_fallback\"\n",
    "        },\n",
    "        \"finish\": {\n",
    "            \"ocrValue\": tb.get(\"finish\"),\n",
    "            \"swFallback\": sw_identity.get('finish'),\n",
    "            \"resolved\": tb.get(\"finish\") or sw_identity.get('finish'),\n",
    "            \"source\": \"ocr_title_block\" if tb.get(\"finish\") else \"sw_fallback\"\n",
    "        },\n",
    "        \"revision\": {\n",
    "            \"ocrValue\": tb.get(\"revision\"),\n",
    "            \"swFallback\": sw_identity.get('revision'),\n",
    "            \"resolved\": tb.get(\"revision\") or sw_identity.get('revision'),\n",
    "            \"source\": \"ocr_title_block\" if tb.get(\"revision\") else \"sw_fallback\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process VLM callouts with post-processing\n",
    "    found_callouts = []\n",
    "    for callout in vlm_data.get('foundCallouts', []):\n",
    "        processed = process_vlm_callout(callout)\n",
    "        if processed:\n",
    "            # Apply unit normalization if inches detected\n",
    "            if detected_units and detected_units.detected_unit == DrawingUnits.INCHES:\n",
    "                processed = normalize_callout_to_mm(processed, DrawingUnits.INCHES)\n",
    "            \n",
    "            # Apply deterministic canonicalization\n",
    "            processed = canonicalize_callout(processed)\n",
    "            \n",
    "            found_callouts.append(processed)\n",
    "    \n",
    "    # Process notes\n",
    "    found_notes = []\n",
    "    for note in vlm_data.get('foundNotes', []):\n",
    "        found_notes.append({\n",
    "            \"rawText\": note.get('rawText', ''),\n",
    "            \"noteType\": note.get('noteType', 'General'),\n",
    "            \"confidence\": 0.8\n",
    "        })\n",
    "    \n",
    "    # Build unit detection info\n",
    "    unit_info = None\n",
    "    if detected_units:\n",
    "        unit_info = {\n",
    "            \"detectedUnit\": detected_units.detected_unit.value,\n",
    "            \"confidence\": detected_units.confidence,\n",
    "            \"evidence\": detected_units.evidence,\n",
    "            \"conversionApplied\": detected_units.detected_unit == DrawingUnits.INCHES\n",
    "        }\n",
    "    \n",
    "    # Build evidence document\n",
    "    evidence = {\n",
    "        \"schemaVersion\": SCHEMA_VERSION,\n",
    "        \"extractionTime\": datetime.now().isoformat(),\n",
    "        \"sourceFile\": DRAWING_PDF_PATH,\n",
    "        \"pageCount\": len(page_artifacts),\n",
    "        \"overallConfidence\": calculate_overall_confidence(found_callouts),\n",
    "        \"identity\": identity,\n",
    "        \"foundCallouts\": found_callouts,\n",
    "        \"foundNotes\": found_notes,\n",
    "        \"views\": [],\n",
    "        \"unitDetection\": unit_info,\n",
    "        \"validationSummary\": generate_validation_summary(found_callouts)\n",
    "    }\n",
    "    \n",
    "    return evidence\n",
    "\n",
    "\n",
    "def process_vlm_callout(callout: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Process a single VLM callout into schema format.\"\"\"\n",
    "    callout_type = callout.get('calloutType', 'Unknown')\n",
    "    raw_text = callout.get('rawText', '')\n",
    "    \n",
    "    if not raw_text:\n",
    "        return None\n",
    "    \n",
    "    processed = {\n",
    "        \"calloutType\": callout_type,\n",
    "        \"rawText\": raw_text,\n",
    "        \"canonical\": None,  # Will be set by canonicalizer\n",
    "        \"confidence\": callout.get('confidence', 0.8),\n",
    "        \"location\": {\n",
    "            \"page\": 1,\n",
    "            \"pageIndex0\": 0,\n",
    "            \"bbox\": callout.get('bbox')  # May have real bbox now\n",
    "        },\n",
    "        \"quantity\": callout.get('quantity', 1),\n",
    "        \"quantityRaw\": callout.get('quantityRaw'),\n",
    "        \"plcs\": None,\n",
    "        \"isTypical\": None,\n",
    "        \"validationWarnings\": []\n",
    "    }\n",
    "    \n",
    "    # Add type-specific data\n",
    "    if callout_type in ['Hole', 'TappedHole', 'Counterbore', 'Countersink']:\n",
    "        hole_data = callout.get('hole', {})\n",
    "        processed['hole'] = {\n",
    "            \"diameterMm\": hole_data.get('diameterMm'),\n",
    "            \"diameterInches\": hole_data.get('diameterInches'),\n",
    "            \"diameterRaw\": hole_data.get('diameterRaw'),\n",
    "            \"depthMm\": hole_data.get('depthMm'),\n",
    "            \"isThrough\": hole_data.get('isThrough')\n",
    "        }\n",
    "        if callout_type == 'TappedHole':\n",
    "            thread_data = callout.get('thread', {})\n",
    "            processed['thread'] = {\n",
    "                \"rawText\": thread_data.get('rawText'),\n",
    "                \"standard\": thread_data.get('standard'),\n",
    "                \"nominalMm\": thread_data.get('nominalMm'),\n",
    "                \"pitch\": thread_data.get('pitch'),\n",
    "                \"tpi\": thread_data.get('tpi')\n",
    "            }\n",
    "    \n",
    "    elif callout_type == 'Fillet':\n",
    "        fillet_data = callout.get('fillet', {})\n",
    "        processed['fillet'] = {\n",
    "            \"radiusMm\": fillet_data.get('radiusMm'),\n",
    "            \"radiusRaw\": fillet_data.get('radiusRaw')\n",
    "        }\n",
    "    \n",
    "    elif callout_type == 'Chamfer':\n",
    "        chamfer_data = callout.get('chamfer', {})\n",
    "        processed['chamfer'] = {\n",
    "            \"chamferType\": chamfer_data.get('chamferType', 'AngleDistance'),\n",
    "            \"distance1Mm\": chamfer_data.get('distance1Mm'),\n",
    "            \"angleDegrees\": chamfer_data.get('angleDegrees', 45)\n",
    "        }\n",
    "    \n",
    "    elif callout_type in ['LinearDimension', 'DiameterDimension', 'RadiusDimension']:\n",
    "        dim_data = callout.get('dimension', {})\n",
    "        processed['dimension'] = {\n",
    "            \"valueMm\": dim_data.get('valueMm'),\n",
    "            \"valueInches\": dim_data.get('valueInches'),\n",
    "            \"valueRaw\": dim_data.get('valueRaw')\n",
    "        }\n",
    "    \n",
    "    return processed\n",
    "\n",
    "\n",
    "def calculate_overall_confidence(callouts: List[Dict]) -> float:\n",
    "    \"\"\"Calculate overall confidence from callouts.\"\"\"\n",
    "    if not callouts:\n",
    "        return 0.0\n",
    "    confidences = [c.get('confidence', 0.5) for c in callouts]\n",
    "    return sum(confidences) / len(confidences)\n",
    "\n",
    "\n",
    "def generate_validation_summary(callouts: List[Dict]) -> Dict:\n",
    "    \"\"\"Generate validation summary for callouts.\"\"\"\n",
    "    warnings_count = {}\n",
    "    callouts_with_warnings = 0\n",
    "    low_confidence = 0\n",
    "    no_location = 0\n",
    "    \n",
    "    for c in callouts:\n",
    "        warnings = c.get('validationWarnings', [])\n",
    "        if warnings:\n",
    "            callouts_with_warnings += 1\n",
    "        for w in warnings:\n",
    "            key = \"other\"\n",
    "            if \"diameter\" in w.lower():\n",
    "                key = \"missingDiameter\"\n",
    "            elif \"depth\" in w.lower():\n",
    "                key = \"missingDepth\"\n",
    "            elif \"thread\" in w.lower() or \"pitch\" in w.lower():\n",
    "                key = \"incompleteThread\"\n",
    "            elif \"confidence\" in w.lower():\n",
    "                key = \"lowConfidence\"\n",
    "            elif \"radius\" in w.lower():\n",
    "                key = \"missingRadius\"\n",
    "            elif \"chamfer\" in w.lower():\n",
    "                key = \"missingChamfer\"\n",
    "            warnings_count[key] = warnings_count.get(key, 0) + 1\n",
    "        \n",
    "        if c.get('confidence', 1.0) < 0.5:\n",
    "            low_confidence += 1\n",
    "        \n",
    "        loc = c.get('location', {})\n",
    "        if not loc.get('bbox'):\n",
    "            no_location += 1\n",
    "    \n",
    "    return {\n",
    "        \"totalCallouts\": len(callouts),\n",
    "        \"calloutsWithWarnings\": callouts_with_warnings,\n",
    "        \"warningCounts\": warnings_count,\n",
    "        \"lowConfidenceCount\": low_confidence,\n",
    "        \"noLocationCount\": no_location\n",
    "    }\n",
    "\n",
    "print(\"VLM evidence extraction with upgrades defined.\")"
   ],
   "id": "NEwC55r33Qt8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utPNNMRj3Qt8"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 5A: Load Qwen2-VL Model\n",
    "# ============================================================\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "print(f\"GPU memory before: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "vlm_processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(f\"Qwen2-VL-7B loaded!\")\n",
    "print(f\"GPU memory after: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ],
   "outputs": [],
   "id": "utPNNMRj3Qt8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOlbkULR3Qt9"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 6A: Deterministic Canonicalization Post-Processor\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "This section provides DETERMINISTIC regex-based canonicalization that\n",
    "runs AFTER VLM extraction. It ensures consistent formatting regardless\n",
    "of how the VLM interpreted the raw text.\n",
    "\n",
    "Key transformations:\n",
    "- Diameter symbols: \u2300, \u00d8, \u00f8, 0/ -> \u00d8 (U+00D8)\n",
    "- Fractions -> decimals: 1/2 -> 0.5\n",
    "- THRU/THROUGH/T standardization\n",
    "- DEEP/DP standardization\n",
    "- Quantity parsing: (4X), 4 PLCS, 4 HOLES, TYP\n",
    "- Thread normalization: M6x1.0, 1/4-20 UNC\n",
    "\"\"\"\n",
    "\n",
    "# Standard symbols\n",
    "DIAMETER_SYMBOL = \"\\u00d8\"  # \u00d8 (U+00D8) - canonical diameter symbol\n",
    "DEGREE_SYMBOL = \"\\u00b0\"    # \u00b0 (U+00B0) - degree symbol\n",
    "PLUS_MINUS = \"\\u00b1\"       # \u00b1 (U+00B1) - plus/minus\n",
    "\n",
    "# Common fractions lookup (for speed)\n",
    "COMMON_FRACTIONS = {\n",
    "    \"1/64\": 0.015625, \"1/32\": 0.03125, \"3/64\": 0.046875, \"1/16\": 0.0625,\n",
    "    \"5/64\": 0.078125, \"3/32\": 0.09375, \"7/64\": 0.109375, \"1/8\": 0.125,\n",
    "    \"9/64\": 0.140625, \"5/32\": 0.15625, \"11/64\": 0.171875, \"3/16\": 0.1875,\n",
    "    \"13/64\": 0.203125, \"7/32\": 0.21875, \"15/64\": 0.234375, \"1/4\": 0.25,\n",
    "    \"17/64\": 0.265625, \"9/32\": 0.28125, \"19/64\": 0.296875, \"5/16\": 0.3125,\n",
    "    \"21/64\": 0.328125, \"11/32\": 0.34375, \"23/64\": 0.359375, \"3/8\": 0.375,\n",
    "    \"25/64\": 0.390625, \"13/32\": 0.40625, \"27/64\": 0.421875, \"7/16\": 0.4375,\n",
    "    \"29/64\": 0.453125, \"15/32\": 0.46875, \"31/64\": 0.484375, \"1/2\": 0.5,\n",
    "    \"33/64\": 0.515625, \"17/32\": 0.53125, \"35/64\": 0.546875, \"9/16\": 0.5625,\n",
    "    \"37/64\": 0.578125, \"19/32\": 0.59375, \"39/64\": 0.609375, \"5/8\": 0.625,\n",
    "    \"41/64\": 0.640625, \"21/32\": 0.65625, \"43/64\": 0.671875, \"11/16\": 0.6875,\n",
    "    \"45/64\": 0.703125, \"23/32\": 0.71875, \"47/64\": 0.734375, \"3/4\": 0.75,\n",
    "    \"49/64\": 0.765625, \"25/32\": 0.78125, \"51/64\": 0.796875, \"13/16\": 0.8125,\n",
    "    \"53/64\": 0.828125, \"27/32\": 0.84375, \"55/64\": 0.859375, \"7/8\": 0.875,\n",
    "    \"57/64\": 0.890625, \"29/32\": 0.90625, \"59/64\": 0.921875, \"15/16\": 0.9375,\n",
    "    \"61/64\": 0.953125, \"31/32\": 0.96875, \"63/64\": 0.984375,\n",
    "}\n",
    "\n",
    "# Matching tolerances (from sw_to_evidence_mapping.json)\n",
    "TOLERANCES = {\n",
    "    \"diameter_mm\": {\"absolute\": 0.15, \"percent\": 0.5},\n",
    "    \"depth_mm\": {\"absolute\": 0.5, \"percent\": 2.0},\n",
    "    \"radius_mm\": {\"absolute\": 0.05, \"percent\": 5.0},\n",
    "    \"chamfer_mm\": {\"absolute\": 0.1, \"percent\": 5.0},\n",
    "    \"angle_deg\": {\"absolute\": 1.0},\n",
    "    \"thread_pitch\": {\"exact\": True},\n",
    "    \"quantity\": {\"exact\": True}\n",
    "}\n",
    "\n",
    "\n",
    "def canonicalize_diameter_symbol(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace all diameter symbol variants with canonical \u00d8 (U+00D8).\n",
    "\n",
    "    Handles: \u2300 (U+2300), \u00f8 (U+00F8), 0/ (typo), DIA, etc.\n",
    "    \"\"\"\n",
    "    # Unicode diameter symbols\n",
    "    text = text.replace(\"\\u2300\", DIAMETER_SYMBOL)  # \u2300 -> \u00d8\n",
    "    text = text.replace(\"\\u00f8\", DIAMETER_SYMBOL)  # \u00f8 -> \u00d8\n",
    "    text = text.replace(\"\\u2205\", DIAMETER_SYMBOL)  # \u2205 -> \u00d8\n",
    "\n",
    "    # OCR misreads\n",
    "    text = re.sub(r'0/', DIAMETER_SYMBOL, text)  # 0/ -> \u00d8\n",
    "    text = re.sub(r'O/', DIAMETER_SYMBOL, text)  # O/ -> \u00d8\n",
    "    text = re.sub(r'\\bDIA\\s*', DIAMETER_SYMBOL, text, flags=re.IGNORECASE)  # DIA -> \u00d8\n",
    "    text = re.sub(r'\\bDIAM(?:ETER)?\\s*', DIAMETER_SYMBOL, text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def fraction_to_decimal(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert fraction strings to decimal.\n",
    "\n",
    "    Handles: \"1/2\", \"1-1/2\", \"1 1/2\" -> \"0.5\", \"1.5\", \"1.5\"\n",
    "    \"\"\"\n",
    "    def replace_fraction(match):\n",
    "        whole = match.group(1)\n",
    "        numer = int(match.group(2))\n",
    "        denom = int(match.group(3))\n",
    "\n",
    "        # Try lookup first\n",
    "        frac_key = f\"{numer}/{denom}\"\n",
    "        if frac_key in COMMON_FRACTIONS:\n",
    "            decimal = COMMON_FRACTIONS[frac_key]\n",
    "        else:\n",
    "            decimal = numer / denom\n",
    "\n",
    "        if whole:\n",
    "            whole_num = int(whole)\n",
    "            return f\"{whole_num + decimal:.4f}\".rstrip('0').rstrip('.')\n",
    "        else:\n",
    "            return f\"{decimal:.4f}\".rstrip('0').rstrip('.')\n",
    "\n",
    "    # Match whole-fraction (1-1/2, 1 1/2) or just fraction (1/2)\n",
    "    pattern = r'(\\d+)?[\\s-]*(\\d+)/(\\d+)'\n",
    "    return re.sub(pattern, replace_fraction, text)\n",
    "\n",
    "\n",
    "def canonicalize_through_hole(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize THROUGH hole notation.\n",
    "\n",
    "    THRU, THROUGH, T/H, THROUGH ALL -> THRU\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\bTHROUGH\\s+ALL\\b', 'THRU', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bTHROUGH\\b', 'THRU', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bT/H\\b', 'THRU', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def canonicalize_depth(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize DEPTH notation.\n",
    "\n",
    "    DP, D/P, DEEP -> DEEP\n",
    "    Also extracts depth value pattern.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\bDP\\b', 'DEEP', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bD/P\\b', 'DEEP', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_quantity(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract quantity information from text.\n",
    "\n",
    "    Patterns:\n",
    "    - (4X), 4X -> quantity=4\n",
    "    - 4 PLCS, 4 PLACES -> quantity=4, plcs=4\n",
    "    - 4 HOLES -> quantity=4\n",
    "    - TYP, TYPICAL -> isTypical=True\n",
    "\n",
    "    Returns dict with: quantity, plcs, quantityRaw, isTypical\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"quantity\": 1,\n",
    "        \"plcs\": None,\n",
    "        \"quantityRaw\": None,\n",
    "        \"isTypical\": None\n",
    "    }\n",
    "\n",
    "    text_upper = text.upper()\n",
    "\n",
    "    # Check for TYP/TYPICAL first\n",
    "    if re.search(r'\\bTYP(?:ICAL)?\\b', text_upper):\n",
    "        result[\"isTypical\"] = True\n",
    "        result[\"quantityRaw\"] = \"TYP\"\n",
    "\n",
    "    # Pattern: (4X) or 4X at start\n",
    "    match = re.search(r'\\((\\d+)\\s*X\\)', text_upper)\n",
    "    if match:\n",
    "        result[\"quantity\"] = int(match.group(1))\n",
    "        result[\"quantityRaw\"] = match.group(0)\n",
    "        return result\n",
    "\n",
    "    match = re.search(r'^(\\d+)\\s*X\\b', text_upper)\n",
    "    if match:\n",
    "        result[\"quantity\"] = int(match.group(1))\n",
    "        result[\"quantityRaw\"] = f\"({match.group(1)}X)\"\n",
    "        return result\n",
    "\n",
    "    # Pattern: 4 PLCS, 4 PLACES\n",
    "    match = re.search(r'(\\d+)\\s*(?:PLCS?|PLACES?)', text_upper)\n",
    "    if match:\n",
    "        result[\"quantity\"] = int(match.group(1))\n",
    "        result[\"plcs\"] = int(match.group(1))\n",
    "        result[\"quantityRaw\"] = f\"{match.group(1)} PLCS\"\n",
    "        return result\n",
    "\n",
    "    # Pattern: 4 HOLES\n",
    "    match = re.search(r'(\\d+)\\s*HOLES?', text_upper)\n",
    "    if match:\n",
    "        result[\"quantity\"] = int(match.group(1))\n",
    "        result[\"quantityRaw\"] = f\"{match.group(1)} HOLES\"\n",
    "        return result\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_hole_callout(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a hole callout string into structured data.\n",
    "\n",
    "    Examples:\n",
    "    - \"\u00d812.70 THRU\" -> {diameterMm: 12.7, isThrough: True}\n",
    "    - \"\u00d8.500 x .750 DEEP\" -> {diameterMm: 12.7, depthMm: 19.05} (if inches)\n",
    "    - \"\u00d812.70 x 25.4 DEEP (4X)\" -> {diameterMm: 12.7, depthMm: 25.4, quantity: 4}\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"diameterMm\": None,\n",
    "        \"diameterRaw\": None,\n",
    "        \"depthMm\": None,\n",
    "        \"depthRaw\": None,\n",
    "        \"isThrough\": False,\n",
    "        \"quantity\": 1,\n",
    "        \"plcs\": None,\n",
    "        \"quantityRaw\": None,\n",
    "        \"isTypical\": None\n",
    "    }\n",
    "\n",
    "    # Canonicalize first\n",
    "    text = canonicalize_diameter_symbol(text)\n",
    "    text = fraction_to_decimal(text)\n",
    "    text = canonicalize_through_hole(text)\n",
    "    text = canonicalize_depth(text)\n",
    "\n",
    "    # Extract quantity\n",
    "    qty_info = parse_quantity(text)\n",
    "    result.update(qty_info)\n",
    "\n",
    "    # Check for THRU\n",
    "    if re.search(r'\\bTHRU\\b', text, re.IGNORECASE):\n",
    "        result[\"isThrough\"] = True\n",
    "\n",
    "    # Extract diameter (after \u00d8 symbol)\n",
    "    dia_match = re.search(r'[\u00d8\u00f8\\u2300]([\\d.]+)', text)\n",
    "    if dia_match:\n",
    "        result[\"diameterMm\"] = float(dia_match.group(1))\n",
    "        result[\"diameterRaw\"] = f\"{DIAMETER_SYMBOL}{dia_match.group(1)}\"\n",
    "\n",
    "    # Extract depth (before DEEP)\n",
    "    depth_match = re.search(r'[xX\u00d7]\\s*([\\d.]+)\\s*(?:mm)?\\s*DEEP', text, re.IGNORECASE)\n",
    "    if depth_match:\n",
    "        result[\"depthMm\"] = float(depth_match.group(1))\n",
    "        result[\"depthRaw\"] = f\"{depth_match.group(1)} DEEP\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_thread_callout(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a thread callout string into structured data.\n",
    "\n",
    "    Examples:\n",
    "    - \"M6x1.0\" -> {standard: \"Metric\", nominalMm: 6, pitch: 1.0}\n",
    "    - \"1/4-20 UNC\" -> {standard: \"UNC\", nominalMm: 6.35, tpi: 20}\n",
    "    - \"#10-32 UNF\" -> {standard: \"UNF\", nominalMm: 4.83, tpi: 32}\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"standard\": None,\n",
    "        \"nominalMm\": None,\n",
    "        \"nominalRaw\": None,\n",
    "        \"pitch\": None,\n",
    "        \"tpi\": None,\n",
    "        \"thread_class\": None\n",
    "    }\n",
    "\n",
    "    text = fraction_to_decimal(text)\n",
    "    text_upper = text.upper()\n",
    "\n",
    "    # Metric thread: M6x1.0, M8x1.25\n",
    "    metric_match = re.search(r'M(\\d+(?:\\.\\d+)?)\\s*[xX\u00d7]\\s*(\\d+(?:\\.\\d+)?)', text_upper)\n",
    "    if metric_match:\n",
    "        result[\"standard\"] = \"Metric\"\n",
    "        result[\"nominalMm\"] = float(metric_match.group(1))\n",
    "        result[\"nominalRaw\"] = f\"M{metric_match.group(1)}\"\n",
    "        result[\"pitch\"] = float(metric_match.group(2))\n",
    "        return result\n",
    "\n",
    "    # Metric thread without pitch: M6\n",
    "    metric_simple = re.search(r'M(\\d+(?:\\.\\d+)?)\\b', text_upper)\n",
    "    if metric_simple:\n",
    "        result[\"standard\"] = \"Metric\"\n",
    "        result[\"nominalMm\"] = float(metric_simple.group(1))\n",
    "        result[\"nominalRaw\"] = f\"M{metric_simple.group(1)}\"\n",
    "        return result\n",
    "\n",
    "    # Unified thread: 1/4-20 UNC, .250-20 UNC\n",
    "    unified_match = re.search(r'([\\d.]+)-(\\d+)\\s*(UN[CFJ]?)', text_upper)\n",
    "    if unified_match:\n",
    "        nominal_inch = float(unified_match.group(1))\n",
    "        result[\"standard\"] = unified_match.group(3)\n",
    "        result[\"nominalMm\"] = nominal_inch * 25.4\n",
    "        result[\"nominalRaw\"] = unified_match.group(1)\n",
    "        result[\"tpi\"] = int(unified_match.group(2))\n",
    "        return result\n",
    "\n",
    "    # Numbered threads: #10-32, #6-32\n",
    "    numbered_match = re.search(r'#(\\d+)-(\\d+)', text_upper)\n",
    "    if numbered_match:\n",
    "        num = int(numbered_match.group(1))\n",
    "        # Convert number to diameter: D = 0.013*N + 0.060 (inches)\n",
    "        diameter_inch = 0.013 * num + 0.060\n",
    "        result[\"standard\"] = \"Unified\"\n",
    "        result[\"nominalMm\"] = diameter_inch * 25.4\n",
    "        result[\"nominalRaw\"] = f\"#{num}\"\n",
    "        result[\"tpi\"] = int(numbered_match.group(2))\n",
    "        return result\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_fillet_callout(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a fillet/radius callout.\n",
    "\n",
    "    Examples:\n",
    "    - \"R3.0\" -> {radiusMm: 3.0}\n",
    "    - \"R.125\" (inches) -> {radiusMm: 3.175}\n",
    "    - \"FILLET R0.030\" -> {radiusMm: 0.76}\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"radiusMm\": None,\n",
    "        \"radiusRaw\": None\n",
    "    }\n",
    "\n",
    "    text = fraction_to_decimal(text)\n",
    "\n",
    "    # Pattern: R followed by number\n",
    "    r_match = re.search(r'R\\s*([\\d.]+)', text, re.IGNORECASE)\n",
    "    if r_match:\n",
    "        result[\"radiusMm\"] = float(r_match.group(1))\n",
    "        result[\"radiusRaw\"] = f\"R{r_match.group(1)}\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_chamfer_callout(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a chamfer callout.\n",
    "\n",
    "    Examples:\n",
    "    - \"0.030 x 45\u00b0\" -> {distance1Mm: 0.76, angleDegrees: 45}\n",
    "    - \"1.0 x 1.0\" -> {distance1Mm: 1.0, distance2Mm: 1.0}\n",
    "    - \"CHAMFER .030 X 45 DEG\" -> {distance1Mm: 0.76, angleDegrees: 45}\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"chamferType\": \"AngleDistance\",\n",
    "        \"distance1Mm\": None,\n",
    "        \"distance2Mm\": None,\n",
    "        \"angleDegrees\": None\n",
    "    }\n",
    "\n",
    "    text = fraction_to_decimal(text)\n",
    "    text = text.replace(\"\u00b0\", \" DEG \").replace(DEGREE_SYMBOL, \" DEG \")\n",
    "\n",
    "    # Pattern: distance x angle (45\u00b0, 45 DEG)\n",
    "    angle_match = re.search(r'([\\d.]+)\\s*[xX\u00d7]\\s*([\\d.]+)\\s*(?:DEG|\u00b0)?', text)\n",
    "    if angle_match:\n",
    "        val1 = float(angle_match.group(1))\n",
    "        val2 = float(angle_match.group(2))\n",
    "\n",
    "        # If second value is 45, 30, 60 - it's probably an angle\n",
    "        if val2 in [30, 45, 60]:\n",
    "            result[\"distance1Mm\"] = val1\n",
    "            result[\"angleDegrees\"] = val2\n",
    "        else:\n",
    "            # Two distances\n",
    "            result[\"chamferType\"] = \"TwoDistances\"\n",
    "            result[\"distance1Mm\"] = val1\n",
    "            result[\"distance2Mm\"] = val2\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_canonical_form(callout: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate canonical text form for a callout.\n",
    "\n",
    "    Output format examples:\n",
    "    - Hole: \"\u00d812.70mm THRU (4X)\"\n",
    "    - Thread: \"M6x1.0 x 12mm DEEP\"\n",
    "    - Fillet: \"R3.00mm\"\n",
    "    - Chamfer: \"0.76mm x 45\u00b0\"\n",
    "    \"\"\"\n",
    "    callout_type = callout.get(\"calloutType\", \"\")\n",
    "\n",
    "    if callout_type in [\"Hole\", \"TappedHole\"]:\n",
    "        hole = callout.get(\"hole\", {})\n",
    "        dia = hole.get(\"diameterMm\")\n",
    "        if not dia:\n",
    "            return callout.get(\"rawText\", \"\")\n",
    "\n",
    "        parts = [f\"{DIAMETER_SYMBOL}{dia:.2f}mm\"]\n",
    "\n",
    "        if callout_type == \"TappedHole\":\n",
    "            thread = callout.get(\"thread\", {})\n",
    "            if thread.get(\"standard\") == \"Metric\":\n",
    "                nom = thread.get(\"nominalMm\")\n",
    "                pitch = thread.get(\"pitch\")\n",
    "                if nom and pitch:\n",
    "                    parts = [f\"M{nom:.0f}x{pitch}\"]\n",
    "\n",
    "        if hole.get(\"isThrough\"):\n",
    "            parts.append(\"THRU\")\n",
    "        elif hole.get(\"depthMm\"):\n",
    "            parts.append(f\"x {hole['depthMm']:.1f}mm DEEP\")\n",
    "\n",
    "        qty = callout.get(\"quantity\", 1)\n",
    "        if qty > 1:\n",
    "            parts.append(f\"({qty}X)\")\n",
    "\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    elif callout_type == \"Fillet\":\n",
    "        fillet = callout.get(\"fillet\", {})\n",
    "        radius = fillet.get(\"radiusMm\")\n",
    "        if radius:\n",
    "            return f\"R{radius:.2f}mm\"\n",
    "        return callout.get(\"rawText\", \"\")\n",
    "\n",
    "    elif callout_type == \"Chamfer\":\n",
    "        chamfer = callout.get(\"chamfer\", {})\n",
    "        d1 = chamfer.get(\"distance1Mm\")\n",
    "        angle = chamfer.get(\"angleDegrees\")\n",
    "        d2 = chamfer.get(\"distance2Mm\")\n",
    "\n",
    "        if d1 and angle:\n",
    "            return f\"{d1:.2f}mm x {angle:.0f}{DEGREE_SYMBOL}\"\n",
    "        elif d1 and d2:\n",
    "            return f\"{d1:.2f}mm x {d2:.2f}mm\"\n",
    "        return callout.get(\"rawText\", \"\")\n",
    "\n",
    "    return callout.get(\"rawText\", \"\")\n",
    "\n",
    "\n",
    "def canonicalize_callout(callout: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Apply full canonicalization to a callout.\n",
    "\n",
    "    1. Parse rawText to extract values\n",
    "    2. Fill in missing fields\n",
    "    3. Generate canonical form\n",
    "    4. Add validation warnings\n",
    "    \"\"\"\n",
    "    callout = callout.copy()\n",
    "    raw_text = callout.get(\"rawText\", \"\")\n",
    "    callout_type = callout.get(\"calloutType\", \"\")\n",
    "\n",
    "    # Parse based on type\n",
    "    if callout_type in [\"Hole\", \"TappedHole\", \"Counterbore\", \"Countersink\"]:\n",
    "        parsed = parse_hole_callout(raw_text)\n",
    "\n",
    "        # Update hole data if not already set\n",
    "        if \"hole\" not in callout or not callout[\"hole\"]:\n",
    "            callout[\"hole\"] = {}\n",
    "\n",
    "        hole = callout[\"hole\"]\n",
    "        if not hole.get(\"diameterMm\") and parsed[\"diameterMm\"]:\n",
    "            hole[\"diameterMm\"] = parsed[\"diameterMm\"]\n",
    "        if not hole.get(\"depthMm\") and parsed[\"depthMm\"]:\n",
    "            hole[\"depthMm\"] = parsed[\"depthMm\"]\n",
    "        if hole.get(\"isThrough\") is None:\n",
    "            hole[\"isThrough\"] = parsed[\"isThrough\"]\n",
    "\n",
    "        callout[\"hole\"] = hole\n",
    "\n",
    "        # Update quantity\n",
    "        if callout.get(\"quantity\", 1) == 1 and parsed[\"quantity\"] > 1:\n",
    "            callout[\"quantity\"] = parsed[\"quantity\"]\n",
    "            callout[\"quantityRaw\"] = parsed[\"quantityRaw\"]\n",
    "        if parsed[\"plcs\"]:\n",
    "            callout[\"plcs\"] = parsed[\"plcs\"]\n",
    "        if parsed[\"isTypical\"]:\n",
    "            callout[\"isTypical\"] = parsed[\"isTypical\"]\n",
    "\n",
    "        # Thread parsing for tapped holes\n",
    "        if callout_type == \"TappedHole\":\n",
    "            thread_parsed = parse_thread_callout(raw_text)\n",
    "            if \"thread\" not in callout or not callout[\"thread\"]:\n",
    "                callout[\"thread\"] = {}\n",
    "\n",
    "            thread = callout[\"thread\"]\n",
    "            if not thread.get(\"standard\") and thread_parsed[\"standard\"]:\n",
    "                thread[\"standard\"] = thread_parsed[\"standard\"]\n",
    "            if not thread.get(\"nominalMm\") and thread_parsed[\"nominalMm\"]:\n",
    "                thread[\"nominalMm\"] = thread_parsed[\"nominalMm\"]\n",
    "            if not thread.get(\"pitch\") and thread_parsed[\"pitch\"]:\n",
    "                thread[\"pitch\"] = thread_parsed[\"pitch\"]\n",
    "            if not thread.get(\"tpi\") and thread_parsed[\"tpi\"]:\n",
    "                thread[\"tpi\"] = thread_parsed[\"tpi\"]\n",
    "\n",
    "            callout[\"thread\"] = thread\n",
    "\n",
    "    elif callout_type == \"Fillet\":\n",
    "        parsed = parse_fillet_callout(raw_text)\n",
    "        if \"fillet\" not in callout or not callout[\"fillet\"]:\n",
    "            callout[\"fillet\"] = {}\n",
    "\n",
    "        fillet = callout[\"fillet\"]\n",
    "        if not fillet.get(\"radiusMm\") and parsed[\"radiusMm\"]:\n",
    "            fillet[\"radiusMm\"] = parsed[\"radiusMm\"]\n",
    "        callout[\"fillet\"] = fillet\n",
    "\n",
    "    elif callout_type == \"Chamfer\":\n",
    "        parsed = parse_chamfer_callout(raw_text)\n",
    "        if \"chamfer\" not in callout or not callout[\"chamfer\"]:\n",
    "            callout[\"chamfer\"] = {}\n",
    "\n",
    "        chamfer = callout[\"chamfer\"]\n",
    "        if not chamfer.get(\"distance1Mm\") and parsed[\"distance1Mm\"]:\n",
    "            chamfer[\"distance1Mm\"] = parsed[\"distance1Mm\"]\n",
    "        if not chamfer.get(\"angleDegrees\") and parsed[\"angleDegrees\"]:\n",
    "            chamfer[\"angleDegrees\"] = parsed[\"angleDegrees\"]\n",
    "        if not chamfer.get(\"distance2Mm\") and parsed[\"distance2Mm\"]:\n",
    "            chamfer[\"distance2Mm\"] = parsed[\"distance2Mm\"]\n",
    "        callout[\"chamfer\"] = chamfer\n",
    "\n",
    "    # Generate canonical form\n",
    "    callout[\"canonical\"] = generate_canonical_form(callout)\n",
    "\n",
    "    # Add validation warnings\n",
    "    callout[\"validationWarnings\"] = generate_callout_warnings(callout)\n",
    "\n",
    "    return callout\n",
    "\n",
    "\n",
    "def generate_callout_warnings(callout: Dict) -> List[str]:\n",
    "    \"\"\"Generate soft validation warnings for a callout.\"\"\"\n",
    "    warnings = []\n",
    "    callout_type = callout.get(\"calloutType\", \"\")\n",
    "\n",
    "    if callout_type in [\"Hole\", \"TappedHole\"]:\n",
    "        hole = callout.get(\"hole\", {})\n",
    "        if not hole.get(\"diameterMm\"):\n",
    "            warnings.append(\"Missing diameter\")\n",
    "        if not hole.get(\"isThrough\") and not hole.get(\"depthMm\"):\n",
    "            warnings.append(\"Blind hole without depth\")\n",
    "\n",
    "        if callout_type == \"TappedHole\":\n",
    "            thread = callout.get(\"thread\", {})\n",
    "            if not thread.get(\"standard\"):\n",
    "                warnings.append(\"Thread standard not identified\")\n",
    "            if not thread.get(\"pitch\") and not thread.get(\"tpi\"):\n",
    "                warnings.append(\"Thread pitch/TPI not specified\")\n",
    "\n",
    "    elif callout_type == \"Fillet\":\n",
    "        fillet = callout.get(\"fillet\", {})\n",
    "        if not fillet.get(\"radiusMm\"):\n",
    "            warnings.append(\"Missing radius\")\n",
    "\n",
    "    elif callout_type == \"Chamfer\":\n",
    "        chamfer = callout.get(\"chamfer\", {})\n",
    "        if not chamfer.get(\"distance1Mm\"):\n",
    "            warnings.append(\"Missing chamfer distance\")\n",
    "\n",
    "    # Low confidence warning\n",
    "    if callout.get(\"confidence\", 1.0) < 0.5:\n",
    "        warnings.append(f\"Low confidence: {callout.get('confidence', 0):.2f}\")\n",
    "\n",
    "    return warnings\n",
    "\n",
    "\n",
    "def canonicalize_all_callouts(callouts: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Apply canonicalization to all callouts in a list.\n",
    "    \"\"\"\n",
    "    return [canonicalize_callout(c) for c in callouts]\n",
    "\n",
    "\n",
    "def values_match(val1: float, val2: float, tolerance_key: str) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Compare two values within tolerance.\n",
    "    Returns (matches, score).\n",
    "    \"\"\"\n",
    "    if val1 is None or val2 is None:\n",
    "        return False, 0.0\n",
    "\n",
    "    tol = TOLERANCES.get(tolerance_key, {})\n",
    "\n",
    "    if tol.get('exact'):\n",
    "        matches = val1 == val2\n",
    "        return matches, 1.0 if matches else 0.0\n",
    "\n",
    "    diff = abs(val1 - val2)\n",
    "    abs_tol = tol.get('absolute', 0.1)\n",
    "    pct_tol = tol.get('percent', 1.0)\n",
    "\n",
    "    pct_diff = (diff / val1 * 100) if val1 > 0 else 100\n",
    "\n",
    "    # Match if within either absolute OR percentage tolerance\n",
    "    if diff <= abs_tol or pct_diff <= pct_tol:\n",
    "        score = 1.0 - (diff / (abs_tol * 2))\n",
    "        return True, max(0.5, min(1.0, score))\n",
    "\n",
    "    return False, 0.0\n",
    "\n",
    "\n",
    "print(\"Deterministic canonicalization post-processor defined.\")"
   ],
   "outputs": [],
   "id": "TOlbkULR3Qt9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtjEaEKj3Qt9"
   },
   "source": [
    "---\n",
    "## Section 6: Rule Matcher -> DiffResult"
   ],
   "id": "AtjEaEKj3Qt9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioN0Dx423Qt9"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 6A: Canonicalization & Matching Rules\n",
    "# ============================================================\n",
    "\n",
    "# Constants for canonicalization\n",
    "INCH_TO_MM = 25.4\n",
    "DIAMETER_SYMBOL = \"\\u00f8\"  # U+00F8\n",
    "DEGREE_SYMBOL = \"\\u00b0\"    # U+00B0\n",
    "\n",
    "# Common fractions\n",
    "COMMON_FRACTIONS = {\n",
    "    \"1/64\": 0.015625, \"1/32\": 0.03125, \"1/16\": 0.0625, \"3/32\": 0.09375,\n",
    "    \"1/8\": 0.125, \"5/32\": 0.15625, \"3/16\": 0.1875, \"7/32\": 0.21875,\n",
    "    \"1/4\": 0.25, \"9/32\": 0.28125, \"5/16\": 0.3125, \"11/32\": 0.34375,\n",
    "    \"3/8\": 0.375, \"13/32\": 0.40625, \"7/16\": 0.4375, \"15/32\": 0.46875,\n",
    "    \"1/2\": 0.5, \"17/32\": 0.53125, \"9/16\": 0.5625, \"19/32\": 0.59375,\n",
    "    \"5/8\": 0.625, \"21/32\": 0.65625, \"11/16\": 0.6875, \"23/32\": 0.71875,\n",
    "    \"3/4\": 0.75, \"25/32\": 0.78125, \"13/16\": 0.8125, \"27/32\": 0.84375,\n",
    "    \"7/8\": 0.875, \"29/32\": 0.90625, \"15/16\": 0.9375, \"31/32\": 0.96875,\n",
    "}\n",
    "\n",
    "# Matching tolerances (from schema)\n",
    "TOLERANCES = {\n",
    "    \"diameter_mm\": {\"absolute\": 0.15, \"percent\": 0.5},\n",
    "    \"depth_mm\": {\"absolute\": 0.5, \"percent\": 2.0},\n",
    "    \"radius_mm\": {\"absolute\": 0.05, \"percent\": 5.0},\n",
    "    \"angle_deg\": {\"absolute\": 1.0},\n",
    "    \"thread_pitch\": {\"exact\": True},\n",
    "    \"quantity\": {\"exact\": True}\n",
    "}\n",
    "\n",
    "\n",
    "def values_match(val1: float, val2: float, tolerance_key: str) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Compare two values within tolerance.\n",
    "    Returns (matches, score).\n",
    "    \"\"\"\n",
    "    if val1 is None or val2 is None:\n",
    "        return False, 0.0\n",
    "\n",
    "    tol = TOLERANCES.get(tolerance_key, {})\n",
    "\n",
    "    if tol.get('exact'):\n",
    "        matches = val1 == val2\n",
    "        return matches, 1.0 if matches else 0.0\n",
    "\n",
    "    diff = abs(val1 - val2)\n",
    "    abs_tol = tol.get('absolute', 0.1)\n",
    "    pct_tol = tol.get('percent', 1.0)\n",
    "\n",
    "    pct_diff = (diff / val1 * 100) if val1 > 0 else 100\n",
    "\n",
    "    # Match if within either absolute OR percentage tolerance\n",
    "    if diff <= abs_tol or pct_diff <= pct_tol:\n",
    "        score = 1.0 - (diff / (abs_tol * 2))  # Score based on how close\n",
    "        return True, max(0.5, min(1.0, score))\n",
    "\n",
    "    return False, 0.0\n",
    "\n",
    "\n",
    "def normalize_thread_callout(raw: str) -> str:\n",
    "    \"\"\"Normalize thread callout for comparison.\"\"\"\n",
    "    # Uppercase and clean\n",
    "    s = raw.upper().strip()\n",
    "    # Normalize separators\n",
    "    s = re.sub(r'\\s*[Xx]\\s*', 'x', s)\n",
    "    s = re.sub(r'\\s*-\\s*', '-', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_hole_callout(callout: Dict) -> str:\n",
    "    \"\"\"Generate canonical form for hole callout.\"\"\"\n",
    "    hole = callout.get('hole', {})\n",
    "\n",
    "    dia_mm = hole.get('diameterMm')\n",
    "    if dia_mm is None:\n",
    "        return callout.get('rawText', '')\n",
    "\n",
    "    canonical = f\"{DIAMETER_SYMBOL}{dia_mm:.2f}mm\"\n",
    "\n",
    "    if hole.get('isThrough'):\n",
    "        canonical += \" THRU\"\n",
    "    elif hole.get('depthMm'):\n",
    "        canonical += f\" x {hole['depthMm']:.1f}mm DEEP\"\n",
    "\n",
    "    qty = callout.get('quantity', 1)\n",
    "    if qty > 1:\n",
    "        canonical += f\" ({qty}X)\"\n",
    "\n",
    "    return canonical\n",
    "\n",
    "print(\"Canonicalization functions defined.\")"
   ],
   "outputs": [],
   "id": "ioN0Dx423Qt9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73ewmDrz3Qt9"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 6B: DiffResult Generation\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class MatchResult:\n",
    "    \"\"\"Result of matching a SW requirement to drawing evidence.\"\"\"\n",
    "    sw_item: Dict[str, Any]\n",
    "    evidence_item: Optional[Dict[str, Any]]\n",
    "    status: str  # \"matched\", \"missing\", \"conflict\", \"ambiguous\"\n",
    "    score: float\n",
    "    notes: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def match_holes(sw_holes: List[Dict], evidence_callouts: List[Dict]) -> List[MatchResult]:\n",
    "    \"\"\"Match SolidWorks holes to drawing evidence.\"\"\"\n",
    "    results = []\n",
    "    evidence_holes = [c for c in evidence_callouts if c.get('calloutType') in ['Hole', 'TappedHole']]\n",
    "    used_evidence = set()\n",
    "\n",
    "    for sw_hole in sw_holes:\n",
    "        sw_dia = sw_hole.get('diameter_mm') or sw_hole.get('diameterMm')\n",
    "        sw_type = sw_hole.get('type', 'Through')\n",
    "        sw_depth = sw_hole.get('depth_mm') or sw_hole.get('depthMm')\n",
    "        sw_count = sw_hole.get('count', 1)\n",
    "\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        best_idx = -1\n",
    "\n",
    "        for idx, ev_hole in enumerate(evidence_holes):\n",
    "            if idx in used_evidence:\n",
    "                continue\n",
    "\n",
    "            hole_data = ev_hole.get('hole', {})\n",
    "            ev_dia = hole_data.get('diameterMm')\n",
    "            ev_through = hole_data.get('isThrough', False)\n",
    "            ev_depth = hole_data.get('depthMm')\n",
    "            ev_count = ev_hole.get('quantity', 1)\n",
    "\n",
    "            # Check diameter match\n",
    "            dia_match, dia_score = values_match(sw_dia, ev_dia, 'diameter_mm')\n",
    "            if not dia_match:\n",
    "                continue\n",
    "\n",
    "            # Check type match\n",
    "            sw_is_through = sw_type.lower() in ['through', 'thru']\n",
    "            if sw_is_through != ev_through:\n",
    "                continue\n",
    "\n",
    "            # Check depth for blind holes\n",
    "            depth_score = 1.0\n",
    "            if not sw_is_through and sw_depth and ev_depth:\n",
    "                depth_match, depth_score = values_match(sw_depth, ev_depth, 'depth_mm')\n",
    "                if not depth_match:\n",
    "                    continue\n",
    "\n",
    "            # Calculate overall score\n",
    "            score = (dia_score + depth_score) / 2\n",
    "            if sw_count == ev_count:\n",
    "                score *= 1.0\n",
    "            else:\n",
    "                score *= 0.8  # Quantity mismatch penalty\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = ev_hole\n",
    "                best_idx = idx\n",
    "\n",
    "        if best_match:\n",
    "            used_evidence.add(best_idx)\n",
    "            notes = []\n",
    "            if sw_count != best_match.get('quantity', 1):\n",
    "                notes.append(f\"Quantity: SW={sw_count}, Drawing={best_match.get('quantity', 1)}\")\n",
    "            results.append(MatchResult(\n",
    "                sw_item=sw_hole,\n",
    "                evidence_item=best_match,\n",
    "                status=\"matched\",\n",
    "                score=best_score,\n",
    "                notes=notes\n",
    "            ))\n",
    "        else:\n",
    "            results.append(MatchResult(\n",
    "                sw_item=sw_hole,\n",
    "                evidence_item=None,\n",
    "                status=\"missing\",\n",
    "                score=0.0,\n",
    "                notes=[f\"No matching hole found for {DIAMETER_SYMBOL}{sw_dia:.2f}mm\"]\n",
    "            ))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_diff_result(\n",
    "    drawing_evidence: Dict[str, Any],\n",
    "    sw_data: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare DrawingEvidence to SolidWorks truth and generate DiffResult.\n",
    "    \"\"\"\n",
    "    matched = []\n",
    "    missing = []\n",
    "    conflicts = []\n",
    "    ambiguous = []\n",
    "    extras = []\n",
    "\n",
    "    # Get SW requirements\n",
    "    sw_comparison = sw_data.get('comparison', {})\n",
    "    sw_hole_groups = sw_comparison.get('holeGroups', [])\n",
    "    sw_features = sw_data.get('features', {})\n",
    "    sw_fillets = sw_features.get('fillets', [])\n",
    "    sw_chamfers = sw_features.get('chamfers', [])\n",
    "\n",
    "    # Get drawing evidence\n",
    "    evidence_callouts = drawing_evidence.get('foundCallouts', [])\n",
    "\n",
    "    # Match holes\n",
    "    if sw_hole_groups:\n",
    "        print(f\"  Matching {len(sw_hole_groups)} hole groups...\")\n",
    "        hole_results = match_holes(sw_hole_groups, evidence_callouts)\n",
    "        for r in hole_results:\n",
    "            item = {\n",
    "                \"type\": \"hole\",\n",
    "                \"swRequirement\": r.sw_item,\n",
    "                \"drawingEvidence\": r.evidence_item,\n",
    "                \"score\": r.score,\n",
    "                \"notes\": r.notes\n",
    "            }\n",
    "            if r.status == \"matched\":\n",
    "                matched.append(item)\n",
    "            elif r.status == \"missing\":\n",
    "                missing.append(item)\n",
    "            elif r.status == \"conflict\":\n",
    "                conflicts.append(item)\n",
    "\n",
    "    # Match fillets (simplified)\n",
    "    evidence_fillets = [c for c in evidence_callouts if c.get('calloutType') == 'Fillet']\n",
    "    for sw_fillet in sw_fillets:\n",
    "        sw_radius = sw_fillet.get('radius_mm') or sw_fillet.get('radiusMm')\n",
    "        found = False\n",
    "        for ev_fillet in evidence_fillets:\n",
    "            ev_radius = ev_fillet.get('fillet', {}).get('radiusMm')\n",
    "            match, score = values_match(sw_radius, ev_radius, 'radius_mm')\n",
    "            if match:\n",
    "                matched.append({\n",
    "                    \"type\": \"fillet\",\n",
    "                    \"swRequirement\": sw_fillet,\n",
    "                    \"drawingEvidence\": ev_fillet,\n",
    "                    \"score\": score,\n",
    "                    \"notes\": []\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing.append({\n",
    "                \"type\": \"fillet\",\n",
    "                \"swRequirement\": sw_fillet,\n",
    "                \"drawingEvidence\": None,\n",
    "                \"score\": 0.0,\n",
    "                \"notes\": [f\"Fillet R{sw_radius:.2f}mm not found\"]\n",
    "            })\n",
    "\n",
    "    # Summary\n",
    "    total_sw = len(sw_hole_groups) + len(sw_fillets) + len(sw_chamfers)\n",
    "\n",
    "    diff_result = {\n",
    "        \"matched\": matched,\n",
    "        \"missing\": missing,\n",
    "        \"conflicts\": conflicts,\n",
    "        \"ambiguous\": ambiguous,\n",
    "        \"extras\": extras,\n",
    "        \"summary\": {\n",
    "            \"totalSwRequirements\": total_sw,\n",
    "            \"matchedCount\": len(matched),\n",
    "            \"missingCount\": len(missing),\n",
    "            \"conflictCount\": len(conflicts),\n",
    "            \"ambiguousCount\": len(ambiguous),\n",
    "            \"extraCount\": len(extras),\n",
    "            \"matchRate\": len(matched) / max(total_sw, 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return diff_result\n",
    "\n",
    "print(\"DiffResult generation functions defined.\")"
   ],
   "outputs": [],
   "id": "73ewmDrz3Qt9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8AT8oaA3Qt9"
   },
   "source": [
    "---\n",
    "## Section 7: Text LLM Judge -> QCReport"
   ],
   "id": "n8AT8oaA3Qt9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YNtTHqd3Qt9"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 7A: QC Report Generation\n",
    "# ============================================================\n",
    "\n",
    "def generate_qc_report(\n",
    "    resolved_identity: ResolvedPartIdentity,\n",
    "    drawing_evidence: Dict[str, Any],\n",
    "    diff_result: Dict[str, Any],\n",
    "    sw_data: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate human-readable QC report in Markdown format.\n",
    "\n",
    "    Uses VLM as judge to provide verdict and recommendations.\n",
    "    \"\"\"\n",
    "    global vlm_model, vlm_processor\n",
    "\n",
    "    summary = diff_result.get('summary', {})\n",
    "\n",
    "    # Build context for judge\n",
    "    sw_identity = sw_data.get('identity', {})\n",
    "\n",
    "    judge_context = f\"\"\"\n",
    "**Part Information:**\n",
    "- Part Number: {resolved_identity.resolvedPartNumber or 'Unknown'}\n",
    "- Description: {sw_identity.get('description', 'Unknown')}\n",
    "- Material: {sw_identity.get('material', 'Unknown')}\n",
    "\n",
    "**Comparison Summary:**\n",
    "- Total SW Requirements: {summary.get('totalSwRequirements', 0)}\n",
    "- Matched: {summary.get('matchedCount', 0)}\n",
    "- Missing: {summary.get('missingCount', 0)}\n",
    "- Conflicts: {summary.get('conflictCount', 0)}\n",
    "- Match Rate: {summary.get('matchRate', 0):.1%}\n",
    "\n",
    "**Missing Items:**\n",
    "\"\"\"\n",
    "\n",
    "    for item in diff_result.get('missing', [])[:10]:\n",
    "        sw_req = item.get('swRequirement', {})\n",
    "        item_type = item.get('type', 'unknown')\n",
    "        if item_type == 'hole':\n",
    "            dia = sw_req.get('diameter_mm') or sw_req.get('diameterMm', 0)\n",
    "            judge_context += f\"- Hole: {DIAMETER_SYMBOL}{dia:.2f}mm\\n\"\n",
    "        elif item_type == 'fillet':\n",
    "            rad = sw_req.get('radius_mm') or sw_req.get('radiusMm', 0)\n",
    "            judge_context += f\"- Fillet: R{rad:.2f}mm\\n\"\n",
    "        else:\n",
    "            judge_context += f\"- {item_type}: {item.get('notes', [])}\\n\"\n",
    "\n",
    "    # Determine verdict based on match rate\n",
    "    match_rate = summary.get('matchRate', 0)\n",
    "    missing_count = summary.get('missingCount', 0)\n",
    "    conflict_count = summary.get('conflictCount', 0)\n",
    "\n",
    "    if match_rate >= 0.95 and conflict_count == 0:\n",
    "        verdict = \"PASS\"\n",
    "        verdict_reason = \"All critical requirements verified\"\n",
    "    elif match_rate >= 0.7 and conflict_count == 0:\n",
    "        verdict = \"NEEDS_REVIEW\"\n",
    "        verdict_reason = f\"{missing_count} items could not be verified\"\n",
    "    else:\n",
    "        verdict = \"FAIL\"\n",
    "        if conflict_count > 0:\n",
    "            verdict_reason = f\"{conflict_count} conflicting dimensions found\"\n",
    "        else:\n",
    "            verdict_reason = f\"{missing_count} required items missing from drawing\"\n",
    "\n",
    "    # Build report\n",
    "    report = f\"\"\"# QC Inspection Report\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Part Identification\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| Part Number | {resolved_identity.resolvedPartNumber or 'Unknown'} |\n",
    "| Description | {sw_identity.get('description', 'Unknown')} |\n",
    "| Material | {sw_identity.get('material', 'Unknown')} |\n",
    "| Drawing File | {os.path.basename(DRAWING_PDF_PATH)} |\n",
    "| SW JSON | {os.path.basename(resolved_identity.jsonPath or 'Not found')} |\n",
    "| ID Confidence | {resolved_identity.confidence:.0%} |\n",
    "\n",
    "## Verdict\n",
    "\n",
    "### **{verdict}**\n",
    "\n",
    "{verdict_reason}\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Metric | Count |\n",
    "|--------|-------|\n",
    "| Total SW Requirements | {summary.get('totalSwRequirements', 0)} |\n",
    "| Matched | {summary.get('matchedCount', 0)} |\n",
    "| Missing | {summary.get('missingCount', 0)} |\n",
    "| Conflicts | {summary.get('conflictCount', 0)} |\n",
    "| **Match Rate** | **{summary.get('matchRate', 0):.1%}** |\n",
    "\n",
    "## Matched Items\n",
    "\n",
    "| Type | SW Requirement | Drawing Evidence | Score |\n",
    "|------|----------------|------------------|-------|\n",
    "\"\"\"\n",
    "\n",
    "    for item in diff_result.get('matched', [])[:20]:\n",
    "        sw_req = item.get('swRequirement', {})\n",
    "        ev = item.get('drawingEvidence', {})\n",
    "        item_type = item.get('type', 'unknown')\n",
    "        score = item.get('score', 0)\n",
    "\n",
    "        if item_type == 'hole':\n",
    "            sw_str = f\"{DIAMETER_SYMBOL}{sw_req.get('diameter_mm', sw_req.get('diameterMm', 0)):.2f}mm\"\n",
    "            ev_str = ev.get('rawText', '') if ev else 'N/A'\n",
    "        else:\n",
    "            sw_str = str(sw_req)\n",
    "            ev_str = ev.get('rawText', '') if ev else 'N/A'\n",
    "\n",
    "        report += f\"| {item_type} | {sw_str} | {ev_str[:30]} | {score:.0%} |\\n\"\n",
    "\n",
    "    report += \"\\n## Missing Items\\n\\n\"\n",
    "\n",
    "    if diff_result.get('missing'):\n",
    "        report += \"| Type | SW Requirement | Notes |\\n\"\n",
    "        report += \"|------|----------------|-------|\\n\"\n",
    "\n",
    "        for item in diff_result.get('missing', []):\n",
    "            sw_req = item.get('swRequirement', {})\n",
    "            item_type = item.get('type', 'unknown')\n",
    "            notes = '; '.join(item.get('notes', []))\n",
    "\n",
    "            if item_type == 'hole':\n",
    "                sw_str = f\"{DIAMETER_SYMBOL}{sw_req.get('diameter_mm', sw_req.get('diameterMm', 0)):.2f}mm\"\n",
    "            elif item_type == 'fillet':\n",
    "                sw_str = f\"R{sw_req.get('radius_mm', sw_req.get('radiusMm', 0)):.2f}mm\"\n",
    "            else:\n",
    "                sw_str = str(sw_req)[:30]\n",
    "\n",
    "            report += f\"| {item_type} | {sw_str} | {notes} |\\n\"\n",
    "    else:\n",
    "        report += \"*No missing items*\\n\"\n",
    "\n",
    "    if diff_result.get('conflicts'):\n",
    "        report += \"\\n## Conflicts\\n\\n\"\n",
    "        report += \"| Type | SW Requirement | Drawing Evidence | Issue |\\n\"\n",
    "        report += \"|------|----------------|------------------|-------|\\n\"\n",
    "\n",
    "        for item in diff_result.get('conflicts', []):\n",
    "            report += f\"| {item.get('type')} | ... | ... | {item.get('notes', [])} |\\n\"\n",
    "\n",
    "    report += f\"\"\"\\n## Recommendations\\n\n",
    "\"\"\"\n",
    "\n",
    "    if verdict == \"PASS\":\n",
    "        report += \"- Drawing appears complete and matches SW model\\n\"\n",
    "        report += \"- No action required\\n\"\n",
    "    elif verdict == \"NEEDS_REVIEW\":\n",
    "        report += \"- Manual review recommended for unverified items\\n\"\n",
    "        report += \"- Check if missing callouts are on different views/sheets\\n\"\n",
    "        report += \"- Verify OCR accuracy for complex dimensions\\n\"\n",
    "    else:\n",
    "        report += \"- Drawing requires updates before release\\n\"\n",
    "        report += \"- Add missing callouts as listed above\\n\"\n",
    "        report += \"- Resolve any dimension conflicts\\n\"\n",
    "\n",
    "    if resolved_identity.needsReview:\n",
    "        report += f\"\\n**Note:** Part identification has low confidence ({resolved_identity.confidence:.0%}). \"\n",
    "        report += \"Verify the correct part was matched.\\n\"\n",
    "\n",
    "    report += \"\\n---\\n*Generated by AI Drawing Inspector v2.0*\\n\"\n",
    "\n",
    "    return report\n",
    "\n",
    "print(\"QC report generation function defined.\")"
   ],
   "outputs": [],
   "id": "-YNtTHqd3Qt9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wupCMOFQ3Qt9"
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 8B: Main Pipeline Function (with All Upgrades)\n",
    "# ============================================================\n",
    "\n",
    "def run_qc_pipeline(pdf_path: str, mode: str = \"fast\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the complete QC pipeline on a single drawing.\n",
    "    \n",
    "    Integrates all 4 upgrades:\n",
    "    1. Real OCR bounding boxes (Tesseract + LightOnOCR hybrid)\n",
    "    2. Title-block / notes-region ROI OCR\n",
    "    3. Explicit unit detection + normalization\n",
    "    4. Deterministic canonicalization post-processor\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to drawing PDF\n",
    "        mode: \"fast\" (first page) or \"full\" (all pages)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with all artifacts and file paths\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"AI DRAWING INSPECTOR v2.0 (with Upgrades)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input: {pdf_path}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: PDF Ingestion\n",
    "    print(\"\\n[1/8] PDF Ingestion...\")\n",
    "    page_artifacts = render_pdf_to_artifacts(pdf_path, mode=mode)\n",
    "    if not page_artifacts:\n",
    "        return {\"error\": \"Failed to render PDF\"}\n",
    "    \n",
    "    # Step 2: OCR Extraction (Upgrade 1: Real bounding boxes)\n",
    "    print(\"\\n[2/8] OCR Extraction (with real bounding boxes)...\")\n",
    "    ocr_blocks = run_ocr_on_all_pages(page_artifacts)\n",
    "    ocr_lines = [b.text for b in ocr_blocks]\n",
    "    print(f\"  Total OCR blocks: {len(ocr_blocks)}\")\n",
    "    \n",
    "    # Count blocks with real bboxes\n",
    "    blocks_with_bbox = sum(1 for b in ocr_blocks if b.bbox and b.bbox.get('width'))\n",
    "    print(f\"  Blocks with real bboxes: {blocks_with_bbox}/{len(ocr_blocks)}\")\n",
    "    \n",
    "    # Step 3: ROI Detection (Upgrade 2: Title block & notes regions)\n",
    "    print(\"\\n[3/8] ROI Detection (title block & notes regions)...\")\n",
    "    title_block_region = None\n",
    "    notes_region = None\n",
    "    title_block_identity = {}\n",
    "    \n",
    "    if page_artifacts:\n",
    "        # Detect regions on first page\n",
    "        first_page = page_artifacts[0]\n",
    "        \n",
    "        title_block_region = detect_title_block_region(first_page, ocr_blocks)\n",
    "        if title_block_region:\n",
    "            print(f\"  Title block found at y={title_block_region.bbox_norm['y']:.2f}\")\n",
    "            print(f\"    Region text samples: {title_block_region.ocr_text[:3]}...\")\n",
    "            \n",
    "            # Extract structured identity from title block\n",
    "            title_block_identity = extract_identity_from_title_block(title_block_region, ocr_blocks)\n",
    "            if title_block_identity.get('partNumber'):\n",
    "                print(f\"    Part Number (from title): {title_block_identity['partNumber']}\")\n",
    "            if title_block_identity.get('material'):\n",
    "                print(f\"    Material (from title): {title_block_identity['material']}\")\n",
    "        else:\n",
    "            print(\"  No title block detected\")\n",
    "        \n",
    "        notes_region = detect_notes_region(first_page, ocr_blocks)\n",
    "        if notes_region:\n",
    "            print(f\"  Notes region found at x={notes_region.bbox_norm['x']:.2f}\")\n",
    "            notes_content = extract_notes_content(notes_region, ocr_blocks)\n",
    "            print(f\"    Found {len(notes_content)} structured notes\")\n",
    "        else:\n",
    "            print(\"  No notes region detected\")\n",
    "    \n",
    "    # Step 4: Unit Detection (Upgrade 3)\n",
    "    print(\"\\n[4/8] Unit Detection...\")\n",
    "    detected_units = detect_units_from_text(ocr_lines)\n",
    "    print(f\"  Detected unit: {detected_units.detected_unit.value}\")\n",
    "    print(f\"  Confidence: {detected_units.confidence:.0%}\")\n",
    "    if detected_units.evidence:\n",
    "        print(f\"  Evidence: {detected_units.evidence[:2]}\")\n",
    "    \n",
    "    # Step 5: Resolve Part Identity\n",
    "    print(\"\\n[5/8] Resolving Part Identity...\")\n",
    "    resolved_identity = resolve_part_identity(pdf_path, ocr_lines)\n",
    "    \n",
    "    # Merge with title block identity\n",
    "    if title_block_identity.get('partNumber') and not resolved_identity.resolvedPartNumber:\n",
    "        # Try to find match for title block part number\n",
    "        match = sw_library.lookup(title_block_identity['partNumber']) if sw_library else None\n",
    "        if match:\n",
    "            resolved_identity = ResolvedPartIdentity(\n",
    "                resolvedPartNumber=title_block_identity['partNumber'],\n",
    "                jsonPath=match.json_path,\n",
    "                confidence=0.85,\n",
    "                candidates=resolved_identity.candidates,\n",
    "                notes=[\"Matched from title block OCR\"] + resolved_identity.notes,\n",
    "                needsReview=False\n",
    "            )\n",
    "    \n",
    "    print(f\"  Resolved: {resolved_identity.resolvedPartNumber}\")\n",
    "    print(f\"  Confidence: {resolved_identity.confidence:.0%}\")\n",
    "    print(f\"  JSON Path: {resolved_identity.jsonPath or 'Not found'}\")\n",
    "    if resolved_identity.needsReview:\n",
    "        print(f\"  WARNING: Needs review - {resolved_identity.notes}\")\n",
    "    \n",
    "    # Load SW data (using BOM-robust loader)\n",
    "    sw_data = {}\n",
    "    if resolved_identity.jsonPath and os.path.exists(resolved_identity.jsonPath):\n",
    "        sw_data, load_error = load_json_robust(Path(resolved_identity.jsonPath))\n",
    "        if sw_data:\n",
    "            print(f\"  SW data loaded: {len(sw_data.get('comparison', {}).get('holeGroups', []))} hole groups\")\n",
    "        else:\n",
    "            print(f\"  Warning: Could not load SW data: {load_error}\")\n",
    "            sw_data = {}\n",
    "    \n",
    "    # Step 6: VLM Evidence Extraction (with Upgrade 4: Canonicalization)\n",
    "    print(\"\\n[6/8] VLM Evidence Extraction (with canonicalization)...\")\n",
    "    drawing_evidence = extract_evidence_with_vlm(\n",
    "        page_artifacts,\n",
    "        ocr_blocks,\n",
    "        resolved_identity,\n",
    "        detected_units=detected_units,\n",
    "        title_block_identity=title_block_identity\n",
    "    )\n",
    "    print(f\"  Found callouts: {len(drawing_evidence.get('foundCallouts', []))}\")\n",
    "    print(f\"  Found notes: {len(drawing_evidence.get('foundNotes', []))}\")\n",
    "    \n",
    "    # Show canonicalization results\n",
    "    callouts = drawing_evidence.get('foundCallouts', [])\n",
    "    canonical_count = sum(1 for c in callouts if c.get('canonical'))\n",
    "    print(f\"  Callouts with canonical form: {canonical_count}/{len(callouts)}\")\n",
    "    \n",
    "    # Show unit conversion status\n",
    "    if drawing_evidence.get('unitDetection', {}).get('conversionApplied'):\n",
    "        print(\"  Unit conversion: INCHES -> MM applied\")\n",
    "    \n",
    "    # Step 7: Diff Generation\n",
    "    print(\"\\n[7/8] Generating Diff...\")\n",
    "    if sw_data:\n",
    "        diff_result = generate_diff_result(drawing_evidence, sw_data)\n",
    "        summary = diff_result.get('summary', {})\n",
    "        print(f\"  Matched: {summary.get('matchedCount', 0)}\")\n",
    "        print(f\"  Missing: {summary.get('missingCount', 0)}\")\n",
    "        print(f\"  Match Rate: {summary.get('matchRate', 0):.1%}\")\n",
    "    else:\n",
    "        diff_result = {\n",
    "            \"matched\": [], \"missing\": [], \"conflicts\": [],\n",
    "            \"ambiguous\": [], \"extras\": [],\n",
    "            \"summary\": {\"totalSwRequirements\": 0, \"matchedCount\": 0,\n",
    "                       \"missingCount\": 0, \"conflictCount\": 0,\n",
    "                       \"ambiguousCount\": 0, \"extraCount\": 0, \"matchRate\": 0}\n",
    "        }\n",
    "        print(\"  No SW data available for comparison\")\n",
    "    \n",
    "    # Step 8: Generate QC Report\n",
    "    print(\"\\n[8/8] Generating QC Report...\")\n",
    "    qc_report = generate_qc_report(resolved_identity, drawing_evidence, diff_result, sw_data)\n",
    "    \n",
    "    # Determine verdict from report\n",
    "    if \"### **PASS**\" in qc_report:\n",
    "        verdict = \"PASS\"\n",
    "    elif \"### **NEEDS_REVIEW**\" in qc_report:\n",
    "        verdict = \"NEEDS_REVIEW\"\n",
    "    else:\n",
    "        verdict = \"FAIL\"\n",
    "    print(f\"  Verdict: {verdict}\")\n",
    "    \n",
    "    # Save Outputs\n",
    "    print(\"\\n[SAVE] Saving Outputs...\")\n",
    "    saved_paths = save_outputs(\n",
    "        resolved_identity, drawing_evidence, diff_result, qc_report, OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    # Print upgrade summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE - UPGRADE STATUS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"[\u2713] Upgrade 1: Real OCR bboxes ({blocks_with_bbox}/{len(ocr_blocks)} blocks)\")\n",
    "    print(f\"[\u2713] Upgrade 2: ROI Detection (title_block={title_block_region is not None}, notes={notes_region is not None})\")\n",
    "    print(f\"[\u2713] Upgrade 3: Unit Detection ({detected_units.detected_unit.value}, {detected_units.confidence:.0%})\")\n",
    "    print(f\"[\u2713] Upgrade 4: Canonicalization ({canonical_count}/{len(callouts)} callouts)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Verdict: {verdict}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    return {\n",
    "        \"verdict\": verdict,\n",
    "        \"resolved_identity\": asdict(resolved_identity),\n",
    "        \"drawing_evidence\": drawing_evidence,\n",
    "        \"diff_result\": diff_result,\n",
    "        \"qc_report\": qc_report,\n",
    "        \"saved_paths\": saved_paths,\n",
    "        \"upgrade_status\": {\n",
    "            \"real_bboxes\": {\"blocks_with_bbox\": blocks_with_bbox, \"total_blocks\": len(ocr_blocks)},\n",
    "            \"roi_detection\": {\n",
    "                \"title_block_found\": title_block_region is not None,\n",
    "                \"notes_region_found\": notes_region is not None\n",
    "            },\n",
    "            \"unit_detection\": {\n",
    "                \"detected_unit\": detected_units.detected_unit.value,\n",
    "                \"confidence\": detected_units.confidence,\n",
    "                \"conversion_applied\": detected_units.detected_unit == DrawingUnits.INCHES\n",
    "            },\n",
    "            \"canonicalization\": {\"callouts_canonicalized\": canonical_count, \"total_callouts\": len(callouts)}\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Main pipeline function with all upgrades defined.\")\n",
    "print(\"\\nUsage: result = run_qc_pipeline(DRAWING_PDF_PATH, mode=MODE)\")"
   ],
   "id": "wupCMOFQ3Qt9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzVyRc0O3Qt9",
    "outputId": "f120ce1d-d214-4a8d-da6a-01c2f6b6befd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 8A: Output Saving Functions\n",
    "# ============================================================\n",
    "\n",
    "def save_outputs(\n",
    "    resolved_identity: ResolvedPartIdentity,\n",
    "    drawing_evidence: Dict[str, Any],\n",
    "    diff_result: Dict[str, Any],\n",
    "    qc_report: str,\n",
    "    output_dir: str\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Save all 4 artifacts to output directory.\n",
    "\n",
    "    Returns dict of saved file paths.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Build filename prefix\n",
    "    pn = resolved_identity.resolvedPartNumber or \"unknown\"\n",
    "    drawing_name = os.path.splitext(os.path.basename(DRAWING_PDF_PATH))[0]\n",
    "    prefix = f\"{pn}_{drawing_name}\"\n",
    "    # Clean filename\n",
    "    prefix = re.sub(r'[^a-zA-Z0-9_-]', '_', prefix)\n",
    "\n",
    "    paths = {}\n",
    "\n",
    "    # 1. ResolvedPartIdentity.json\n",
    "    identity_path = os.path.join(output_dir, f\"{prefix}_ResolvedPartIdentity.json\")\n",
    "    with open(identity_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(asdict(resolved_identity), f, indent=2)\n",
    "    paths['identity'] = identity_path\n",
    "    print(f\"  Saved: {identity_path}\")\n",
    "\n",
    "    # 2. DrawingEvidence.json\n",
    "    evidence_path = os.path.join(output_dir, f\"{prefix}_DrawingEvidence.json\")\n",
    "    with open(evidence_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(drawing_evidence, f, indent=2)\n",
    "    paths['evidence'] = evidence_path\n",
    "    print(f\"  Saved: {evidence_path}\")\n",
    "\n",
    "    # Validate against schema\n",
    "    if DRAWING_EVIDENCE_SCHEMA:\n",
    "        try:\n",
    "            jsonschema.validate(drawing_evidence, DRAWING_EVIDENCE_SCHEMA)\n",
    "            print(f\"  Schema validation: PASSED\")\n",
    "        except jsonschema.ValidationError as e:\n",
    "            print(f\"  Schema validation: FAILED - {e.message}\")\n",
    "\n",
    "    # 3. DiffResult.json\n",
    "    diff_path = os.path.join(output_dir, f\"{prefix}_DiffResult.json\")\n",
    "    with open(diff_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(diff_result, f, indent=2)\n",
    "    paths['diff'] = diff_path\n",
    "    print(f\"  Saved: {diff_path}\")\n",
    "\n",
    "    # 4. QCReport.md\n",
    "    report_path = os.path.join(output_dir, f\"{prefix}_QCReport.md\")\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(qc_report)\n",
    "    paths['report'] = report_path\n",
    "    print(f\"  Saved: {report_path}\")\n",
    "\n",
    "    return paths\n",
    "\n",
    "print(\"Output saving function defined.\")"
   ],
   "outputs": [],
   "id": "jzVyRc0O3Qt9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEUvuVb83Qt9",
    "outputId": "cb8308b2-65ed-4423-8f67-58bb2676a7ab",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    }
   },
   "source": [
    "# ============================================================\n",
    "# SECTION 8C: Run Pipeline (Upload and Execute)\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Upload PDF if not set\n",
    "if not DRAWING_PDF_PATH or not os.path.exists(DRAWING_PDF_PATH):\n",
    "    print(\"Upload a drawing PDF to inspect:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if uploaded:\n",
    "        DRAWING_PDF_PATH = list(uploaded.keys())[0]\n",
    "        print(f\"\\nUsing: {DRAWING_PDF_PATH}\")\n",
    "    else:\n",
    "        print(\"No file uploaded.\")\n",
    "\n",
    "# Run pipeline\n",
    "if DRAWING_PDF_PATH and os.path.exists(DRAWING_PDF_PATH):\n",
    "    result = run_qc_pipeline(DRAWING_PDF_PATH, mode=MODE)\n",
    "\n",
    "    # Display report\n",
    "    if 'qc_report' in result:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"QC REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        display(Markdown(result['qc_report']))\n",
    "else:\n",
    "    print(\"No PDF file available. Set DRAWING_PDF_PATH or upload a file.\")"
   ],
   "outputs": [],
   "id": "NEUvuVb83Qt9"
  }
 ]
}