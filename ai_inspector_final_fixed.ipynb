{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFrLGYPswHHZ"
      },
      "source": [
        "# AI Engineering Drawing Inspector (Final Version)\n",
        "\n",
        "A context-aware GD&T checker that uses:\n",
        "- Part context from BOM structure\n",
        "- RAG retrieval from ASME Y14.5 standard\n",
        "- Qwen2-VL for visual inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiTvRp4rwHHa"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmkO97ziwHHa",
        "outputId": "3731f36c-5925-45bb-98e0-36c6a6a7bb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All packages installed!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate\n",
        "!pip install -q qwen-vl-utils\n",
        "!pip install -q pdf2image\n",
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install -q bitsandbytes\n",
        "!apt-get install -y poppler-utils > /dev/null 2>&1\n",
        "\n",
        "# Production Pipeline Dependencies (OCR + High-Res Rendering)\n",
        "!pip install -q pymupdf paddleocr paddlepaddle opencv-python-headless\n",
        "\n",
        "print(\"All packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_VXcblCwHHa",
        "outputId": "2cc92830-8741-41d8-a68e-08f0130fcbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Production Pipeline Imports\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmqDweB4wHHa"
      },
      "source": [
        "## 2. Load Model (Qwen2-VL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "2cb373474b194b8da3ef1dc6753df2e3",
            "da6c3848895042bb9985d5febae6250a",
            "f2c44e9a7b7744e682ae9b1959b31764",
            "40681f3c555c4b7c8151e328ebafcbb3",
            "93130fc6f2204866813a448860e0b8e3",
            "47c9fcd759d749e9866e8c5e4a42908b",
            "07ee67ba714d4787b7c3a8650421445f",
            "8ca9810080244d0aafac09c78ab027ea",
            "97f20662d5754c30818216f746845a52",
            "0d3c5bc3a43f40f595a4ecfc1da171a5",
            "4998675f0ff244c8a4309534a2fe7e09"
          ]
        },
        "id": "8fokt7vLwHHa",
        "outputId": "6d24f93f-89c6-4f8f-ecb4-6e15ccd20e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen/Qwen2-VL-72B-Instruct in 4-bit (NF4)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cb373474b194b8da3ef1dc6753df2e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Qwen2-VL-72B (4-bit) Loaded Successfully!\n",
            "Memory Footprint: 40.45 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "# === OPTION A: Qwen2-VL-72B (4-Bit Quantized) ===\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-72B-Instruct\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID} in 4-bit (NF4)...\")\n",
        "\n",
        "# Define 4-bit configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Load Model with SDPA (Native Flash Attention for PyTorch 2.0+)\n",
        "# This avoids needing to install the flash_attn library while keeping high speed on A100\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    attn_implementation=\"sdpa\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "print(\"‚úÖ Qwen2-VL-72B (4-bit) Loaded Successfully!\")\n",
        "print(f\"Memory Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YkOM8aOwHHa"
      },
      "source": [
        "## 3. Load Context Databases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK5ZAuphwHHa",
        "outputId": "c5530898-6106-4299-dfe9-2dc717d3cb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: Upload Configuration Files\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FILE STATUS:\n",
            "============================================================\n",
            "File Mapping:  OK\n",
            "Structure:     OK\n",
            "RAG Index:     OK\n",
            "\n",
            "Data directory: /content\n"
          ]
        }
      ],
      "source": [
        "# === CONFIGURATION - FILE UPLOAD ===\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Expected config filenames\n",
        "MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
        "STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
        "RAG_INDEX_FILE = \"asme_visual_index.pkl\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Upload Configuration Files\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Helper to locate a file (cwd or inside rag_data)\n",
        "def locate_file(filename):\n",
        "    # Check current directory\n",
        "    if os.path.exists(filename):\n",
        "        return os.path.abspath(filename)\n",
        "    # Check rag_data subdirectory\n",
        "    nested_path = os.path.join(\"rag_data\", filename)\n",
        "    if os.path.exists(nested_path):\n",
        "        return os.path.abspath(nested_path)\n",
        "    return None\n",
        "\n",
        "# Check for existing files first\n",
        "FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
        "STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
        "RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "# Upload missing files\n",
        "missing_files = []\n",
        "if not FILE_MAPPING_PATH:\n",
        "    missing_files.append(MAPPING_FILE)\n",
        "if not STRUCTURE_PATH:\n",
        "    missing_files.append(STRUCTURE_FILE)\n",
        "if not RAG_INDEX_PATH:\n",
        "    missing_files.append(RAG_INDEX_FILE)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n",
        "    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Check if a ZIP was uploaded\n",
        "    for filename in uploaded:\n",
        "        if filename.lower().endswith('.zip'):\n",
        "            print(f\"\\nExtracting {filename}...\")\n",
        "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall(\"rag_data\")\n",
        "            print(\"Extraction complete.\")\n",
        "            break\n",
        "\n",
        "    # Re-locate files after upload\n",
        "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n",
        "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n",
        "    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "# Set DATA_DIR\n",
        "if FILE_MAPPING_PATH:\n",
        "    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\n",
        "else:\n",
        "    DATA_DIR = \"/content\"\n",
        "\n",
        "# Print status\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FILE STATUS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"File Mapping:  {'OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else 'MISSING'}\")\n",
        "print(f\"Structure:     {'OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else 'MISSING'}\")\n",
        "print(f\"RAG Index:     {'OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else 'MISSING (optional)'}\")\n",
        "print(f\"\\nData directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykrHElKlwHHb",
        "outputId": "7504d772-a2b4-4a52-fab3-839da9f64bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading file mapping...\n",
            "  Loaded 260 file mappings\n",
            "Loading part structure...\n",
            "Building part context database...\n",
            "  Built context for 127 unique parts\n",
            "\n",
            "Context databases loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# === PART 1: GLOBAL LOADING ===\n",
        "\n",
        "def normalize_pn(pn):\n",
        "    \"\"\"Normalize part number for lookup (remove dashes, spaces, lowercase).\"\"\"\n",
        "    return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
        "\n",
        "\n",
        "def load_context_databases():\n",
        "    \"\"\"\n",
        "    Load and build all context databases:\n",
        "    1. filename_to_pn: Maps filenames to part numbers\n",
        "    2. part_context_db: Full context for each part\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load File Mapping ---\n",
        "    print(\"Loading file mapping...\")\n",
        "    with open(FILE_MAPPING_PATH, 'r') as f:\n",
        "        file_mapping_list = json.load(f)\n",
        "\n",
        "    # Build filename -> part number lookup\n",
        "    filename_to_pn = {}\n",
        "    for entry in file_mapping_list:\n",
        "        filename = entry['file']\n",
        "        pn = entry['pn']\n",
        "        if pn:\n",
        "            filename_to_pn[filename] = pn\n",
        "            # Also add with .pdf extension variations\n",
        "            filename_to_pn[filename + '.pdf'] = pn\n",
        "            filename_to_pn[filename + '.PDF'] = pn\n",
        "\n",
        "    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n",
        "\n",
        "    # --- Load Structure JSON ---\n",
        "    print(\"Loading part structure...\")\n",
        "    with open(STRUCTURE_PATH, 'r') as f:\n",
        "        structure_data = json.load(f)\n",
        "\n",
        "    # --- Build Part Context Database ---\n",
        "    print(\"Building part context database...\")\n",
        "    part_context_db = {}\n",
        "\n",
        "    for assembly_name, parts_list in structure_data.items():\n",
        "\n",
        "        for part in parts_list:\n",
        "            pn = part['pn']\n",
        "            desc = part['desc']\n",
        "\n",
        "            # Create rich sibling list (PN + Description)\n",
        "            siblings_list = []\n",
        "            siblings_pns = []\n",
        "\n",
        "            for p_sibling in parts_list:\n",
        "                if p_sibling['pn'] != pn:\n",
        "                    # Clean description to avoid JSON format issues\n",
        "                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n",
        "                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n",
        "                    siblings_pns.append(p_sibling['pn'])\n",
        "\n",
        "            # Join them\n",
        "            siblings_str = \"; \".join(siblings_list[:12]) # Increase limit slightly\n",
        "            if len(siblings_list) > 12:\n",
        "                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n",
        "\n",
        "            # Create lookup key (normalized)\n",
        "            lookup_key = normalize_pn(pn)\n",
        "\n",
        "            # Store context\n",
        "            part_context_db[lookup_key] = {\n",
        "                'pn': pn,\n",
        "                'description': desc,\n",
        "                'assembly': assembly_name,\n",
        "                'siblings': siblings_str,\n",
        "                'siblings_list': siblings_pns\n",
        "            }\n",
        "\n",
        "            # Also store with original PN as key\n",
        "            part_context_db[pn] = part_context_db[lookup_key]\n",
        "\n",
        "    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n",
        "\n",
        "    return filename_to_pn, part_context_db\n",
        "\n",
        "\n",
        "# Load databases\n",
        "filename_to_pn, part_context_db = load_context_databases()\n",
        "print(\"\\nContext databases loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fecfcc5a",
        "outputId": "e7da488e-fa00-4f26-c64f-434887def839"
      },
      "source": [
        "# Fix environment conflict: Uninstall LangChain\n",
        "# (The 'langchain.docstore' error is caused by a conflict with the installed PaddleOCR environment)\n",
        "!pip uninstall -y langchain langchain-community\n",
        "!pip install -q paddleocr paddlepaddle\n",
        "\n",
        "print(\"‚úÖ Environment cleaned. LangChain uninstalled.\")\n",
        "print(\"üëâ If you still see an error in the next cell, please Restart Session (Runtime > Restart session) and run from here.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m‚úÖ Environment cleaned. LangChain uninstalled.\n",
            "üëâ If you still see an error in the next cell, please Restart Session (Runtime > Restart session) and run from here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d605a04",
        "outputId": "8a3ae866-14bc-4842-fa30-3c1c1de21f1f"
      },
      "source": [
        "import sys\n",
        "import logging\n",
        "\n",
        "print(\"Loading OCR Engine...\")\n",
        "\n",
        "# Initialize global variable\n",
        "if 'ocr_engine' not in globals():\n",
        "    ocr_engine = None\n",
        "\n",
        "def initialize_ocr():\n",
        "    global ocr_engine\n",
        "    try:\n",
        "        from paddleocr import PaddleOCR\n",
        "        # Updated to use correct parameter replacing deprecated use_angle_cls\n",
        "        # Note: We keep use_textline_orientation=True for init, but will disable cls in inference if needed\n",
        "        ocr_engine = PaddleOCR(use_textline_orientation=True, lang='en')\n",
        "        print(\"‚úÖ OCR Engine Ready!\")\n",
        "        return True\n",
        "    except RuntimeError as e:\n",
        "        if \"PDX has already been initialized\" in str(e):\n",
        "            print(\"‚ö†Ô∏è OCR already initialized (ignoring restart check).\")\n",
        "            return True\n",
        "        print(f\"‚ùå Runtime Error: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error initializing OCR: {e}\")\n",
        "        return False\n",
        "\n",
        "# Try to initialize immediately\n",
        "initialize_ocr()\n",
        "\n",
        "def get_drawing_text_ocr(image_input):\n",
        "    \"\"\"\n",
        "    Runs robust OCR on the drawing and returns a clean list of found text.\n",
        "    Lazy-loads the OCR engine if it's missing.\n",
        "    Compatible with PaddleOCR v3+ (PP-OCRv5).\n",
        "    \"\"\"\n",
        "    global ocr_engine\n",
        "\n",
        "    # Lazy Load Check\n",
        "    if ocr_engine is None:\n",
        "        print(\"‚ö†Ô∏è OCR Engine not active. Attempting to reload...\")\n",
        "        if not initialize_ocr():\n",
        "            return []\n",
        "\n",
        "    try:\n",
        "        # Run OCR using the new predict() API for PaddleOCR v3+\n",
        "        # The new API doesn't use cls parameter in predict()\n",
        "        result = ocr_engine.predict(image_input)\n",
        "\n",
        "        text_set = set()  # Use set to remove duplicates\n",
        "\n",
        "        # New PaddleOCR returns a dict or list of dicts\n",
        "        # Handle the new response format\n",
        "        if result:\n",
        "            for page_result in result:\n",
        "                # Check if it's the new format (dict with 'rec_texts' and 'rec_scores')\n",
        "                if isinstance(page_result, dict):\n",
        "                    texts = page_result.get('rec_texts', [])\n",
        "                    scores = page_result.get('rec_scores', [])\n",
        "                    for text, score in zip(texts, scores):\n",
        "                        if score > 0.85:\n",
        "                            text_set.add(text.strip())\n",
        "                # Fallback for older format (list of lines)\n",
        "                elif isinstance(page_result, list):\n",
        "                    for line in page_result:\n",
        "                        if line and len(line) >= 2:\n",
        "                            text_content = line[1][0]\n",
        "                            confidence = line[1][1]\n",
        "                            if confidence > 0.85:\n",
        "                                text_set.add(text_content.strip())\n",
        "\n",
        "        return sorted(list(text_set))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è OCR Warning: {e}\")\n",
        "        return []"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
            "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-LCNet_x1_0_doc_ori`.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading OCR Engine...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
            "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/UVDoc`.\u001b[0m\n",
            "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
            "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
            "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
            "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
            "\u001b[32mCreating model: ('en_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
            "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OCR Engine Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "AsY8eejBwHHb",
        "outputId": "8d9e785d-d9b1-4f59-dbeb-23446b1a41b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RAG SYSTEM SETUP\n",
            "============================================================\n",
            "\n",
            "[STEP 1/3] Loading CLIP model for semantic search...\n",
            "  CLIP model loaded!\n",
            "\n",
            "[STEP 2/3] Loading RAG Index...\n",
            "  Found existing index: /content/asme_visual_index.pkl\n",
            "\n",
            "[STEP 3/3] Setting up RAG Visual Database (ASME page images)...\n",
            "  No existing image database found.\n",
            "\n",
            "  >> Please upload your RAG visual database as a ZIP file:\n",
            "     (The ZIP should contain PNG/JPG images of ASME Y14.5 pages)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-865c331b-7a9b-4697-b59a-cd46c342cfe6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-865c331b-7a9b-4697-b59a-cd46c342cfe6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rag_visual_db.zip to rag_visual_db.zip\n",
            "\n",
            "  Extracting rag_visual_db.zip...\n",
            "  Extracted 294 images\n",
            "\n",
            "============================================================\n",
            "Optimizing search index...\n",
            "\n",
            "RAG SYSTEM STATUS: READY\n",
            "  Index entries:    294\n",
            "  Image directory:  /content/rag_visual_db\n",
            "  Images exist:     True\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# === Load RAG Index & Visual Database ===\n",
        "import os\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Initialize globals\n",
        "rag_data = []\n",
        "rag_embeddings = None\n",
        "rag_available = False\n",
        "RAG_IMAGE_DIR = None\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RAG SYSTEM SETUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Load CLIP Model\n",
        "# ============================================================\n",
        "print(\"\\n[STEP 1/3] Loading CLIP model for semantic search...\")\n",
        "search_model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"  CLIP model loaded!\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Load or Upload RAG Index (.pkl file)\n",
        "# ============================================================\n",
        "print(\"\\n[STEP 2/3] Loading RAG Index...\")\n",
        "\n",
        "index_loaded = False\n",
        "\n",
        "# Check if RAG_INDEX_PATH was set in cell-7\n",
        "if 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH):\n",
        "    print(f\"  Found existing index: {RAG_INDEX_PATH}\")\n",
        "    with open(RAG_INDEX_PATH, 'rb') as f:\n",
        "        rag_data = pickle.load(f)\n",
        "    index_loaded = True\n",
        "else:\n",
        "    print(\"  No RAG index found.\")\n",
        "    print(\"\\n  >> Please upload your RAG index file (asme_visual_index.pkl):\")\n",
        "\n",
        "    try:\n",
        "        uploaded_index = files.upload()\n",
        "\n",
        "        for filename in uploaded_index:\n",
        "            if filename.endswith('.pkl'):\n",
        "                RAG_INDEX_PATH = os.path.abspath(filename)\n",
        "                with open(RAG_INDEX_PATH, 'rb') as f:\n",
        "                    rag_data = pickle.load(f)\n",
        "                index_loaded = True\n",
        "                print(f\"\\n  Loaded index: {filename} ({len(rag_data)} entries)\")\n",
        "                break\n",
        "\n",
        "        if not index_loaded:\n",
        "            print(\"  WARNING: No .pkl file was uploaded!\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Upload error: {e}\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Load or Upload RAG Visual Database (images folder)\n",
        "# ============================================================\n",
        "print(\"\\n[STEP 3/3] Setting up RAG Visual Database (ASME page images)...\")\n",
        "\n",
        "# Check if images already exist somewhere\n",
        "existing_locations = [\n",
        "    \"/content/rag_visual_db\",\n",
        "    \"/content/rag_data/rag_visual_db\",\n",
        "    \"/content/data/rag_visual_db\",\n",
        "    \"rag_visual_db\",\n",
        "    \"rag_data/rag_visual_db\",\n",
        "]\n",
        "\n",
        "# Also check DATA_DIR if it exists\n",
        "if 'DATA_DIR' in dir() and DATA_DIR:\n",
        "    existing_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n",
        "\n",
        "found_images = False\n",
        "for loc in existing_locations:\n",
        "    if loc and os.path.exists(loc) and os.path.isdir(loc):\n",
        "        # Count images\n",
        "        img_count = len([f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "        if img_count > 0:\n",
        "            RAG_IMAGE_DIR = os.path.abspath(loc)\n",
        "            found_images = True\n",
        "            print(f\"  Found existing images: {RAG_IMAGE_DIR} ({img_count} files)\")\n",
        "            break\n",
        "\n",
        "if not found_images:\n",
        "    print(\"  No existing image database found.\")\n",
        "    print(\"\\n  >> Please upload your RAG visual database as a ZIP file:\")\n",
        "    print(\"     (The ZIP should contain PNG/JPG images of ASME Y14.5 pages)\")\n",
        "\n",
        "    try:\n",
        "        uploaded_zip = files.upload()\n",
        "\n",
        "        for filename in uploaded_zip:\n",
        "            if filename.lower().endswith('.zip'):\n",
        "                # Create directory and extract\n",
        "                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "                if os.path.exists(RAG_IMAGE_DIR):\n",
        "                    shutil.rmtree(RAG_IMAGE_DIR)\n",
        "                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "                print(f\"\\n  Extracting {filename}...\")\n",
        "                with zipfile.ZipFile(filename, 'r') as zf:\n",
        "                    zf.extractall(RAG_IMAGE_DIR)\n",
        "\n",
        "                # Find all images (including in subdirectories)\n",
        "                all_images = []\n",
        "                for root, dirs, fls in os.walk(RAG_IMAGE_DIR):\n",
        "                    for f in fls:\n",
        "                        if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                            all_images.append(os.path.join(root, f))\n",
        "\n",
        "                print(f\"  Extracted {len(all_images)} images\")\n",
        "\n",
        "                # If all images are in a subdirectory, point to that\n",
        "                if all_images:\n",
        "                    common_dir = os.path.commonpath(all_images)\n",
        "                    if os.path.isdir(common_dir) and common_dir != RAG_IMAGE_DIR:\n",
        "                        RAG_IMAGE_DIR = common_dir\n",
        "                        print(f\"  Image directory: {RAG_IMAGE_DIR}\")\n",
        "\n",
        "                found_images = True\n",
        "                break\n",
        "\n",
        "        if not found_images:\n",
        "            print(\"  WARNING: No ZIP file uploaded. RAG retrieval will not work.\")\n",
        "            RAG_IMAGE_DIR = \"/content/rag_visual_db\"  # Placeholder\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Upload error: {e}\")\n",
        "        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "\n",
        "# ============================================================\n",
        "# FINALIZE: Build search index\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "if index_loaded and isinstance(rag_data, list) and len(rag_data) > 0:\n",
        "    print(\"Optimizing search index...\")\n",
        "    embeddings_list = [item['embedding'] for item in rag_data]\n",
        "    rag_embeddings = np.array(embeddings_list).astype('float32')\n",
        "    rag_available = True\n",
        "\n",
        "    print(\"\\nRAG SYSTEM STATUS: READY\")\n",
        "    print(f\"  Index entries:    {len(rag_data)}\")\n",
        "    print(f\"  Image directory:  {RAG_IMAGE_DIR}\")\n",
        "    print(f\"  Images exist:     {os.path.exists(RAG_IMAGE_DIR) if RAG_IMAGE_DIR else False}\")\n",
        "else:\n",
        "    print(\"RAG SYSTEM STATUS: NOT READY\")\n",
        "    print(\"  Index not loaded. Please re-run this cell and upload the .pkl file.\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFD4nCWywHHb"
      },
      "source": [
        "## 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Y7DliIwHHb",
        "outputId": "78791372-51b2-467d-fb01-ca749c867593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def extract_filename_key(filepath):\n",
        "    \"\"\"\n",
        "    Extract the filename key for lookup from a full path.\n",
        "    Handles various formats and extensions.\n",
        "    \"\"\"\n",
        "    # Get just the filename\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    # Remove extension\n",
        "    name_no_ext = os.path.splitext(filename)[0]\n",
        "\n",
        "    # NEW: Remove Colab/OS duplicate counter like \" (1)\"\n",
        "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)\n",
        "\n",
        "    # Clean up common suffixes\n",
        "    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n",
        "\n",
        "    return name_cleaned.strip()\n",
        "\n",
        "\n",
        "def get_part_context(filepath):\n",
        "    \"\"\"\n",
        "    Look up part context from filename.\n",
        "    Returns (part_number, context_dict) or (None, None) if not found.\n",
        "    \"\"\"\n",
        "    filename_key = extract_filename_key(filepath)\n",
        "\n",
        "    # Try direct lookup\n",
        "    if filename_key in filename_to_pn:\n",
        "        pn = filename_to_pn[filename_key]\n",
        "        lookup_key = normalize_pn(pn)\n",
        "        if lookup_key in part_context_db:\n",
        "            return pn, part_context_db[lookup_key]\n",
        "\n",
        "    # Try with extension\n",
        "    for ext in ['.pdf', '.PDF']:\n",
        "        key = filename_key + ext\n",
        "        if key in filename_to_pn:\n",
        "            pn = filename_to_pn[key]\n",
        "            lookup_key = normalize_pn(pn)\n",
        "            if lookup_key in part_context_db:\n",
        "                return pn, part_context_db[lookup_key]\n",
        "\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def build_context_string(pn, context):\n",
        "    \"\"\"\n",
        "    Build the context string for the inspection prompt.\n",
        "    \"\"\"\n",
        "    if context is None:\n",
        "        return \"CONTEXT: Unknown Part (General Syntax Check Only). No assembly context available.\"\n",
        "\n",
        "    desc = context.get('description', 'Unknown')\n",
        "    assembly = context.get('assembly', 'Unknown Assembly')\n",
        "    siblings = context.get('siblings', 'None listed')\n",
        "\n",
        "    context_str = f\"\"\"CONTEXT: This is Part {pn} ({desc}).\n",
        "It belongs to the {assembly}.\n",
        "It must assemble with these mating parts: {siblings}.\n",
        "CRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n",
        "\n",
        "    return context_str\n",
        "\n",
        "\n",
        "def pdf_to_image(pdf_path, dpi=150):\n",
        "    \"\"\"\n",
        "    Convert first page of PDF to PIL Image.\n",
        "    \"\"\"\n",
        "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n",
        "    if pages:\n",
        "        return pages[0]\n",
        "    return None\n",
        "\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3aLheGBwHHb",
        "outputId": "d8a213ee-421f-4122-ec0d-79dc267a9bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model query function defined.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def query_model(messages, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Send a query to Qwen2-VL and get response.\n",
        "    \"\"\"\n",
        "    # Check if model is loaded\n",
        "    if 'model' not in globals() or 'processor' not in globals():\n",
        "        raise RuntimeError(\"‚ö†Ô∏è Qwen2-VL Model is not loaded. Please run the 'Reload Model' cell below.\")\n",
        "\n",
        "    # Apply chat template\n",
        "    text = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Process vision info\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "    response = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "\n",
        "print(\"Model query function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gETA6XD5wHHb",
        "outputId": "09461b92-e283-4f86-8a6a-40452499cefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieve_asme_pages() function defined - uses RAG_IMAGE_DIR from upload.\n"
          ]
        }
      ],
      "source": [
        "def retrieve_asme_pages(keywords, top_k=2):\n",
        "    \"\"\"\n",
        "    Retrieve relevant ASME standard pages based on GD&T keywords.\n",
        "\n",
        "    Uses the RAG_IMAGE_DIR set during the upload process in cell-9.\n",
        "    \"\"\"\n",
        "    global RAG_IMAGE_DIR\n",
        "\n",
        "    if not rag_available or rag_embeddings is None:\n",
        "        print(\"  WARNING: RAG system not available for retrieval\")\n",
        "        return []\n",
        "\n",
        "    if RAG_IMAGE_DIR is None:\n",
        "        print(\"  WARNING: RAG_IMAGE_DIR not set. Run the RAG setup cell first.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # 1. Encode query\n",
        "        query_vector = search_model.encode([keywords])\n",
        "\n",
        "        # 2. Compute similarity scores\n",
        "        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "        retrieved_images = []\n",
        "        print(f\"  RAG Search: '{keywords[:50]}...'\")\n",
        "\n",
        "        # 3. Retrieve top-k images using the uploaded RAG_IMAGE_DIR\n",
        "        for idx in top_indices:\n",
        "            item = rag_data[idx]\n",
        "\n",
        "            # Normalize the path from index (handle Windows backslashes)\n",
        "            rel_path = item['path'].replace('\\\\', '/')\n",
        "\n",
        "            # Build possible paths to try\n",
        "            paths_to_try = [\n",
        "                os.path.join(RAG_IMAGE_DIR, rel_path),                    # Full relative path\n",
        "                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)), # Just filename\n",
        "            ]\n",
        "\n",
        "            # If rel_path has subdirectories, also try without them\n",
        "            path_parts = rel_path.split('/')\n",
        "            if len(path_parts) > 1:\n",
        "                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n",
        "                # Try with just the last subdirectory\n",
        "                if len(path_parts) > 2:\n",
        "                    paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-2], path_parts[-1]))\n",
        "\n",
        "            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n",
        "\n",
        "            # Try each path\n",
        "            image_loaded = False\n",
        "            for try_path in paths_to_try:\n",
        "                if os.path.exists(try_path):\n",
        "                    try:\n",
        "                        img = Image.open(try_path).convert('RGB')\n",
        "                        retrieved_images.append(img)\n",
        "                        image_loaded = True\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error opening image: {e}\")\n",
        "\n",
        "            if not image_loaded:\n",
        "                print(f\"      Image not found in {RAG_IMAGE_DIR}\")\n",
        "\n",
        "        return retrieved_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  RAG retrieval error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "print(\"retrieve_asme_pages() function defined - uses RAG_IMAGE_DIR from upload.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === PRODUCTION PIPELINE HELPER FUNCTIONS ===\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "print(\"‚öôÔ∏è Initializing Production Pipeline...\")\n",
        "\n",
        "# Initialize OCR Engine once (Global) - reuse existing if available\n",
        "if 'ocr_engine' not in dir() or ocr_engine is None:\n",
        "    try:\n",
        "        from paddleocr import PaddleOCR\n",
        "        # Updated: removed show_log (deprecated) and updated angle_cls param\n",
        "        ocr_engine = PaddleOCR(use_textline_orientation=True, lang='en')\n",
        "        print(\"  OCR Engine initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  OCR Engine warning: {e}\")\n",
        "        ocr_engine = None\n",
        "\n",
        "def render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Renders the first page of a PDF to a High-Res PIL Image.\n",
        "    Replaces pdf2image for better speed and clarity.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        page = doc.load_page(0)\n",
        "        zoom = dpi / 72.0\n",
        "        mat = fitz.Matrix(zoom, zoom)\n",
        "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        doc.close()\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Rendering Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_paddleocr(img: Image.Image) -> List[str]:\n",
        "    \"\"\"\n",
        "    Runs PaddleOCR on the image and returns a sorted, unique list of text found.\n",
        "    Normalizes common engineering symbols (√ò -> DIA).\n",
        "    \"\"\"\n",
        "    if ocr_engine is None:\n",
        "        print(\"‚ö†Ô∏è OCR Engine not available\")\n",
        "        return []\n",
        "\n",
        "    img_np = np.array(img)\n",
        "    # CHANGED: cls=False to avoid 'unexpected keyword argument cls' error\n",
        "    result = ocr_engine.ocr(img_np, cls=False)\n",
        "\n",
        "    texts = []\n",
        "    if result and result[0]:\n",
        "        for line in result[0]:\n",
        "            text_content, confidence = line[1]\n",
        "            if confidence > 0.6:  # Filter low-confidence noise\n",
        "                # Normalize symbols\n",
        "                clean_text = text_content.replace(\"√ò\", \"DIA \").strip()\n",
        "                texts.append(clean_text)\n",
        "\n",
        "    # Deduplicate and sort\n",
        "    return sorted(list(set(texts)))\n",
        "\n",
        "def make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n",
        "    \"\"\"\n",
        "    Splits the image into 4 overlapping quadrants (TL, TR, BL, BR).\n",
        "    Allows the model to see small dimensions clearly.\n",
        "    \"\"\"\n",
        "    w, h = full_img.size\n",
        "    tile_w, tile_h = w // 2, h // 2\n",
        "    overlap = int(min(w, h) * 0.15)  # 15% overlap ensures no text is cut in half\n",
        "\n",
        "    # Define crop boxes: (left, top, right, bottom)\n",
        "    boxes = {\n",
        "        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n",
        "        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n",
        "        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n",
        "        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n",
        "    }\n",
        "\n",
        "    tiles = []\n",
        "    for name, box in boxes.items():\n",
        "        tiles.append((name, full_img.crop(box)))\n",
        "    return tiles\n",
        "\n",
        "print(\"‚úÖ Production Pipeline Helpers Loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJGxTszX-2XY",
        "outputId": "bd619b37-a123-45d7-86bc-05f66cad0437"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Initializing Production Pipeline...\n",
            "‚úÖ Production Pipeline Helpers Loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmd8w7bcwHHb"
      },
      "source": [
        "## 5. Main Inspection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "7BcVNR3hwHHb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def inspect_drawing_rag(drawing_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Main inspection function for engineering drawings.\n",
        "\n",
        "    HYBRID INSPECTION SYSTEM (Vision + OCR + Chain-of-Thought):\n",
        "    - Phase 0: Pre-processing (Identity, Image Load, OCR Extraction)\n",
        "    - Step 1: Extract features using Vision + OCR data\n",
        "    - Step 2: Perform Audit (Tier 1, 2, 3) using extracted text + Context\n",
        "\n",
        "    Args:\n",
        "        drawing_path: Path to PDF drawing file\n",
        "        verbose: Print detailed progress\n",
        "\n",
        "    Returns:\n",
        "        dict with keys: 'result', 'part_number', 'description', 'details'\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"INSPECTING: {os.path.basename(drawing_path)}\")\n",
        "        print('='*60)\n",
        "\n",
        "    # === PHASE 0: PRE-PROCESSING ===\n",
        "\n",
        "    # 1. Identity\n",
        "    if verbose:\n",
        "        print(\"\\n[1/5] Identifying part...\")\n",
        "\n",
        "    pn, context = get_part_context(drawing_path)\n",
        "\n",
        "    if not context:\n",
        "        # Added explicit failure logging\n",
        "        if verbose:\n",
        "            print(f\"  ‚ùå Identity Unknown. Could not find context for '{drawing_path}'\")\n",
        "            print(\"  (Check file mapping or cleanup logic)\")\n",
        "        return {\n",
        "            'result': 'FAIL',\n",
        "            'part_number': None,\n",
        "            'description': None,\n",
        "            'details': 'Identity Unknown - Context logic failed.'\n",
        "        }\n",
        "\n",
        "    context_str = build_context_string(pn, context)\n",
        "\n",
        "    if pn:\n",
        "        if verbose:\n",
        "            print(f\"  Part Number: {pn}\")\n",
        "            print(f\"  Description: {context.get('description', 'N/A')}\")\n",
        "            print(f\"  Assembly: {context.get('assembly', 'N/A')}\")\n",
        "            if context.get('siblings'):\n",
        "                print(f\"  Mating Parts: {context.get('siblings', 'N/A')}\")\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"  Part not found in database - general inspection only\")\n",
        "\n",
        "    # 2. Image Load\n",
        "    if verbose:\n",
        "        print(\"\\n[2/5] Loading drawing & running OCR scan...\")\n",
        "\n",
        "    try:\n",
        "        drawing_image = pdf_to_image(drawing_path)\n",
        "        if drawing_image is None:\n",
        "            return {\n",
        "                'result': 'ERROR',\n",
        "                'part_number': pn,\n",
        "                'description': context.get('description') if context else None,\n",
        "                'details': 'Failed to convert PDF to image'\n",
        "            }\n",
        "        if verbose:\n",
        "            print(f\"  Drawing loaded: {drawing_image.size}\")\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'result': 'ERROR',\n",
        "            'part_number': pn,\n",
        "            'description': context.get('description') if context else None,\n",
        "            'details': f'Error loading PDF: {str(e)}'\n",
        "        }\n",
        "\n",
        "    # 3. OCR EXTRACTION (The \"Bionic Eye\")\n",
        "    ocr_text_list = []\n",
        "    ocr_text_block = \"\"\n",
        "\n",
        "    try:\n",
        "        # Convert PIL Image to numpy array for OCR\n",
        "        ocr_input = np.array(drawing_image)\n",
        "        ocr_text_list = get_drawing_text_ocr(ocr_input)\n",
        "        ocr_text_block = \"\\n\".join(ocr_text_list)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  OCR Found {len(ocr_text_list)} text elements: {ocr_text_list[:5]}...\")\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"  OCR Warning: {e} - Proceeding with vision-only mode\")\n",
        "\n",
        "    # === PHASE A: VISION + OCR EXTRACTION ===\n",
        "    if verbose:\n",
        "        print(\"\\n[3/5] CoT Step 1: Extraction (Vision + OCR)...\")\n",
        "\n",
        "    # Build extraction prompt with OCR data injected\n",
        "    if ocr_text_block:\n",
        "        extraction_prompt = f\"\"\"You are an Expert Engineering Drawing Scanner.\n",
        "\n",
        "I have run an automated OCR scan on this drawing. Here is the raw text found:\n",
        "--- OCR DATA START ---\n",
        "{ocr_text_block}\n",
        "--- OCR DATA END ---\n",
        "\n",
        "YOUR TASK:\n",
        "Use the OCR Data to help you visually locate and confirm the features on the drawing image.\n",
        "Extract the following strictly. If the OCR list contains the number, trust it.\n",
        "\n",
        "1. **Thread Callouts** (e.g. 'M10x1.5', '1/4-20 UNC'). Look for these specifically in the OCR data.\n",
        "2. **Bore/Hole Dimensions** (e.g. '√ò0.500', '√ò1.00').\n",
        "3. **Material Note**.\n",
        "4. **GD&T Symbols** (Vision only).\n",
        "\n",
        "Output the clean list of features found.\"\"\"\n",
        "    else:\n",
        "        # Fallback to vision-only mode if OCR failed\n",
        "        extraction_prompt = \"\"\"Scan this drawing and extract the exact text for:\n",
        "1.  **Thread Callouts** (e.g., '1/4-20 UNC', 'M6x1.0').\n",
        "2.  **Bore/Hole Dimensions** with tolerances (e.g., '√ò0.500 +0.001/-0.000').\n",
        "3.  **Material Note**.\n",
        "4.  **GD&T Symbols**.\n",
        "List them exactly as written on the print. Do not analyze yet.\"\"\"\n",
        "\n",
        "    # Initialize conversation history\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": drawing_image},\n",
        "                {\"type\": \"text\", \"text\": extraction_prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Get extraction\n",
        "    extraction_text = query_model(messages, max_tokens=512)\n",
        "\n",
        "    # Append assistant response to history\n",
        "    messages.append(\n",
        "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": extraction_text}]}\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Extracted Features:\\n{extraction_text[:300]}...\")\n",
        "\n",
        "    # === PHASE B: RAG RETRIEVAL ===\n",
        "    if verbose:\n",
        "        print(\"\\n[4/5] Retrieving ASME reference pages...\")\n",
        "\n",
        "    asme_images = []\n",
        "\n",
        "    # Use the extracted text to drive RAG retrieval\n",
        "    if rag_available:\n",
        "        if verbose:\n",
        "            print(\"  Mode: Context-Aware Retrieval (using extracted features)\")\n",
        "        # Search using the extracted text (first 200 chars are usually most relevant)\n",
        "        rag_query = extraction_text\n",
        "        if len(rag_query) < 20:\n",
        "            rag_query = \"General Dimensioning Rules\"\n",
        "        asme_images = retrieve_asme_pages(rag_query, top_k=2)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(\"  WARNING: RAG not available - proceeding without ASME references\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Total ASME pages for audit: {len(asme_images)}\")\n",
        "\n",
        "    # === PHASE C: STRICT LOGIC COMPARATOR ===\n",
        "    if verbose:\n",
        "        print(\"\\n[5/5] CoT Step 2: Strict Logic Audit...\")\n",
        "\n",
        "    # Build mating parts string\n",
        "    mating_parts_str = \"None specified\"\n",
        "    if context and context.get('siblings'):\n",
        "        mating_parts_str = context.get('siblings')\n",
        "\n",
        "    # Construct the Strict Logic Comparator Prompt\n",
        "    audit_prompt = f\"\"\"You are a Strict Logic Comparator.\n",
        "Compare the REQUIREMENTS (Context) vs ACTUALS (Extracted Data).\n",
        "\n",
        "1. REQUIREMENTS (Mating Parts):\n",
        "{context_str}\n",
        "\n",
        "2. ACTUALS (Found on Drawing):\n",
        "{extraction_text}\n",
        "\n",
        "STRICT RULES:\n",
        "- You must verify if the **specific dimension** required by the mating part exists in the ACTUALS.\n",
        "- If Mating Part is 'Screw 3/4-16' and ACTUALS contains ONLY 'M10', '√ò0.50' -> **FAIL** (Mismatch).\n",
        "- If Mating Part is 'Bearing √ò0.75' and ACTUALS contains '√ò1.75' -> **FAIL** (Mismatch).\n",
        "- If the feature is NOT in the ACTUALS list, report: 'CANNOT VERIFY - Feature not found in OCR/Vision data'.\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "ADDITIONAL CHECKS & OUTPUT FORMAT:\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "You must also report on Tier 1 (General) and Tier 2 (GD&T) based on the extraction.\n",
        "\n",
        "**OUTPUT STRUCTURE (Follow EXACTLY):**\n",
        "\n",
        "**Line 1:** PASS or FAIL (Overall result)\n",
        "\n",
        "**Then provide:**\n",
        "\n",
        "1. **Tier 1 (General)**: State clearly if Material, Title Block, and General Tolerances are PRESENT or MISSING.\n",
        "\n",
        "2. **Tier 2 (GD&T Syntax)**: Comments on symbol formatting.\n",
        "\n",
        "3. **Tier 3 (Assembly Fit Analysis):**\n",
        "   - 'Mating Part [PN] -> [PASS/FAIL]: [Evidence from ACTUALS list]'\n",
        "\n",
        "4. **Citations:** Reference ASME images if applicable.\n",
        "\n",
        "5. **Recommendations**\"\"\"\n",
        "\n",
        "    # Build content for the second user message\n",
        "    content_2 = []\n",
        "\n",
        "    # Add RAG images to this turn if available\n",
        "    for img in asme_images:\n",
        "        content_2.append({\"type\": \"image\", \"image\": img})\n",
        "\n",
        "    content_2.append({\"type\": \"text\", \"text\": audit_prompt})\n",
        "\n",
        "    # Append to history\n",
        "    messages.append({\"role\": \"user\", \"content\": content_2})\n",
        "\n",
        "    # Get final audit result\n",
        "    audit_response = query_model(messages, max_tokens=1500)\n",
        "\n",
        "    # Parse result\n",
        "    first_line = audit_response.split('\\n')[0].strip().upper()\n",
        "    if 'PASS' in first_line and 'FAIL' not in first_line:\n",
        "        result = 'PASS'\n",
        "    elif 'FAIL' in first_line:\n",
        "        result = 'FAIL'\n",
        "    else:\n",
        "        # Check deeper in response for result\n",
        "        response_upper = audit_response.upper()\n",
        "        if 'TIER 1 FAILURE' in response_upper or 'TIER 2 FAILURE' in response_upper or 'TIER 3 FAILURE' in response_upper:\n",
        "            result = 'FAIL'\n",
        "        elif '**FAIL**' in response_upper:\n",
        "            result = 'FAIL'\n",
        "        elif '**PASS**' in response_upper:\n",
        "            result = 'PASS'\n",
        "        else:\n",
        "            result = 'REVIEW'\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"RESULT: {result}\")\n",
        "        print('='*60)\n",
        "        print(audit_response)\n",
        "\n",
        "    return {\n",
        "        'result': result,\n",
        "        'part_number': pn,\n",
        "        'description': context.get('description') if context else None,\n",
        "        'assembly': context.get('assembly') if context else None,\n",
        "        'mating_parts': mating_parts_str,\n",
        "        'gdt_symbols': extraction_text,\n",
        "        'ocr_text_count': len(ocr_text_list),\n",
        "        'asme_pages_used': len(asme_images),\n",
        "        'details': audit_response\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_drawing_production(pdf_path, context_str=None):\n",
        "    \"\"\"\n",
        "    Production-grade inspection using OCR + High-Res Tiling.\n",
        "\n",
        "    This is the upgraded \"Hybrid Vision Pipeline\" that solves resolution-induced\n",
        "    hallucinations by using:\n",
        "    1. Deterministic OCR (PaddleOCR) to read text before the LLM sees it\n",
        "    2. Dynamic Tiling (4 quadrants) to improve resolution\n",
        "    3. PyMuPDF (fitz) for high-speed rendering\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to PDF drawing file\n",
        "        context_str: Optional context string. If None, will be auto-generated from part lookup.\n",
        "\n",
        "    Returns:\n",
        "        str: The inspection result text\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\\nINSPECTING (Production): {pdf_path}\\n{'='*60}\")\n",
        "\n",
        "    # Auto-generate context if not provided\n",
        "    if context_str is None:\n",
        "        pn, ctx = get_part_context(pdf_path)\n",
        "        if ctx:\n",
        "            context_str = ctx.get('siblings', 'No mating parts specified')\n",
        "            print(f\"  Part: {pn} ({ctx.get('description', 'N/A')})\")\n",
        "        else:\n",
        "            context_str = \"Unknown part - general inspection only\"\n",
        "            print(\"  Part not found in database\")\n",
        "\n",
        "    # --- Phase A: Perception ---\n",
        "    print(\"[1/4] Rendering High-Res Image...\")\n",
        "    full_img = render_pdf_page(pdf_path, dpi=300)\n",
        "    if not full_img:\n",
        "        return \"FAIL: Image Rendering Failed\"\n",
        "    print(f\"  Image size: {full_img.size}\")\n",
        "\n",
        "    print(\"[2/4] Extracting Deterministic OCR Evidence...\")\n",
        "    ocr_texts = run_paddleocr(full_img)\n",
        "    # Create a compact evidence block\n",
        "    ocr_block = \"\\n\".join([f\"- {t}\" for t in ocr_texts[:80]])\n",
        "    print(f\"  > OCR Found {len(ocr_texts)} text elements.\")\n",
        "\n",
        "    print(\"[3/4] Generating High-Res Tiles...\")\n",
        "    tiles = make_overlapping_tiles(full_img)\n",
        "    print(f\"  > Generated {len(tiles)} tiles\")\n",
        "\n",
        "    # --- Phase B: Reasoning ---\n",
        "    print(\"[4/4] Running Strict Logic Inference (Qwen2-VL-72B)...\")\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a Senior Quality Control Engineer.\n",
        "\n",
        "    **CORE PROTOCOL:**\n",
        "    1. **OCR IS AUTHORITY:** The 'OCR EVIDENCE' list is the ground truth for text.\n",
        "    2. **VISUAL VERIFICATION:** Use the 'TILES' to visually confirm geometry (e.g., is that hole threaded?).\n",
        "    3. **STRICT COMPARISON:** Compare the 'MATING HYPOTHESIS' against the 'OCR EVIDENCE'.\n",
        "\n",
        "    **FAILURE RULES:**\n",
        "    - If Hypothesis needs '3/4-16' and Evidence says 'M10', output **FAIL**.\n",
        "    - If Evidence is missing for a specific mating part, output **CANNOT VERIFY**.\n",
        "    - Do NOT hallucinate a fit. Mismatches must be flagged.\n",
        "    \"\"\"\n",
        "\n",
        "    user_text = f\"\"\"\n",
        "    **PART 1: OCR EVIDENCE (FACTS)**\n",
        "    {ocr_block}\n",
        "\n",
        "    **PART 2: MATING HYPOTHESIS (REQUIREMENTS)**\n",
        "    {context_str}\n",
        "\n",
        "    **TASK:**\n",
        "    For each Mating Part in the Hypothesis:\n",
        "    1. SEARCH the OCR list and Tiles for the matching feature.\n",
        "    2. COMPARE the dimensions/threads strictly.\n",
        "    3. REPORT: 'Mating Part [Name] -> [PASS/FAIL]: [Evidence]'\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the Multi-Image Payload\n",
        "    # Order: Full Image -> 4 Tiles -> Text Prompt\n",
        "    content_payload = []\n",
        "    content_payload.append({'type': 'image', 'image': full_img})\n",
        "    content_payload.append({'type': 'text', 'text': \"FULL DRAWING VIEW\"})\n",
        "\n",
        "    for name, tile in tiles:\n",
        "        content_payload.append({'type': 'image', 'image': tile})\n",
        "        content_payload.append({'type': 'text', 'text': f\"ZOOMED TILE: {name}\"})\n",
        "\n",
        "    content_payload.append({'type': 'text', 'text': user_text})\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_prompt},\n",
        "        {'role': 'user', 'content': content_payload}\n",
        "    ]\n",
        "\n",
        "    # Inference\n",
        "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Combine all images for the processor\n",
        "    image_inputs = [full_img] + [t[1] for t in tiles]\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
        "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Clean response\n",
        "    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RESULT:\")\n",
        "    print('='*60)\n",
        "    print(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Production Pipeline Loaded. Ready to Inspect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peirUbTi-2XY",
        "outputId": "d1159ea1-eef7-4ab7-9d6d-b5205e36ae5b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Production Pipeline Loaded. Ready to Inspect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc7S5QTzwHHb"
      },
      "source": [
        "## 6. Batch Inspection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-tlYl9GwHHb",
        "outputId": "4dbabcb7-94d7-4361-9f09-8e9640283cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch inspection function defined.\n"
          ]
        }
      ],
      "source": [
        "def inspect_batch(drawing_folder, output_file=\"inspection_results.json\", limit=None):\n",
        "    \"\"\"\n",
        "    Inspect all PDF drawings in a folder.\n",
        "\n",
        "    Args:\n",
        "        drawing_folder: Path to folder containing PDF drawings\n",
        "        output_file: Path to save results JSON\n",
        "        limit: Max number of files to process (None for all)\n",
        "\n",
        "    Returns:\n",
        "        List of inspection results\n",
        "    \"\"\"\n",
        "    import glob\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    # Find all PDFs\n",
        "    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n",
        "    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n",
        "    pdf_files = list(set(pdf_files))  # Remove duplicates\n",
        "\n",
        "    if limit:\n",
        "        pdf_files = pdf_files[:limit]\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files to inspect\")\n",
        "\n",
        "    results = []\n",
        "    pass_count = 0\n",
        "    fail_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for pdf_path in tqdm(pdf_files, desc=\"Inspecting\"):\n",
        "        try:\n",
        "            result = inspect_drawing_rag(pdf_path, verbose=False)\n",
        "            result['file'] = os.path.basename(pdf_path)\n",
        "            results.append(result)\n",
        "\n",
        "            if result['result'] == 'PASS':\n",
        "                pass_count += 1\n",
        "            elif result['result'] == 'FAIL':\n",
        "                fail_count += 1\n",
        "            else:\n",
        "                error_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'file': os.path.basename(pdf_path),\n",
        "                'result': 'ERROR',\n",
        "                'details': str(e)\n",
        "            })\n",
        "            error_count += 1\n",
        "\n",
        "    # Save results\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"BATCH INSPECTION SUMMARY\")\n",
        "    print('='*60)\n",
        "    print(f\"Total inspected: {len(results)}\")\n",
        "    print(f\"PASS: {pass_count} ({100*pass_count/len(results):.1f}%)\")\n",
        "    print(f\"FAIL: {fail_count} ({100*fail_count/len(results):.1f}%)\")\n",
        "    print(f\"ERROR/REVIEW: {error_count} ({100*error_count/len(results):.1f}%)\")\n",
        "    print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"Batch inspection function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM1_6wuZwHHb"
      },
      "source": [
        "## 7. Test the Inspector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0hzx2gEywHHb",
        "outputId": "936081db-a3db-47f2-ac53-240786c51f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a PDF drawing to inspect:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b633b2a3-323d-48f6-9b8d-ed3a4c055464\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b633b2a3-323d-48f6-9b8d-ed3a4c055464\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 51754201_03.pdf to 51754201_03.pdf\n",
            "Inspecting 51754201_03.pdf...\n",
            "\n",
            "============================================================\n",
            "INSPECTING: 51754201_03.pdf\n",
            "============================================================\n",
            "\n",
            "[1/5] Identifying part...\n",
            "  Part Number: 517542\n",
            "  Description: HEAD CASTING\n",
            "  Assembly: IDLER WHEEL ASSEMBLY\n",
            "  Mating Parts: 513889 (HEAD SLIDE & SHAFT SUBASSY); 005956 (ADJUSTMENT SCREW 3/4-16X2.31 0.42DIA.); 1008019 (TENSION HANDWHEEL SUBASSY); 108749 (BRNG BALL THRST 3/4X1-21/32X35/64); 204138 (WSHR OD:4.25X.78/THK:.1345 MILD STL); 108750 (SPR CPRSN 1.124X2.00X2.600 4660LB/IN); 108748 (TUBE RD 1.00ODX.782IDX1.75 LONG); 199306 (WASHER-WROUGHT, 3/4); 176858 (BANDWHEEL IDLER ASSY); 204455 (RING RTNG EXT 30MM DIA SHAFT); 122652 (SHIM .010); 122653 (SHIM .020)... and 5 more\n",
            "\n",
            "[2/5] Loading drawing & running OCR scan...\n",
            "  Drawing loaded: (2550, 1651)\n",
            "‚ö†Ô∏è OCR Warning: PaddleOCR.predict() got an unexpected keyword argument 'cls'\n",
            "  OCR Found 0 text elements: []...\n",
            "\n",
            "[3/5] CoT Step 1: Extraction (Vision + OCR)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1929867609.py:48: DeprecationWarning: Please use `predict` instead.\n",
            "  result = ocr_engine.ocr(image_input, cls=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted Features:\n",
            "Here is the extracted text from the drawing:\n",
            "\n",
            "### Thread Callouts\n",
            "- M10X1.5-6H THRU\n",
            "\n",
            "### Bore/Hole Dimensions with Tolerances\n",
            "- √ò0.50 ¬±0.03\n",
            "- √ò0.38 ¬±0.03\n",
            "- √ò0.25 ¬±0.03\n",
            "- √ò0.19 ¬±0.03\n",
            "- √ò0.13 ¬±0.03\n",
            "- √ò1.00 ¬±0.03\n",
            "- √ò1.13 ¬±0.03\n",
            "- √ò1.75 ¬±0.03\n",
            "\n",
            "### Material Note\n",
            "- CLASS 30 GRAY IRON\n",
            "\n",
            "### GD&T Symbols\n",
            "- No...\n",
            "\n",
            "[4/5] Retrieving ASME reference pages...\n",
            "  Mode: Context-Aware Retrieval (using extracted features)\n",
            "  RAG Search: 'Here is the extracted text from the drawing:\n",
            "\n",
            "### ...'\n",
            "    - 11_Profile_Tolerances_P252.png (Score: 31.456)\n",
            "    - 04_Fundamental_Rules_P29.png (Score: 30.745)\n",
            "  Total ASME pages for audit: 2\n",
            "\n",
            "[5/5] CoT Step 2: Strict Logic Audit...\n"
          ]
        }
      ],
      "source": [
        "# === SINGLE FILE TEST ===\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload a PDF drawing to inspect:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Take the first uploaded file\n",
        "    test_drawing = list(uploaded.keys())[0]\n",
        "    print(f\"Inspecting {test_drawing}...\")\n",
        "    result = inspect_drawing_rag(test_drawing, verbose=True)\n",
        "else:\n",
        "    print(\"No file uploaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00c88766",
        "outputId": "4a0bd8e0-43eb-4730-ce5f-5474b39601da"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# === RECOVERY: RELOAD CONTEXT DATABASES ===\n",
        "# If the runtime was restarted, these variables are lost. We need to reload them.\n",
        "\n",
        "if 'filename_to_pn' not in globals() or 'part_context_db' not in globals():\n",
        "    print(\"üîÑ Reloading Context Databases (Required after restart)...\")\n",
        "\n",
        "    # 1. Locate Files\n",
        "    MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
        "    STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
        "\n",
        "    def locate_file(filename):\n",
        "        if os.path.exists(filename): return os.path.abspath(filename)\n",
        "        if os.path.exists(os.path.join(\"rag_data\", filename)): return os.path.abspath(os.path.join(\"rag_data\", filename))\n",
        "        return None\n",
        "\n",
        "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
        "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
        "\n",
        "    if not FILE_MAPPING_PATH or not STRUCTURE_PATH:\n",
        "        print(\"‚ùå ERROR: Could not find config files. Please re-upload 400S_file_part_mapping.json and 400S_detailed_structure_fixed.json\")\n",
        "    else:\n",
        "        # 2. Load Mapping\n",
        "        with open(FILE_MAPPING_PATH, 'r') as f:\n",
        "            file_mapping_list = json.load(f)\n",
        "\n",
        "        filename_to_pn = {}\n",
        "        for entry in file_mapping_list:\n",
        "            pn = entry.get('pn')\n",
        "            fname = entry.get('file')\n",
        "            if pn and fname:\n",
        "                filename_to_pn[fname] = pn\n",
        "                filename_to_pn[fname + '.pdf'] = pn\n",
        "                filename_to_pn[fname + '.PDF'] = pn\n",
        "\n",
        "        # 3. Load Structure\n",
        "        with open(STRUCTURE_PATH, 'r') as f:\n",
        "            structure_data = json.load(f)\n",
        "\n",
        "        part_context_db = {}\n",
        "        def normalize_pn(pn): return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
        "\n",
        "        for assembly_name, parts_list in structure_data.items():\n",
        "            for part in parts_list:\n",
        "                pn = part['pn']\n",
        "                desc = part['desc']\n",
        "                # Build siblings list\n",
        "                siblings_list = [f\"{p['pn']} ({str(p['desc']).replace('\"', \"'\")})\" for p in parts_list if p['pn'] != pn]\n",
        "                siblings_str = \"; \".join(siblings_list[:12])\n",
        "                if len(siblings_list) > 12: siblings_str += \"...\"\n",
        "\n",
        "                key = normalize_pn(pn)\n",
        "                context = {\n",
        "                    'pn': pn,\n",
        "                    'description': desc,\n",
        "                    'assembly': assembly_name,\n",
        "                    'siblings': siblings_str,\n",
        "                    'siblings_list': [p['pn'] for p in parts_list if p['pn'] != pn]\n",
        "                }\n",
        "                # Store by normalized key and raw key\n",
        "                part_context_db[key] = context\n",
        "                part_context_db[pn] = context\n",
        "\n",
        "        print(f\"‚úÖ Context Restored: {len(filename_to_pn)} file mappings, {len(part_context_db)//2} part contexts.\")\n",
        "else:\n",
        "    print(\"‚úÖ Context databases are already active.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Reloading Context Databases (Required after restart)...\n",
            "‚úÖ Context Restored: 780 file mappings, 127 part contexts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc691493",
        "outputId": "f5f28246-2e27-46c7-806d-2ed5f175c633"
      },
      "source": [
        "# Install PaddlePaddle (Retry with standard PyPI)\n",
        "!pip install -q paddlepaddle\n",
        "!pip install -q \"paddleocr>=2.0.1\"\n",
        "\n",
        "# Install OpenCV for image handling\n",
        "!pip install -q opencv-python-headless\n",
        "\n",
        "print(\"‚úÖ OCR Libraries Installed!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OCR Libraries Installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "666f60f1",
        "outputId": "1d289435-1a92-4882-f6cb-d325c04fb89e"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Define DATA_DIR if missing (Recovery)\n",
        "if 'DATA_DIR' not in globals():\n",
        "    DATA_DIR = \"/content\"\n",
        "\n",
        "# Check where the code is looking\n",
        "rag_db_path = os.path.join(DATA_DIR, \"rag_visual_db\")\n",
        "\n",
        "print(f\"Current DATA_DIR: {DATA_DIR}\")\n",
        "print(f\"Code looks for RAG images at: {rag_db_path}\")\n",
        "\n",
        "if os.path.exists(rag_db_path):\n",
        "    print(\"‚úÖ Folder exists.\")\n",
        "    # Count images\n",
        "    images = glob.glob(os.path.join(rag_db_path, \"**\", \"*.png\"), recursive=True)\n",
        "    images += glob.glob(os.path.join(rag_db_path, \"**\", \"*.jpg\"), recursive=True)\n",
        "    print(f\"‚úÖ Found {len(images)} images in database.\")\n",
        "else:\n",
        "    print(\"‚ùå Folder NOT found.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current DATA_DIR: /content\n",
            "Code looks for RAG images at: /content/rag_visual_db\n",
            "‚úÖ Folder exists.\n",
            "‚úÖ Found 294 images in database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JB5KJmE_wHHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "2a899988-d821-4c99-99b1-aa88f9eb9f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a ZIP file containing PDF drawings for batch inspection:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e1066b0-49f2-4db0-926f-b0cc70a3087e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e1066b0-49f2-4db0-926f-b0cc70a3087e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-983648008.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upload a ZIP file containing PDF drawings for batch inspection:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    162\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    163\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# === BATCH TEST ===\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "print(\"Upload a ZIP file containing PDF drawings for batch inspection:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Handle the first zip found\n",
        "    zip_filename = next((f for f in uploaded if f.lower().endswith('.zip')), None)\n",
        "\n",
        "    if zip_filename:\n",
        "        # Cleanup old batch dir\n",
        "        batch_dir = \"batch_drawings\"\n",
        "        if os.path.exists(batch_dir):\n",
        "            shutil.rmtree(batch_dir)\n",
        "        os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {zip_filename} to {batch_dir}...\")\n",
        "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(batch_dir)\n",
        "\n",
        "        # Count files\n",
        "        pdfs = glob.glob(os.path.join(batch_dir, \"**/*.pdf\"), recursive=True)\n",
        "        print(f\"Found {len(pdfs)} PDFs in archive.\")\n",
        "\n",
        "        print(f\"Running batch inspection...\")\n",
        "        results = inspect_batch(\n",
        "            drawing_folder=batch_dir,\n",
        "            output_file=\"inspection_results.json\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"No .zip file found in upload.\")\n",
        "else:\n",
        "    print(\"No files uploaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG8mlYNMwHHb"
      },
      "source": [
        "## 8. View Failed Inspections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DgLE5trwHHb"
      },
      "outputs": [],
      "source": [
        "def show_failures(results):\n",
        "    \"\"\"Display details of failed inspections.\"\"\"\n",
        "    failures = [r for r in results if r.get('result') == 'FAIL']\n",
        "\n",
        "    print(f\"\\nFAILED INSPECTIONS: {len(failures)}\")\n",
        "    print('='*60)\n",
        "\n",
        "    for i, fail in enumerate(failures, 1):\n",
        "        print(f\"\\n[{i}] {fail.get('file', 'Unknown')}\")\n",
        "        print(f\"    Part: {fail.get('part_number', 'N/A')} - {fail.get('description', 'N/A')}\")\n",
        "        print(f\"    Assembly: {fail.get('assembly', 'N/A')}\")\n",
        "        print(f\"    GD&T Found: {fail.get('gdt_symbols', 'N/A')}\")\n",
        "        print(f\"    Details: {fail.get('details', 'N/A')[:500]}...\")\n",
        "\n",
        "\n",
        "# Usage:\n",
        "# show_failures(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2cb373474b194b8da3ef1dc6753df2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da6c3848895042bb9985d5febae6250a",
              "IPY_MODEL_f2c44e9a7b7744e682ae9b1959b31764",
              "IPY_MODEL_40681f3c555c4b7c8151e328ebafcbb3"
            ],
            "layout": "IPY_MODEL_93130fc6f2204866813a448860e0b8e3"
          }
        },
        "da6c3848895042bb9985d5febae6250a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47c9fcd759d749e9866e8c5e4a42908b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_07ee67ba714d4787b7c3a8650421445f",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "f2c44e9a7b7744e682ae9b1959b31764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ca9810080244d0aafac09c78ab027ea",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97f20662d5754c30818216f746845a52",
            "value": 38
          }
        },
        "40681f3c555c4b7c8151e328ebafcbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d3c5bc3a43f40f595a4ecfc1da171a5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4998675f0ff244c8a4309534a2fe7e09",
            "value": "‚Äá38/38‚Äá[13:08&lt;00:00,‚Äá16.04s/it]"
          }
        },
        "93130fc6f2204866813a448860e0b8e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c9fcd759d749e9866e8c5e4a42908b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07ee67ba714d4787b7c3a8650421445f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ca9810080244d0aafac09c78ab027ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f20662d5754c30818216f746845a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d3c5bc3a43f40f595a4ecfc1da171a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4998675f0ff244c8a4309534a2fe7e09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}