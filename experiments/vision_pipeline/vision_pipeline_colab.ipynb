{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Pipeline: GPT-4o Engineering Drawing Inspector\n",
    "\n",
    "**What this does:** Inspects engineering drawing PDFs against SolidWorks CAD data using GPT-4o vision.\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "Drawing Image -> GPT-4o Vision Extract -> Normalize -> Validate -> Expand -> Match vs SW -> Score -> QC Report\n",
    "```\n",
    "\n",
    "**No GPU required** — runs on CPU + OpenAI API.\n",
    "\n",
    "**How to use:**\n",
    "1. Run Cell 1 to install the package\n",
    "2. Run Cell 2 to upload your drawing image and SolidWorks JSON\n",
    "3. Run all remaining cells\n",
    "4. Review the QC report in Cell 13\n",
    "\n",
    "**Required:**\n",
    "- `OPENAI_API_KEY` (set in Colab Secrets or enter when prompted)\n",
    "- Drawing image (PNG/JPG)\n",
    "- SolidWorks JSON file (from SolidWorksExtractor)\n",
    "\n",
    "**Optional:**\n",
    "- `sw_mating_context.json` — assembly mating relationships\n",
    "- `sw_mate_specs.json` — thread specs and interface constraints\n",
    "- `sw_part_context_complete.json` — part number bridging database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Install ai_inspector package from GitHub\n",
    "import subprocess, sys, os\n",
    "\n",
    "# Clone the repo (force fresh to avoid stale cache)\n",
    "REPO_URL = 'https://github.com/Continental-Direct/AI-tool.git'\n",
    "REPO_DIR = '/content/AI-tool'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !rm -rf {REPO_DIR}\n",
    "\n",
    "!git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Install the package + dependencies\n",
    "!pip install -q openai Pillow python-dotenv\n",
    "!pip install -q -e {REPO_DIR}\n",
    "\n",
    "# Add to path\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print('\\nInstallation complete.')\n",
    "print(f'Python: {sys.version}')\n",
    "\n",
    "# Verify import\n",
    "from ai_inspector.pipeline.vision_pipeline import VisionPipeline\n",
    "from ai_inspector.report.qc_report import generate_from_pipeline\n",
    "print('ai_inspector package imported successfully.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Upload files and configure API key\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = '/content/inspection_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# API KEY — tries Colab Secrets first, then prompts\n",
    "# ============================================================\n",
    "OPENAI_API_KEY = None\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(f'OPENAI_API_KEY loaded from Colab Secrets (len={len(OPENAI_API_KEY)})')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    import getpass\n",
    "    OPENAI_API_KEY = getpass.getpass('Enter your OpenAI API key: ')\n",
    "    print(f'OPENAI_API_KEY entered (len={len(OPENAI_API_KEY)})')\n",
    "\n",
    "# ============================================================\n",
    "# UPLOAD FILES\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "\n",
    "print('\\n--- Upload your DRAWING IMAGE (PNG/JPG) ---')\n",
    "uploaded_images = files.upload()\n",
    "SAMPLE_IMAGE = '/content/' + list(uploaded_images.keys())[0]\n",
    "print(f'Drawing: {SAMPLE_IMAGE}')\n",
    "\n",
    "print('\\n--- Upload your SOLIDWORKS JSON ---')\n",
    "print('(Click Cancel or upload an empty file to skip SW comparison)')\n",
    "try:\n",
    "    uploaded_sw = files.upload()\n",
    "    if uploaded_sw:\n",
    "        SW_JSON_PATH = '/content/' + list(uploaded_sw.keys())[0]\n",
    "        print(f'SW JSON: {SW_JSON_PATH}')\n",
    "    else:\n",
    "        SW_JSON_PATH = ''\n",
    "except Exception:\n",
    "    SW_JSON_PATH = ''\n",
    "    print('No SW JSON uploaded — running extraction only (no matching).')\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: Assembly context databases\n",
    "# ============================================================\n",
    "print('\\n--- (Optional) Upload assembly context files ---')\n",
    "print('Upload sw_mating_context.json, sw_mate_specs.json, sw_part_context_complete.json')\n",
    "print('Or click Cancel to skip assembly context.')\n",
    "\n",
    "MATING_CONTEXT_PATH = ''\n",
    "MATE_SPECS_PATH = ''\n",
    "PART_CONTEXT_PATH = ''\n",
    "\n",
    "try:\n",
    "    uploaded_ctx = files.upload()\n",
    "    for fname in uploaded_ctx.keys():\n",
    "        fpath = '/content/' + fname\n",
    "        if 'mating_context' in fname:\n",
    "            MATING_CONTEXT_PATH = fpath\n",
    "        elif 'mate_specs' in fname:\n",
    "            MATE_SPECS_PATH = fpath\n",
    "        elif 'part_context' in fname:\n",
    "            PART_CONTEXT_PATH = fpath\n",
    "        print(f'  Loaded: {fname}')\n",
    "except Exception:\n",
    "    print('No assembly context files uploaded.')\n",
    "\n",
    "# ============================================================\n",
    "print(f'\\n=== Configuration ===')\n",
    "print(f'Drawing:        {SAMPLE_IMAGE}')\n",
    "print(f'SW JSON:        {SW_JSON_PATH or \"<none>\"}')\n",
    "print(f'Mating context: {MATING_CONTEXT_PATH or \"<none>\"}')\n",
    "print(f'Mate specs:     {MATE_SPECS_PATH or \"<none>\"}')\n",
    "print(f'Part context:   {PART_CONTEXT_PATH or \"<none>\"}')\n",
    "print(f'Output dir:     {OUTPUT_DIR}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Import and create pipeline\n",
    "from ai_inspector.pipeline.vision_pipeline import VisionPipeline\n",
    "from ai_inspector.pipeline.yolo_pipeline import PipelineResult\n",
    "\n",
    "pipeline = VisionPipeline(api_key=OPENAI_API_KEY)\n",
    "pipeline.load()\n",
    "\n",
    "print(f'VisionPipeline ready: {pipeline.is_loaded}')\n",
    "print('No GPU models needed — extraction via GPT-4o API')\n",
    "print(f'PipelineResult fields: {list(PipelineResult.__dataclass_fields__.keys())}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Run the vision pipeline\n",
    "import time, os\n",
    "\n",
    "print(f'Running GPT-4o vision pipeline on: {os.path.basename(SAMPLE_IMAGE)}')\n",
    "print(f'Mating context: {\"ON\" if MATING_CONTEXT_PATH else \"OFF\"}')\n",
    "print(f'Mate specs:     {\"ON\" if MATE_SPECS_PATH else \"OFF\"}')\n",
    "print(f'Part context:   {\"ON\" if PART_CONTEXT_PATH else \"OFF\"}')\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result = pipeline.run(\n",
    "    image_path=SAMPLE_IMAGE,\n",
    "    sw_json_path=SW_JSON_PATH or None,\n",
    "    page_id='page_0',\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_crops=False,\n",
    "    use_vlm=False,\n",
    "    mating_context_path=MATING_CONTEXT_PATH or None,\n",
    "    mate_specs_path=MATE_SPECS_PATH or None,\n",
    "    part_context_path=PART_CONTEXT_PATH or None,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "extracted = result.packet_summary.get('extracted_callouts', 0)\n",
    "print(f'Pipeline completed in {elapsed:.1f}s')\n",
    "print(f'Extracted callouts: {extracted}  (via GPT-4o vision)')\n",
    "print(f'Match results: {len(result.match_results)}')\n",
    "print(f'Mating context: {\"found\" if result.mating_context else \"not found\"}')\n",
    "print(f'Mate specs: {\"found\" if result.mate_specs else \"not found\"}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Scores and summary\n",
    "import json\n",
    "\n",
    "print('=== SCORES ===')\n",
    "if result.scores:\n",
    "    for key, val in result.scores.items():\n",
    "        print(f'  {key:25s}: {val}')\n",
    "else:\n",
    "    print('  (no scores — run without SW data)')\n",
    "\n",
    "print('\\n=== EXPANSION SUMMARY ===')\n",
    "print(json.dumps(result.expansion_summary, indent=2))\n",
    "\n",
    "print('\\n=== VALIDATION STATS ===')\n",
    "print(json.dumps(result.validation_stats, indent=2))\n",
    "\n",
    "print('\\n=== PACKET SUMMARY ===')\n",
    "print(json.dumps(result.packet_summary, indent=2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Match results table\n",
    "from ai_inspector.comparison.matcher import MatchStatus\n",
    "\n",
    "if not result.match_results:\n",
    "    print('No match results (no SW data provided or no features detected).')\n",
    "else:\n",
    "    print(f'{\"#\":>3s} {\"Status\":15s} {\"Type\":18s} {\"Delta\":>10s} {\"Notes\"}')\n",
    "    print('=' * 90)\n",
    "\n",
    "    for i, r in enumerate(result.match_results):\n",
    "        callout_type = ''\n",
    "        if r.drawing_callout:\n",
    "            callout_type = r.drawing_callout.get('calloutType', '')\n",
    "        elif r.sw_feature:\n",
    "            callout_type = r.sw_feature.feature_type\n",
    "\n",
    "        delta_str = f'{r.delta:+.4f}' if r.delta is not None else 'N/A'\n",
    "\n",
    "        marker = {\n",
    "            MatchStatus.MATCHED: '[OK]',\n",
    "            MatchStatus.MISSING: '[MISS]',\n",
    "            MatchStatus.EXTRA: '[EXTRA]',\n",
    "            MatchStatus.TOLERANCE_FAIL: '[TOL]',\n",
    "            MatchStatus.SKIPPED: '[SKIP]',\n",
    "        }.get(r.status, '[?]')\n",
    "\n",
    "        print(f'{i:3d} {marker + \" \" + r.status.value:15s} '\n",
    "              f'{callout_type:18s} {delta_str:>10s} {r.notes[:50]}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Assembly Mating Context\n",
    "\n",
    "mc = result.mating_context\n",
    "if not mc:\n",
    "    print('No mating context (path not provided or part not found in database).')\n",
    "else:\n",
    "    print('=== ASSEMBLY MATING CONTEXT ===')\n",
    "    print()\n",
    "    print(f'  Part number:   {mc.get(\"part_number\", \"N/A\")}')\n",
    "    print(f'  Description:   {mc.get(\"description\", \"N/A\")}')\n",
    "    print(f'  Type:          {mc.get(\"type\", \"N/A\")}')\n",
    "    print(f'  Assembly:      {mc.get(\"assembly\", \"N/A\")}')\n",
    "\n",
    "    siblings = mc.get('siblings', [])\n",
    "    if siblings:\n",
    "        print(f'\\n  --- Sibling Components ({len(siblings)}) ---')\n",
    "        for s in siblings:\n",
    "            print(f'    {s.get(\"pn\", \"?\")} - {s.get(\"desc\", \"?\")} ({s.get(\"type\", \"?\")})')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Mate Specifications\n",
    "\n",
    "ms = result.mate_specs\n",
    "if not ms:\n",
    "    print('No mate specs (path not provided or part/siblings not found).')\n",
    "else:\n",
    "    source = ms.get('source', 'direct')\n",
    "    print(f'=== MATE SPECIFICATIONS (source: {source}) ===')\n",
    "    print()\n",
    "\n",
    "    if source == 'sibling_cross_reference':\n",
    "        print(f'  Part {ms.get(\"part_number\")} not directly in mate_specs.')\n",
    "        print(f'  Showing specs from {len(ms.get(\"sibling_specs\", []))} sibling(s):')\n",
    "        for spec in ms.get('sibling_specs', []):\n",
    "            pn = spec.get('part_number', '?')\n",
    "            desc = spec.get('description', '')\n",
    "            print(f'\\n  --- Sibling: {pn} ({desc}) ---')\n",
    "            for m in spec.get('mates_with', []):\n",
    "                line = f'    {m.get(\"mate_type\", \"?\")} with {m.get(\"part\", \"?\")}'\n",
    "                if m.get('description'):\n",
    "                    line += f' ({m[\"description\"]})'\n",
    "                if m.get('thread'):\n",
    "                    line += f'  ** THREAD: {m[\"thread\"]} pitch={m.get(\"pitch\",\"\")} len={m.get(\"length\",\"\")} **'\n",
    "                print(line)\n",
    "    else:\n",
    "        print(f'  Part: {ms.get(\"part_number\", \"?\")}')\n",
    "        print(f'  Description: {ms.get(\"description\", \"\")}')\n",
    "        for m in ms.get('mates_with', []):\n",
    "            line = f'    {m.get(\"mate_type\", \"?\")} with {m.get(\"part\", \"?\")}'\n",
    "            if m.get('description'):\n",
    "                line += f' ({m[\"description\"]})'\n",
    "            if m.get('thread'):\n",
    "                line += f'  ** THREAD: {m[\"thread\"]} pitch={m.get(\"pitch\",\"\")} len={m.get(\"length\",\"\")} **'\n",
    "            print(line)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 9: GPT-4o Extracted Callouts\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "callouts_path = Path(OUTPUT_DIR) / 'callouts_extracted.json'\n",
    "if callouts_path.exists():\n",
    "    with open(callouts_path, 'r', encoding='utf-8') as f:\n",
    "        callouts = json.load(f)\n",
    "    print(f'=== GPT-4o Extracted Callouts ({len(callouts)}) ===')\n",
    "    print()\n",
    "    for i, c in enumerate(callouts):\n",
    "        ct = c.get('calloutType', '?')\n",
    "        raw = c.get('raw', '')[:60]\n",
    "        qty = c.get('quantity', 1)\n",
    "        parts = [f'{ct}']\n",
    "        if c.get('diameter') is not None:\n",
    "            parts.append(f'dia={c[\"diameter\"]}')\n",
    "        if c.get('radius') is not None:\n",
    "            parts.append(f'R={c[\"radius\"]}')\n",
    "        if c.get('nominal') is not None:\n",
    "            parts.append(f'nom={c[\"nominal\"]}')\n",
    "        if c.get('thread'):\n",
    "            t = c['thread']\n",
    "            parts.append(f'thread={t.get(\"raw\", \"\")}')\n",
    "        if qty > 1:\n",
    "            parts.append(f'qty={qty}')\n",
    "        print(f'  {i+1:2d}. {\" | \".join(parts)}')\n",
    "        print(f'      raw: \"{raw}\"')\n",
    "else:\n",
    "    print('No callouts_extracted.json found.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 10: List saved artifacts\n",
    "from pathlib import Path\n",
    "\n",
    "out = Path(OUTPUT_DIR)\n",
    "print(f'Artifacts saved to: {OUTPUT_DIR}/')\n",
    "print()\n",
    "\n",
    "for f in sorted(out.rglob('*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f'  {f.relative_to(out)}  ({size_kb:.1f} KB)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 11: GPT-4o QC Report — full JSON context\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from ai_inspector.report.qc_report import generate_from_pipeline\n",
    "\n",
    "# --- Load extracted callouts from debug output ---\n",
    "extracted_callouts = []\n",
    "callouts_path = Path(OUTPUT_DIR) / 'callouts_extracted.json'\n",
    "if callouts_path.exists():\n",
    "    with open(callouts_path, 'r', encoding='utf-8') as f:\n",
    "        extracted_callouts = json.load(f)\n",
    "\n",
    "# --- Load validated callouts from debug output ---\n",
    "validated_callouts = []\n",
    "validated_path = Path(OUTPUT_DIR) / 'validated_callouts.json'\n",
    "if validated_path.exists():\n",
    "    with open(validated_path, 'r', encoding='utf-8') as f:\n",
    "        validated_callouts = json.load(f)\n",
    "\n",
    "# --- Load SW identity (part number, description, material, etc.) ---\n",
    "sw_identity = {}\n",
    "if SW_JSON_PATH and os.path.exists(SW_JSON_PATH):\n",
    "    with open(SW_JSON_PATH, 'r', encoding='utf-8-sig') as f:\n",
    "        sw_raw = json.load(f)\n",
    "    sw_identity = sw_raw.get('identity', {})\n",
    "\n",
    "# --- Generate report with full JSON context ---\n",
    "print(f'Part: {sw_identity.get(\"partNumber\", \"?\")} -- {sw_identity.get(\"description\", \"?\")}')\n",
    "print(f'Extracted callouts: {len(extracted_callouts)}')\n",
    "print(f'Validated callouts: {len(validated_callouts)}')\n",
    "print(f'Match results: {len(result.match_results)}')\n",
    "print()\n",
    "print('Sending full inspection context as JSON to GPT-4o...')\n",
    "\n",
    "report, inspection_context = generate_from_pipeline(\n",
    "    result=result,\n",
    "    extracted_callouts=extracted_callouts,\n",
    "    validated_callouts=validated_callouts,\n",
    "    sw_identity=sw_identity,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4o',\n",
    ")\n",
    "\n",
    "print(f'Report generated -- Status: {report.status}')\n",
    "print(f'Model: {report.model_used}')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print(report.report_text)\n",
    "print('=' * 70)\n",
    "\n",
    "# Save report\n",
    "report_path = Path(OUTPUT_DIR) / 'qc_report.md'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(report.report_text)\n",
    "\n",
    "# Save full context that was sent to GPT\n",
    "context_path = Path(OUTPUT_DIR) / 'inspection_context.json'\n",
    "with open(context_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(inspection_context, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "# Save structured report data\n",
    "report_data_path = Path(OUTPUT_DIR) / 'qc_report.json'\n",
    "with open(report_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report.to_dict(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f'\\nReport saved to: {report_path}')\n",
    "print(f'Full context saved to: {context_path}')\n",
    "print(f'Report data saved to: {report_data_path}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 12: Download results\n",
    "from google.colab import files as colab_files\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Zip all output artifacts\n",
    "zip_path = shutil.make_archive('/content/inspection_results', 'zip', OUTPUT_DIR)\n",
    "print(f'Results zipped: {zip_path}')\n",
    "print()\n",
    "\n",
    "# List contents\n",
    "out = Path(OUTPUT_DIR)\n",
    "for f in sorted(out.rglob('*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f'  {f.relative_to(out)}  ({size_kb:.1f} KB)')\n",
    "\n",
    "print('\\nDownloading results zip...')\n",
    "colab_files.download(zip_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 13: Cleanup\n",
    "import gc\n",
    "\n",
    "pipeline.unload()\n",
    "gc.collect()\n",
    "print('Pipeline unloaded. Done.')"
   ],
   "outputs": []
  }
 ]
}