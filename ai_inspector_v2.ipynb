{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/ai_inspector_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QxQqDl17wtB"
   },
   "source": [
    "# AI Engineering Drawing Inspector v2.0\n",
    "\n",
    "**Single-File QC Pipeline**\n",
    "\n",
    "Outputs:\n",
    "1. `ResolvedPartIdentity.json`\n",
    "2. `DrawingEvidence.json`\n",
    "3. `DiffResult.json`\n",
    "4. `QCReport.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUxTZFYm7wtC"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q pymupdf opencv-python-headless jsonschema pillow pytesseract\n",
    "!pip install -q accelerate qwen-vl-utils bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import os, json, re, gc\n",
    "import torch\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DRAWING_PDF_PATH = \"\"\n",
    "SOLIDWORKS_JSON_DIR = \"sw_json_library\"\n",
    "OUTPUT_DIR = \"qc_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "metadata": {
    "id": "BywKsGSW7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: BOM-Robust JSON Loader\n",
    "def load_json_robust(filepath) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"Load JSON with BOM handling. Tries: utf-8-sig, utf-8, latin-1\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=enc) as f:\n",
    "                return json.load(f), None\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except json.JSONDecodeError as e:\n",
    "            if 'BOM' in str(e) and enc == 'utf-8':\n",
    "                continue\n",
    "            return None, f\"JSON error: {str(e)[:50]}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Error: {str(e)[:50]}\"\n",
    "    return None, \"Failed all encodings\"\n",
    "\n",
    "print(\"load_json_robust defined\")"
   ],
   "metadata": {
    "id": "Fip3em4Z7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: PDF Rendering\n",
    "@dataclass\n",
    "class PageArtifact:\n",
    "    pageIndex0: int\n",
    "    page: int\n",
    "    image: Image.Image\n",
    "    width: int\n",
    "    height: int\n",
    "    dpi: int\n",
    "    direct_text: Optional[str] = None\n",
    "\n",
    "def render_pdf(pdf_path: str, dpi: int = 300) -> List[PageArtifact]:\n",
    "    \"\"\"Render first page of PDF to image.\"\"\"\n",
    "    artifacts = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    zoom = dpi / 72.0\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    direct_text = page.get_text(\"text\")\n",
    "\n",
    "    artifacts.append(PageArtifact(\n",
    "        pageIndex0=0, page=1, image=img,\n",
    "        width=pix.width, height=pix.height, dpi=dpi,\n",
    "        direct_text=direct_text if len(direct_text.strip()) > 10 else None\n",
    "    ))\n",
    "    doc.close()\n",
    "    print(f\"Rendered: {pix.width}x{pix.height}px\")\n",
    "    return artifacts\n",
    "\n",
    "print(\"render_pdf defined\")"
   ],
   "metadata": {
    "id": "g5CgMeOp7wtD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: SolidWorks JSON Library\n",
    "@dataclass\n",
    "class SwPartEntry:\n",
    "    json_path: str\n",
    "    part_number: str\n",
    "    filename_stem: str = \"\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class SwJsonLibrary:\n",
    "    def __init__(self):\n",
    "        self.by_part_number: Dict[str, SwPartEntry] = {}\n",
    "        self.by_filename: Dict[str, SwPartEntry] = {}\n",
    "        self.all_entries: List[SwPartEntry] = []\n",
    "\n",
    "    def _normalize(self, s: str) -> str:\n",
    "        return re.sub(r'[-\\s_]', '', str(s or '')).lower()\n",
    "\n",
    "    def load_from_directory(self, directory: str):\n",
    "        json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for jp in json_files:\n",
    "            data, err = load_json_robust(jp)\n",
    "            if data is None:\n",
    "                continue\n",
    "            pn = data.get('identity', {}).get('partNumber', '')\n",
    "            entry = SwPartEntry(str(jp), pn, jp.stem, data)\n",
    "            self.all_entries.append(entry)\n",
    "            if pn:\n",
    "                self.by_part_number[pn] = entry\n",
    "                self.by_part_number[self._normalize(pn)] = entry\n",
    "            self.by_filename[jp.stem] = entry\n",
    "            self.by_filename[self._normalize(jp.stem)] = entry\n",
    "        print(f\"Loaded {len(self.all_entries)} files\")\n",
    "\n",
    "    def lookup(self, candidate: str) -> Optional[SwPartEntry]:\n",
    "        if not candidate:\n",
    "            return None\n",
    "        norm = self._normalize(candidate)\n",
    "        return self.by_part_number.get(candidate) or self.by_part_number.get(norm) or \\\n",
    "               self.by_filename.get(candidate) or self.by_filename.get(norm)\n",
    "\n",
    "sw_library = SwJsonLibrary()\n",
    "print(\"SwJsonLibrary defined\")"
   ],
   "metadata": {
    "id": "w7qDW6w27wtE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6: Part Identity Resolution (Robust Matching)\n",
    "\n",
    "@dataclass\n",
    "class ResolvedPartIdentity:\n",
    "    partNumber: str\n",
    "    confidence: float\n",
    "    source: str\n",
    "    swJsonPath: Optional[str] = None\n",
    "    candidates_tried: List[str] = field(default_factory=list)\n",
    "\n",
    "def clean_filename(filename: str) -> str:\n",
    "    \"\"\"Remove known suffixes like Paint, REV, etc.\"\"\"\n",
    "    cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', filename, flags=re.IGNORECASE)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_pn_candidates(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract potential part number candidates from filename.\n",
    "    Handles: 1013572_01, 101357201-03, 314884W_0, 046-935-REV-A\n",
    "    Returns list of candidates (most specific to least).\n",
    "    \"\"\"\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    # Remove duplicate markers like (1), (2)\n",
    "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)\n",
    "    cleaned = clean_filename(name_no_ext)\n",
    "    parts = re.split(r'[\\s_]+', cleaned)\n",
    "\n",
    "    if not parts:\n",
    "        return []\n",
    "\n",
    "    base = parts[0]\n",
    "    candidates = []\n",
    "\n",
    "    # 1. Base as-is\n",
    "    candidates.append(base)\n",
    "\n",
    "    # 2. Without hyphens\n",
    "    base_no_hyphen = base.replace('-', '')\n",
    "    if base_no_hyphen != base:\n",
    "        candidates.append(base_no_hyphen)\n",
    "\n",
    "    # 3. Remove letter suffixes (046-935A -> 046-935)\n",
    "    if base and base[-1].isalpha() and len(base) > 1:\n",
    "        candidates.append(base[:-1])\n",
    "        candidates.append(base[:-1].replace('-', ''))\n",
    "\n",
    "    # 4. Handle revision pattern (046-935-01 -> 046-935)\n",
    "    rev_match = re.match(r'^(.+)-(\\d{1,2})$', base)\n",
    "    if rev_match:\n",
    "        main_part = rev_match.group(1)\n",
    "        candidates.append(main_part)\n",
    "        candidates.append(main_part.replace('-', ''))\n",
    "\n",
    "    # 5. Handle REV suffix (046-935-REV-A -> 046-935)\n",
    "    rev_alpha = re.match(r'^(.+?)[-_]?REV[-_]?[A-Z0-9]*$', base, re.IGNORECASE)\n",
    "    if rev_alpha:\n",
    "        candidates.append(rev_alpha.group(1))\n",
    "        candidates.append(rev_alpha.group(1).replace('-', ''))\n",
    "\n",
    "    # 6. Peeling - progressively remove trailing digits\n",
    "    temp = base_no_hyphen\n",
    "    while len(temp) > 5:\n",
    "        temp = temp[:-1]\n",
    "        candidates.append(temp)\n",
    "\n",
    "    # Remove duplicates, preserve order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return unique\n",
    "\n",
    "def resolve_part_identity(pdf_path: str, artifacts: List[PageArtifact], sw_lib: SwJsonLibrary) -> ResolvedPartIdentity:\n",
    "    \"\"\"Resolve part identity using robust filename matching.\"\"\"\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    candidates = extract_pn_candidates(filename)\n",
    "\n",
    "    # Try each candidate against SW library\n",
    "    for candidate in candidates:\n",
    "        entry = sw_lib.lookup(candidate)\n",
    "        if entry:\n",
    "            return ResolvedPartIdentity(\n",
    "                partNumber=entry.part_number or candidate,\n",
    "                confidence=1.0,\n",
    "                source=\"filename+sw\",\n",
    "                swJsonPath=entry.json_path,\n",
    "                candidates_tried=candidates\n",
    "            )\n",
    "\n",
    "    # Try PDF embedded text\n",
    "    for art in artifacts:\n",
    "        if art.direct_text:\n",
    "            text_candidates = extract_pn_candidates(art.direct_text[:200])\n",
    "            for candidate in text_candidates[:5]:\n",
    "                entry = sw_lib.lookup(candidate)\n",
    "                if entry:\n",
    "                    return ResolvedPartIdentity(\n",
    "                        partNumber=entry.part_number or candidate,\n",
    "                        confidence=0.8,\n",
    "                        source=\"pdf_text+sw\",\n",
    "                        swJsonPath=entry.json_path,\n",
    "                        candidates_tried=candidates + text_candidates[:5]\n",
    "                    )\n",
    "\n",
    "    # Fallback - use first candidate or filename stem\n",
    "    fallback_pn = candidates[0] if candidates else Path(pdf_path).stem\n",
    "    return ResolvedPartIdentity(\n",
    "        partNumber=fallback_pn,\n",
    "        confidence=0.3,\n",
    "        source=\"fallback\",\n",
    "        swJsonPath=None,\n",
    "        candidates_tried=candidates\n",
    "    )\n",
    "\n",
    "print(\"resolve_part_identity defined (robust matching)\")"
   ],
   "metadata": {
    "id": "U5TWtUzhJnRw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Load SolidWorks Library (Upload ZIP)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(SOLIDWORKS_JSON_DIR) or not list(Path(SOLIDWORKS_JSON_DIR).glob(\"*.json\")):\n",
    "    print(\"Upload your sw_json_library.zip file:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded:\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as z:\n",
    "                z.extractall(SOLIDWORKS_JSON_DIR)\n",
    "            print(f\"Extracted to {SOLIDWORKS_JSON_DIR}\")\n",
    "            break\n",
    "\n",
    "sw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\n",
    "print(f\"Library ready: {len(sw_library.all_entries)} parts indexed\")"
   ],
   "metadata": {
    "id": "qOgdhn3iJnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Upload and Render PDF Drawing\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Upload your PDF drawing:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded:\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        DRAWING_PDF_PATH = filename\n",
    "        break\n",
    "\n",
    "print(f\"Processing: {DRAWING_PDF_PATH}\")\n",
    "artifacts = render_pdf(DRAWING_PDF_PATH)\n",
    "\n",
    "# Display the rendered image\n",
    "if artifacts:\n",
    "    display(artifacts[0].image.resize((800, int(800 * artifacts[0].height / artifacts[0].width))))"
   ],
   "metadata": {
    "id": "ZQJrQtNzJnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 9: Resolve Part Identity\n",
    "part_identity = resolve_part_identity(DRAWING_PDF_PATH, artifacts, sw_library)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RESOLVED PART IDENTITY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Part Number:  {part_identity.partNumber}\")\n",
    "print(f\"Confidence:   {part_identity.confidence}\")\n",
    "print(f\"Source:       {part_identity.source}\")\n",
    "print(f\"SW JSON:      {part_identity.swJsonPath or 'Not found'}\")\n",
    "print(f\"Candidates:   {part_identity.candidates_tried[:5]}\")\n",
    "\n",
    "# Save to output\n",
    "identity_out = os.path.join(OUTPUT_DIR, \"ResolvedPartIdentity.json\")\n",
    "with open(identity_out, 'w') as f:\n",
    "    json.dump(asdict(part_identity), f, indent=2)\n",
    "print(f\"\\nSaved: {identity_out}\")"
   ],
   "metadata": {
    "id": "nWyBd12_JnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 10: Load LightOnOCR-2 and Run OCR\n",
    "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
    "from google.colab import userdata\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Get HF token\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    hf_token = None\n",
    "\n",
    "print(\"Loading LightOnOCR-2-1B...\")\n",
    "ocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n",
    "\n",
    "ocr_processor = LightOnOcrProcessor.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    torch_dtype=ocr_dtype,\n",
    "    token=hf_token\n",
    ").to(ocr_device)\n",
    "\n",
    "print(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "def run_lighton_ocr(image: Image.Image) -> List[str]:\n",
    "    \"\"\"Run LightOnOCR-2 on image, return list of text lines.\"\"\"\n",
    "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
    "\n",
    "    img = image.convert(\"RGB\")\n",
    "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n",
    "\n",
    "    inputs = ocr_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return [line.strip() for line in output_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "# Run OCR on the drawing\n",
    "print(\"Running OCR on drawing...\")\n",
    "ocr_lines = run_lighton_ocr(artifacts[0].image)\n",
    "print(f\"OCR extracted {len(ocr_lines)} lines\")\n",
    "print(\"\\nFirst 10 lines:\")\n",
    "for line in ocr_lines[:10]:\n",
    "    print(f\"  {line}\")"
   ],
   "metadata": {
    "id": "NQuVga-jJnRx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: Parse OCR to JSON Callouts (Fixed: captures .281 correctly)\n\nINCH_TO_MM = 25.4\n\n# Regex patterns for extracting callouts\nPATTERNS = {\n    # Metric threads: M10x1.5, M6X1.0\n    'metric_thread': r'M(\\d+(?:\\.\\d+)?)\\s*[xX]\\s*(\\d+(?:\\.\\d+)?)',\n    # Imperial threads: 1/2-13, 3/8-16\n    'imperial_thread': r'(\\d+/\\d+)\\s*-\\s*(\\d+)',\n    # Through holes - FIXED: captures .281, 0.281, 12.70 correctly\n    'thru_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*(?:mm|MM|\")?\\s*THRU',\n    # Blind holes: o12.70 x 25 DEEP\n    'blind_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*[xX]\\s*(\\d+\\.?\\d*)\\s*(?:DEEP|DP)',\n    # Dimensions with tolerance: 25.4 \u00b10.05\n    'dim_tolerance': r'(\\d+\\.?\\d*)\\s*[\u00b1]\\s*(\\d+\\.?\\d*)',\n    # Counterbore: CBORE or counterbore\n    'counterbore': r'(?:CBORE|C\\'BORE|COUNTERBORE)\\s*[oO\u00d8\u2205\u03c6]?\\s*(\\d+\\.?\\d*)',\n    # Countersink: CSK or countersink\n    'countersink': r'(?:CSK|C\\'SINK|COUNTERSINK)\\s*(\\d+\\.?\\d*)\\s*[\u00b0]?',\n    # Fillet radius: R3, R0.5\n    'fillet': r'\\bR(\\d+\\.?\\d*)\\b',\n    # Chamfer: 0.5 X 45, 1x45\n    'chamfer': r'(\\d+\\.?\\d*)\\s*[xX]\\s*45\\s*[\u00b0]?',\n    # Quantity: 4X, 2 PLCS, (3X)\n    'quantity': r'[(\\s]?(\\d+)\\s*[xX)\\s]|(\\d+)\\s*PL(?:CS?|ACES?)',\n}\n\ndef is_likely_imperial(value: float, raw_text: str) -> bool:\n    \"\"\"Determine if a dimension is likely in inches (not mm).\"\"\"\n    # Explicit inch marker\n    if '\"' in raw_text or 'IN' in raw_text.upper():\n        return True\n    # Values < 1.0 without 'mm' are likely inches (e.g., .281, 0.500)\n    if value < 1.0 and 'mm' not in raw_text.lower():\n        return True\n    # Common imperial sizes in decimal\n    imperial_sizes = [0.125, 0.1875, 0.25, 0.281, 0.3125, 0.375, 0.4375, 0.5, 0.625, 0.75, 0.875]\n    for imp in imperial_sizes:\n        if abs(value - imp) < 0.01:\n            return True\n    return False\n\ndef convert_to_mm(value: float, raw_text: str) -> float:\n    \"\"\"Convert value to mm if it appears to be in inches.\"\"\"\n    if is_likely_imperial(value, raw_text):\n        return round(value * INCH_TO_MM, 3)\n    return value\n\ndef parse_ocr_to_callouts(ocr_lines: List[str]) -> Dict[str, Any]:\n    \"\"\"Parse OCR text lines into structured callouts.\"\"\"\n    callouts = []\n    raw_text = \"\\n\".join(ocr_lines)\n\n    # Extract metric threads (case-insensitive but preserve for display)\n    for match in re.finditer(PATTERNS['metric_thread'], raw_text, re.IGNORECASE):\n        callouts.append({\n            'calloutType': 'TappedHole',\n            'thread': {\n                'standard': 'Metric',\n                'nominalDiameterMm': float(match.group(1)),\n                'pitch': float(match.group(2))\n            },\n            'raw': match.group(0)\n        })\n\n    # Extract imperial threads\n    for match in re.finditer(PATTERNS['imperial_thread'], raw_text):\n        callouts.append({\n            'calloutType': 'TappedHole',\n            'thread': {\n                'standard': 'Imperial',\n                'size': match.group(1),\n                'tpi': int(match.group(2))\n            },\n            'raw': match.group(0)\n        })\n\n    # Extract through holes (with imperial detection)\n    for match in re.finditer(PATTERNS['thru_hole'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        diameter_mm = convert_to_mm(val, raw)\n        callouts.append({\n            'calloutType': 'Hole',\n            'diameterMm': diameter_mm,\n            'diameterRaw': val,\n            'isImperial': is_likely_imperial(val, raw),\n            'isThrough': True,\n            'raw': raw\n        })\n\n    # Extract blind holes\n    for match in re.finditer(PATTERNS['blind_hole'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        depth_val = float(match.group(2))\n        diameter_mm = convert_to_mm(val, raw)\n        depth_mm = convert_to_mm(depth_val, raw)\n        callouts.append({\n            'calloutType': 'Hole',\n            'diameterMm': diameter_mm,\n            'depthMm': depth_mm,\n            'isThrough': False,\n            'raw': raw\n        })\n\n    # Extract fillets\n    for match in re.finditer(PATTERNS['fillet'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        callouts.append({\n            'calloutType': 'Fillet',\n            'radiusMm': convert_to_mm(val, raw),\n            'raw': raw\n        })\n\n    # Extract chamfers\n    for match in re.finditer(PATTERNS['chamfer'], raw_text, re.IGNORECASE):\n        raw = match.group(0)\n        val = float(match.group(1))\n        callouts.append({\n            'calloutType': 'Chamfer',\n            'distance1Mm': convert_to_mm(val, raw),\n            'angleDegrees': 45,\n            'raw': raw\n        })\n\n    # Build evidence document\n    evidence = {\n        'schemaVersion': '1.1.1',\n        'partNumber': part_identity.partNumber,\n        'extractedAt': datetime.now().isoformat() + 'Z',\n        'ocrLineCount': len(ocr_lines),\n        'foundCallouts': callouts,\n        'rawOcrSample': ocr_lines[:20]\n    }\n\n    return evidence\n\n# Parse and display\nevidence = parse_ocr_to_callouts(ocr_lines)\n\nprint(\"=\"*50)\nprint(\"DRAWING EVIDENCE\")\nprint(\"=\"*50)\nprint(f\"Callouts found: {len(evidence['foundCallouts'])}\")\nfor c in evidence['foundCallouts'][:10]:\n    extra = \"\"\n    if c.get('isImperial'):\n        extra = f\" (raw={c.get('diameterRaw')}\\\" -> {c.get('diameterMm')}mm)\"\n    print(f\"  {c['calloutType']}: {c.get('raw', '')}{extra}\")\n\n# Save to output\nevidence_out = os.path.join(OUTPUT_DIR, \"DrawingEvidence.json\")\nwith open(evidence_out, 'w') as f:\n    json.dump(evidence, f, indent=2)\nprint(f\"\\nSaved: {evidence_out}\")",
   "metadata": {
    "id": "3ZlPb5FJQBHV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 12: Generate DiffResult (Fixed: uses comparison.holeGroups)\n",
    "\n",
    "def extract_sw_requirements(sw_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract requirements from SolidWorks JSON using comparison.holeGroups.\"\"\"\n",
    "    requirements = []\n",
    "\n",
    "    # Primary source: comparison.holeGroups (reconciled/canonical data)\n",
    "    comparison = sw_data.get('comparison', {})\n",
    "    hole_groups = comparison.get('holeGroups', [])\n",
    "\n",
    "    for hg in hole_groups:\n",
    "        hole_type = hg.get('holeType', '')\n",
    "        canonical = hg.get('canonical', '')\n",
    "        count = hg.get('count', 1)\n",
    "        diameters = hg.get('diameters', {})\n",
    "        thread = hg.get('thread', {})\n",
    "\n",
    "        if hole_type == 'Tapped':\n",
    "            # Tapped hole - extract thread info\n",
    "            requirements.append({\n",
    "                'type': 'TappedHole',\n",
    "                'thread': {\n",
    "                    'standard': thread.get('standard', 'Metric'),\n",
    "                    'nominalDiameterMm': thread.get('majorDiameterMm') or diameters.get('threadNominalDiameterMm'),\n",
    "                    'pitch': thread.get('pitch'),\n",
    "                    'callout': thread.get('callout', canonical)\n",
    "                },\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "        elif hole_type == 'Through':\n",
    "            # Plain through hole\n",
    "            diameter_mm = diameters.get('pilotOrTapDrillDiameterMm')\n",
    "            requirements.append({\n",
    "                'type': 'Hole',\n",
    "                'diameterMm': diameter_mm,\n",
    "                'diameterInches': diameters.get('pilotOrTapDrillDiameterInches'),\n",
    "                'isThrough': True,\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "        elif hole_type == 'Blind':\n",
    "            # Blind hole\n",
    "            diameter_mm = diameters.get('pilotOrTapDrillDiameterMm')\n",
    "            requirements.append({\n",
    "                'type': 'Hole',\n",
    "                'diameterMm': diameter_mm,\n",
    "                'isThrough': False,\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "\n",
    "    # Fallback: features.holeWizardHoles if no comparison data\n",
    "    if not requirements:\n",
    "        features = sw_data.get('features', {})\n",
    "        for hole in features.get('holeWizardHoles', []):\n",
    "            if hole.get('isTapped'):\n",
    "                thread_size = hole.get('threadSize', '')\n",
    "                # Parse M6x1.0 format\n",
    "                m = re.match(r'M(\\d+(?:\\.\\d+)?)[xX](\\d+(?:\\.\\d+)?)', thread_size)\n",
    "                if m:\n",
    "                    requirements.append({\n",
    "                        'type': 'TappedHole',\n",
    "                        'thread': {\n",
    "                            'standard': 'Metric',\n",
    "                            'nominalDiameterMm': float(m.group(1)),\n",
    "                            'pitch': float(m.group(2)),\n",
    "                            'callout': thread_size\n",
    "                        },\n",
    "                        'count': hole.get('instanceCount', 1),\n",
    "                        'source': 'sw_features.holeWizardHoles'\n",
    "                    })\n",
    "            else:\n",
    "                requirements.append({\n",
    "                    'type': 'Hole',\n",
    "                    'diameterMm': hole.get('diameter', 0) * 1000,  # meters to mm\n",
    "                    'isThrough': hole.get('isThrough', False),\n",
    "                    'count': hole.get('instanceCount', 1),\n",
    "                    'source': 'sw_features.holeWizardHoles'\n",
    "                })\n",
    "\n",
    "        # Fillets\n",
    "        for fillet in features.get('fillets', []):\n",
    "            requirements.append({\n",
    "                'type': 'Fillet',\n",
    "                'radiusMm': fillet.get('radius'),\n",
    "                'source': 'sw_features'\n",
    "            })\n",
    "\n",
    "        # Chamfers\n",
    "        for chamfer in features.get('chamfers', []):\n",
    "            requirements.append({\n",
    "                'type': 'Chamfer',\n",
    "                'distance1Mm': chamfer.get('distance1'),\n",
    "                'angleDegrees': chamfer.get('angle', 45),\n",
    "                'source': 'sw_features'\n",
    "            })\n",
    "\n",
    "    return requirements\n",
    "\n",
    "def compare_callout_to_requirement(callout: Dict, req: Dict, tolerance: float = 0.5) -> bool:\n",
    "    \"\"\"Check if a drawing callout matches a SW requirement.\"\"\"\n",
    "    ctype = callout.get('calloutType')\n",
    "    rtype = req.get('type')\n",
    "\n",
    "    if ctype != rtype:\n",
    "        return False\n",
    "\n",
    "    if ctype == 'Hole':\n",
    "        d1 = callout.get('diameterMm', 0)\n",
    "        d2 = req.get('diameterMm', 0)\n",
    "        if d1 and d2 and abs(d1 - d2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'TappedHole':\n",
    "        t1 = callout.get('thread', {})\n",
    "        t2 = req.get('thread', {})\n",
    "        # Match by nominal diameter (within 0.1mm tolerance)\n",
    "        nom1 = t1.get('nominalDiameterMm', 0)\n",
    "        nom2 = t2.get('nominalDiameterMm', 0)\n",
    "        if nom1 and nom2 and abs(nom1 - nom2) < 0.1:\n",
    "            # Also check pitch if available\n",
    "            p1 = t1.get('pitch')\n",
    "            p2 = t2.get('pitch')\n",
    "            if p1 and p2:\n",
    "                return abs(p1 - p2) < 0.01\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'Fillet':\n",
    "        r1 = callout.get('radiusMm', 0)\n",
    "        r2 = req.get('radiusMm', 0)\n",
    "        if r1 and r2 and abs(r1 - r2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'Chamfer':\n",
    "        d1 = callout.get('distance1Mm', 0)\n",
    "        d2 = req.get('distance1Mm', 0)\n",
    "        if d1 and d2 and abs(d1 - d2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def generate_diff_result(evidence: Dict, sw_data: Dict) -> Dict:\n",
    "    \"\"\"Compare drawing evidence against SolidWorks requirements.\"\"\"\n",
    "    callouts = evidence.get('foundCallouts', [])\n",
    "    requirements = extract_sw_requirements(sw_data)\n",
    "\n",
    "    found = []\n",
    "    missing = []\n",
    "    matched_callouts = set()\n",
    "    matched_requirements = set()\n",
    "\n",
    "    # Check each requirement against callouts\n",
    "    for ri, req in enumerate(requirements):\n",
    "        match_found = False\n",
    "        for ci, callout in enumerate(callouts):\n",
    "            if ci not in matched_callouts and compare_callout_to_requirement(callout, req):\n",
    "                found.append({\n",
    "                    'status': 'FOUND',\n",
    "                    'requirement': req,\n",
    "                    'evidence': callout,\n",
    "                    'note': f\"Matched: {req.get('canonical', req.get('type'))}\"\n",
    "                })\n",
    "                matched_callouts.add(ci)\n",
    "                matched_requirements.add(ri)\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            missing.append({\n",
    "                'status': 'MISSING',\n",
    "                'requirement': req,\n",
    "                'evidence': None,\n",
    "                'note': f\"Not found in drawing: {req.get('canonical', req.get('type'))}\"\n",
    "            })\n",
    "\n",
    "    # Extra callouts not matched to any requirement\n",
    "    extra = []\n",
    "    for ci, callout in enumerate(callouts):\n",
    "        if ci not in matched_callouts:\n",
    "            extra.append({\n",
    "                'status': 'EXTRA',\n",
    "                'requirement': None,\n",
    "                'evidence': callout,\n",
    "                'note': f\"In drawing but not in SW: {callout.get('raw', callout.get('calloutType'))}\"\n",
    "            })\n",
    "\n",
    "    diff_result = {\n",
    "        'partNumber': evidence.get('partNumber'),\n",
    "        'generatedAt': datetime.now().isoformat() + 'Z',\n",
    "        'summary': {\n",
    "            'totalRequirements': len(requirements),\n",
    "            'found': len(found),\n",
    "            'missing': len(missing),\n",
    "            'extra': len(extra),\n",
    "            'matchRate': f\"{len(found)/len(requirements)*100:.1f}%\" if requirements else \"N/A\"\n",
    "        },\n",
    "        'details': {\n",
    "            'found': found,\n",
    "            'missing': missing,\n",
    "            'extra': extra\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return diff_result\n",
    "\n",
    "# Load SW data and generate diff\n",
    "if part_identity.swJsonPath:\n",
    "    sw_data, err = load_json_robust(part_identity.swJsonPath)\n",
    "    if sw_data:\n",
    "        # Show what we're extracting\n",
    "        requirements = extract_sw_requirements(sw_data)\n",
    "        print(\"=\"*50)\n",
    "        print(\"SW REQUIREMENTS EXTRACTED\")\n",
    "        print(\"=\"*50)\n",
    "        for req in requirements:\n",
    "            print(f\"  {req['type']}: {req.get('canonical', req.get('thread', {}).get('callout', ''))}\")\n",
    "\n",
    "        diff_result = generate_diff_result(evidence, sw_data)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DIFF RESULT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Part: {diff_result['partNumber']}\")\n",
    "        print(f\"  Total Requirements: {diff_result['summary']['totalRequirements']}\")\n",
    "        print(f\"  FOUND:   {diff_result['summary']['found']}\")\n",
    "        print(f\"  MISSING: {diff_result['summary']['missing']}\")\n",
    "        print(f\"  EXTRA:   {diff_result['summary']['extra']}\")\n",
    "        print(f\"  Match Rate: {diff_result['summary']['matchRate']}\")\n",
    "\n",
    "        if diff_result['details']['found']:\n",
    "            print(\"\\nMatched:\")\n",
    "            for item in diff_result['details']['found']:\n",
    "                print(f\"  \u2713 {item['note']}\")\n",
    "\n",
    "        if diff_result['details']['missing']:\n",
    "            print(\"\\nMissing from drawing:\")\n",
    "            for item in diff_result['details']['missing']:\n",
    "                print(f\"  \u2717 {item['note']}\")\n",
    "\n",
    "        if diff_result['details']['extra']:\n",
    "            print(\"\\nExtra in drawing:\")\n",
    "            for item in diff_result['details']['extra']:\n",
    "                print(f\"  ? {item['note']}\")\n",
    "\n",
    "        # Save\n",
    "        diff_out = os.path.join(OUTPUT_DIR, \"DiffResult.json\")\n",
    "        with open(diff_out, 'w') as f:\n",
    "            json.dump(diff_result, f, indent=2)\n",
    "        print(f\"\\nSaved: {diff_out}\")\n",
    "    else:\n",
    "        print(f\"Error loading SW JSON: {err}\")\n",
    "else:\n",
    "    print(\"No SW JSON path - cannot generate diff\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZuYl-gdQBHV",
    "outputId": "5188d064-ae6e-459d-ed99-3cd4f0234e6e"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}