{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/tests/notebooks/test_ocr_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkgBGfgioVvu"
      },
      "source": [
        "# Test OCR Pipeline: Rotation (M3) + OCR Adapter (M4) + Crop Reader (M5)\n",
        "\n",
        "This notebook tests the OCR stages of the AI Inspector pipeline:\n",
        "\n",
        "- **M3 -- Rotation Selector**: Tries 4 rotations (0, 90, 180, 270) of each crop,\n",
        "  runs OCR on each, and picks the rotation with the best text quality score.\n",
        "- **M4 -- OCR Adapter**: Wraps LightOnOCR-2-1B with confidence estimation and\n",
        "  canonicalization. Exposes a `read_simple(image) -> (text, confidence)` interface.\n",
        "- **M5 -- Crop Reader**: Full OCR-to-parse pipeline per crop: OCR -> canonicalize\n",
        "  -> regex parse by YOLO class -> optional VLM fallback.\n",
        "\n",
        "**Runtime requirement:** GPU with at least 8 GB VRAM (A100 preferred).\n",
        "\n",
        "**Note:** LightOnOCR-2-1B is a gated model on HuggingFace. You need an HF_TOKEN\n",
        "with access granted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV6F-XiEoVvv"
      },
      "source": [
        "# Cell 1: Install dependencies\n",
        "# NOTE: Set your runtime to GPU before running (Runtime > Change runtime type > A100)\n",
        "%pip install transformers torch pillow accelerate matplotlib --quiet\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git /content/AI-Drawing-Inspector 2>/dev/null || \\\n",
        "    (cd /content/AI-Drawing-Inspector && git pull)\n",
        "\n",
        "print('Dependencies installed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Q8fzRdoVvw"
      },
      "source": [
        "# Cell 2: Set paths, set HF_TOKEN (Drive mount skipped)\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add repo to Python path\n",
        "sys.path.insert(0, '/content/AI-Drawing-Inspector')\n",
        "\n",
        "# ---- Environment detection ----\n",
        "try:\n",
        "    from google.colab import files, userdata\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    files = None\n",
        "    userdata = None\n",
        "\n",
        "print(\"Skipping Google Drive mount as requested.\")\n",
        "\n",
        "# ---- HuggingFace token (required for LightOnOCR-2) ----\n",
        "HF_TOKEN = None\n",
        "if IN_COLAB and userdata is not None:\n",
        "    try:\n",
        "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not HF_TOKEN and IN_COLAB:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        entered = getpass('Enter HF_TOKEN (leave blank to skip): ').strip()\n",
        "        HF_TOKEN = entered or None\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if HF_TOKEN:\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(f'HF_TOKEN set (length={len(HF_TOKEN)})')\n",
        "else:\n",
        "    print('WARNING: HF_TOKEN not set. OCR model loading will fail until token is provided.')\n",
        "\n",
        "# ---- Paths ----\n",
        "# Search for crops in local directories\n",
        "candidate_dirs = [\n",
        "    '/content/debug/crops',\n",
        "    '/content',\n",
        "]\n",
        "\n",
        "CROPS_DIR = next((d for d in candidate_dirs if os.path.isdir(d)), '/content')\n",
        "print(f'Crops directory: {CROPS_DIR}')\n",
        "\n",
        "# Reminder for manual upload\n",
        "if IN_COLAB:\n",
        "    print('Ready for manual file uploads.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjRUdf-UoVvw"
      },
      "source": [
        "# Cell 3: Import modules\n",
        "from ai_inspector.extractors.ocr_adapter import OCRAdapter, MockOCRAdapter\n",
        "from ai_inspector.extractors.rotation import (\n",
        "    select_best_rotation, _compute_text_quality, ROTATIONS\n",
        ")\n",
        "from ai_inspector.extractors.crop_reader import read_crop, read_crops_batch\n",
        "from ai_inspector.extractors.canonicalize import canonicalize\n",
        "from ai_inspector.contracts import OCRResult, RotationResult, ReaderResult\n",
        "from PIL import Image\n",
        "\n",
        "print('All OCR pipeline modules imported successfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIpypV25oVvw"
      },
      "source": [
        "# Cell 4: Load OCR model (LightOnOCR-2-1B)\n",
        "import torch\n",
        "\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise RuntimeError('HF_TOKEN is required to load LightOnOCR-2. Set token in Cell 2.')\n",
        "\n",
        "adapter = OCRAdapter(hf_token=HF_TOKEN)\n",
        "adapter.load()\n",
        "print(f'OCR adapter loaded: {adapter.is_loaded}')\n",
        "\n",
        "# Quick smoke test with a blank image\n",
        "blank = Image.new('RGB', (200, 50), 'white')\n",
        "result = adapter.read(blank)\n",
        "print(f'Smoke test -- text: \"{result.text}\", confidence: {result.confidence:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIcs5lsAoVvw"
      },
      "source": [
        "# Cell 5: Test rotation selection on sample crops\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "# Load sample crop images (PNG files from crops dir or uploaded files)\n",
        "crop_files = sorted(glob.glob(f'{CROPS_DIR}/*.png'))[:3]\n",
        "\n",
        "# Include uploaded PNGs from /content when not already in CROPS_DIR\n",
        "extra_uploads = [p for p in sorted(glob.glob('/content/*.png')) if p not in crop_files]\n",
        "for p in extra_uploads:\n",
        "    if len(crop_files) < 3:\n",
        "        crop_files.append(p)\n",
        "\n",
        "if not crop_files:\n",
        "    print('No crop files found. Creating synthetic test crops...')\n",
        "    # Create a synthetic crop with text-like content\n",
        "    from PIL import ImageDraw, ImageFont\n",
        "    for text, fname in [\n",
        "        ('\\u2300.500 THRU', 'synth_hole.png'),\n",
        "        ('R.125', 'synth_fillet.png'),\n",
        "        ('M10x1.5', 'synth_thread.png'),\n",
        "    ]:\n",
        "        img = Image.new('RGB', (300, 80), 'white')\n",
        "        draw = ImageDraw.Draw(img)\n",
        "        draw.text((10, 20), text, fill='black')\n",
        "        path = f'/content/{fname}'\n",
        "        img.save(path)\n",
        "        crop_files.append(path)\n",
        "\n",
        "# Test rotation selection on each crop\n",
        "for crop_path in crop_files:\n",
        "    crop_img = Image.open(crop_path).convert('RGB')\n",
        "    print(f'\\n--- {os.path.basename(crop_path)} ({crop_img.size}) ---')\n",
        "\n",
        "    # Show all 4 rotations with their scores\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
        "    for idx, angle in enumerate(ROTATIONS):\n",
        "        if angle == 0:\n",
        "            rotated = crop_img\n",
        "        else:\n",
        "            rotated = crop_img.rotate(-angle, resample=Image.BICUBIC, expand=True)\n",
        "\n",
        "        text, conf = adapter.read_simple(rotated)\n",
        "        quality = _compute_text_quality(text)\n",
        "\n",
        "        axes[idx].imshow(rotated)\n",
        "        axes[idx].set_title(f'{angle}deg\\nq={quality:.1f} c={conf:.2f}\\n\"{text[:30]}\"', fontsize=8)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(os.path.basename(crop_path), fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Run official rotation selector\n",
        "    rot_result = select_best_rotation(crop_img, adapter.read_simple, yolo_class='Hole')\n",
        "    print(f'  Best rotation: {rot_result.rotation_used}deg')\n",
        "    print(f'  Quality score: {rot_result.quality_score:.2f}')\n",
        "    print(f'  Raw text: \"{rot_result.raw}\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imNssOBFoVvw"
      },
      "source": [
        "# Cell 6: Test canonicalization on sample strings\n",
        "test_strings = [\n",
        "    ('\\u00d8.500 THRU', 'Diameter variant Oslash'),\n",
        "    ('\\u2205.250 DEEP .750', 'Diameter variant empty set'),\n",
        "    ('2 X \\u2300.375', 'Quantity with spaces'),\n",
        "    ('R.125 TYP.', 'Fillet radius'),\n",
        "    ('M10\\u00d71.5', 'Metric thread with multiplication sign'),\n",
        "    ('.045 \\u00d7 45\\u00ba', 'Chamfer with degree variant'),\n",
        "    ('+/- .005', 'Plus-minus tolerance'),\n",
        "    ('3/8\\u201316 UNC', 'Thread with en-dash'),\n",
        "    ('  2X   \\u2300.500   THRU  ', 'Extra whitespace'),\n",
        "]\n",
        "\n",
        "print(f'{\"Input\":40s} | {\"Canonicalized\":40s} | Description')\n",
        "print('-' * 110)\n",
        "for raw, desc in test_strings:\n",
        "    canon = canonicalize(raw)\n",
        "    print(f'{repr(raw):40s} | {repr(canon):40s} | {desc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdpvGKUToVvw"
      },
      "source": [
        "# Cell 7: Test crop_reader.read_crop() -- full OCR->parse pipeline on crops\n",
        "from ai_inspector.detection.classes import CLASS_TO_CALLOUT_TYPE\n",
        "\n",
        "# Define test cases: (crop_image_or_path, yolo_class)\n",
        "test_cases = []\n",
        "for crop_path in crop_files:\n",
        "    # Infer class from filename or default to Hole\n",
        "    name = os.path.basename(crop_path).lower()\n",
        "    if 'fillet' in name:\n",
        "        cls = 'Fillet'\n",
        "    elif 'thread' in name or 'tapped' in name:\n",
        "        cls = 'TappedHole'\n",
        "    elif 'chamfer' in name:\n",
        "        cls = 'Chamfer'\n",
        "    else:\n",
        "        cls = 'Hole'\n",
        "    test_cases.append((crop_path, cls))\n",
        "\n",
        "results = []\n",
        "for crop_path, yolo_class in test_cases:\n",
        "    crop_img = Image.open(crop_path).convert('RGB')\n",
        "\n",
        "    # First run rotation selection to get best OCR text\n",
        "    rot = select_best_rotation(crop_img, adapter.read_simple, yolo_class=yolo_class)\n",
        "\n",
        "    # Then run crop_reader with pre-OCR text from rotation stage\n",
        "    pre_ocr = None\n",
        "    if rot.ocr_result:\n",
        "        pre_ocr = (rot.ocr_result.text, rot.ocr_result.confidence)\n",
        "\n",
        "    reader_result = read_crop(\n",
        "        image=crop_img,\n",
        "        ocr_fn=adapter.read_simple,\n",
        "        yolo_class=yolo_class,\n",
        "        pre_ocr=pre_ocr,\n",
        "    )\n",
        "    results.append((os.path.basename(crop_path), yolo_class, reader_result))\n",
        "\n",
        "    print(f'\\n--- {os.path.basename(crop_path)} (class={yolo_class}) ---')\n",
        "    print(f'  Callout type: {reader_result.callout_type}')\n",
        "    print(f'  Raw text:     \"{reader_result.raw}\"')\n",
        "    print(f'  Parse source: {reader_result.source}')\n",
        "    print(f'  OCR conf:     {reader_result.ocr_confidence:.2f}')\n",
        "    print(f'  Parsed fields: {reader_result.parsed}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoF5lt70oVvw"
      },
      "source": [
        "# Cell 8: Print OCR results table\n",
        "print(f'\\n{\"File\":25s} {\"Class\":15s} {\"Raw Text\":30s} {\"Parsed Type\":15s} {\"Source\":8s} {\"Conf\":>6s}')\n",
        "print('=' * 110)\n",
        "\n",
        "for fname, yolo_class, rr in results:\n",
        "    raw_display = rr.raw[:28] + '..' if len(rr.raw) > 30 else rr.raw\n",
        "    print(\n",
        "        f'{fname:25s} '\n",
        "        f'{yolo_class:15s} '\n",
        "        f'{raw_display:30s} '\n",
        "        f'{rr.callout_type:15s} '\n",
        "        f'{rr.source:8s} '\n",
        "        f'{rr.ocr_confidence:6.2f}'\n",
        "    )\n",
        "\n",
        "# Parsed fields detail\n",
        "print('\\n--- Parsed Fields Detail ---')\n",
        "for fname, yolo_class, rr in results:\n",
        "    print(f'\\n{fname}:')\n",
        "    if rr.parsed:\n",
        "        for k, v in rr.parsed.items():\n",
        "            if not k.startswith('_'):\n",
        "                print(f'  {k}: {v}')\n",
        "    else:\n",
        "        print('  (no parsed fields)')\n",
        "\n",
        "# Cleanup\n",
        "adapter.unload()\n",
        "print('\\nOCR adapter unloaded.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}