{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# YOLO11-OBB Finetuning for Engineering Drawing Callouts (v2)\n\nTrain a YOLO11s-OBB model to detect 4 callout types:\n- **Hole** (idx 0) — diameter callouts\n- **TappedHole** (idx 1) — thread callouts\n- **Fillet** (idx 4) — radius callouts\n- **Chamfer** (idx 5) — chamfer callouts\n\n**Monitoring:** TensorBoard (run `%tensorboard --logdir runs` in a cell)\n**Model saving:** Hugging Face Hub\n\n**Requirements:** A100 GPU, Roboflow API key, HuggingFace token"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q ultralytics roboflow huggingface_hub\n\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    props = torch.cuda.get_device_properties(0)\n    mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n    print(f\"Memory: {mem / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU detected. Training will be very slow.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repo & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_DIR = \"/content/AI-Drawing-Inspector\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from roboflow import Roboflow\n\n# --- Roboflow credentials ---\nROBOFLOW_API_KEY = \"IHyhfpN5KngIAXiN5MdM\"\nROBOFLOW_WORKSPACE = \"ai-drawing-inspector\"\nROBOFLOW_PROJECT = \"ai-inspector-callout-detection\"\nROBOFLOW_VERSION = 3  # Updated to version 3 with expanded dataset\n# -----------------------------\n\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\nproject = rf.workspace(ROBOFLOW_WORKSPACE).project(ROBOFLOW_PROJECT)\ndataset = project.version(ROBOFLOW_VERSION).download(\"yolov8-obb\", location=\"/content/dataset\")\n\nDATASET_ROOT = \"/content/dataset\"\nprint(f\"Dataset downloaded to: {DATASET_ROOT}\")\nprint(f\"Train images: {len(os.listdir(os.path.join(DATASET_ROOT, 'train', 'images')))}\")\n\n# Check if valid split exists\nvalid_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\nif os.path.isdir(valid_img_dir) and len(os.listdir(valid_img_dir)) > 0:\n    print(f\"Val images:   {len(os.listdir(valid_img_dir))}\")\nelse:\n    print(\"No valid split from Roboflow — will create one next.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 3b: Create Train/Valid Split (if needed)\n\nRoboflow may export all images into train/. This cell splits 20% into valid/,\nensuring all 4 classes are represented in the validation set.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import shutil, random\nfrom collections import defaultdict\n\ntrain_img_dir = os.path.join(DATASET_ROOT, \"train\", \"images\")\ntrain_lbl_dir = os.path.join(DATASET_ROOT, \"train\", \"labels\")\nvalid_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\nvalid_lbl_dir = os.path.join(DATASET_ROOT, \"valid\", \"labels\")\n\n# Only split if valid is empty or missing\nneeds_split = True\nif os.path.isdir(valid_img_dir):\n    valid_files = [f for f in os.listdir(valid_img_dir) if f.endswith(('.jpg','.png','.jpeg'))]\n    if len(valid_files) > 0:\n        needs_split = False\n        print(f\"Valid split already exists ({len(valid_files)} images). Skipping.\")\n\nif needs_split:\n    os.makedirs(valid_img_dir, exist_ok=True)\n    os.makedirs(valid_lbl_dir, exist_ok=True)\n\n    # Roboflow alphabetical: 0=Chamfer, 1=Fillet, 2=Hole, 3=TappedHole\n    RF_CLASS_MAP = {0: \"Chamfer\", 1: \"Fillet\", 2: \"Hole\", 3: \"TappedHole\"}\n\n    # Parse classes per image\n    image_classes = {}\n    for img in sorted(os.listdir(train_img_dir)):\n        if not img.endswith(('.jpg', '.png', '.jpeg')):\n            continue\n        base = os.path.splitext(img)[0]\n        lbl_path = os.path.join(train_lbl_dir, base + '.txt')\n        classes = set()\n        if os.path.exists(lbl_path):\n            with open(lbl_path) as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) >= 9:\n                        classes.add(RF_CLASS_MAP.get(int(parts[0]), 'unk'))\n        image_classes[img] = classes\n\n    random.seed(42)\n    val_set = set()\n\n    # Ensure each class is represented in valid\n    for target_cls in [\"Chamfer\", \"TappedHole\", \"Fillet\", \"Hole\"]:\n        candidates = [img for img, cls in image_classes.items()\n                      if target_cls in cls and img not in val_set]\n        random.shuffle(candidates)\n        for img in candidates[:3]:\n            val_set.add(img)\n\n    # Fill to ~20% total\n    target_val = max(16, int(len(image_classes) * 0.2))\n    remaining = [img for img in image_classes if img not in val_set]\n    random.shuffle(remaining)\n    while len(val_set) < target_val and remaining:\n        val_set.add(remaining.pop())\n\n    # Move files\n    for img in sorted(val_set):\n        base = os.path.splitext(img)[0]\n        shutil.move(os.path.join(train_img_dir, img), os.path.join(valid_img_dir, img))\n        lbl_file = base + '.txt'\n        lbl_src = os.path.join(train_lbl_dir, lbl_file)\n        if os.path.exists(lbl_src):\n            shutil.move(lbl_src, os.path.join(valid_lbl_dir, lbl_file))\n\n    train_count = len([f for f in os.listdir(train_img_dir) if f.endswith(('.jpg','.png'))])\n    valid_count = len([f for f in os.listdir(valid_img_dir) if f.endswith(('.jpg','.png'))])\n    print(f\"Split complete: {train_count} train / {valid_count} valid\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Remap Class Indices\n",
    "\n",
    "Roboflow exports classes alphabetically (Chamfer=0, Fillet=1, Hole=2, TappedHole=3).\n",
    "Our pipeline uses classes.py indices (Hole=0, TappedHole=1, Fillet=4, Chamfer=5).\n",
    "This cell remaps all label files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "from ai_inspector.fine_tuning.data_generator import (\n",
    "    remap_labels,\n",
    "    ROBOFLOW_TO_CLASSES_PY,\n",
    ")\n",
    "\n",
    "print(\"Remapping class indices:\")\n",
    "print(f\"  Roboflow -> classes.py: {ROBOFLOW_TO_CLASSES_PY}\")\n",
    "print()\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    label_dir = os.path.join(DATASET_ROOT, split, \"labels\")\n",
    "    if os.path.isdir(label_dir):\n",
    "        result = remap_labels(label_dir, ROBOFLOW_TO_CLASSES_PY)\n",
    "        print(f\"  {split}: {result['files']} files, {result['annotations']} annotations\")\n",
    "    else:\n",
    "        print(f\"  {split}: no labels directory\")\n",
    "\n",
    "# Verify a sample\n",
    "train_labels = os.path.join(DATASET_ROOT, \"train\", \"labels\")\n",
    "sample_file = sorted(os.listdir(train_labels))[0]\n",
    "print(f\"\\nSample label ({sample_file}):\")\n",
    "with open(os.path.join(train_labels, sample_file)) as f:\n",
    "    for line in f.readlines()[:3]:\n",
    "        cls_idx = int(line.split()[0])\n",
    "        from ai_inspector.detection.classes import IDX_TO_CLASS\n",
    "        cls_name = IDX_TO_CLASS.get(cls_idx, f\"UNKNOWN({cls_idx})\")\n",
    "        print(f\"  class {cls_idx} = {cls_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Generate dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_inspector.fine_tuning.data_generator import generate_dataset_yaml\n",
    "\n",
    "yaml_path = os.path.join(DATASET_ROOT, \"dataset.yaml\")\n",
    "generate_dataset_yaml(yaml_path, DATASET_ROOT)\n",
    "\n",
    "print(f\"Generated: {yaml_path}\")\n",
    "print()\n",
    "with open(yaml_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Train YOLO11s-OBB\n\n**Model:** yolo11s-obb (small) — more capacity than nano for better accuracy\n\n**Monitoring:** TensorBoard — run the cell below to launch, then run training\n\nDrawing-safe augmentation settings:\n- No horizontal/vertical flip (text becomes unreadable)\n- No hue/saturation changes (drawings are monochrome)\n- Minimal rotation (callouts are axis-aligned)\n- Reduced mosaic probability"
  },
  {
   "cell_type": "code",
   "source": "# Launch TensorBoard to monitor training in real-time\n# Run this cell FIRST, then run the training cell below\n%load_ext tensorboard\n%tensorboard --logdir runs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ultralytics import YOLO\nimport os\n\n# --- TensorBoard ---\n# Run this in a separate cell BEFORE training to monitor live:\n#   %load_ext tensorboard\n#   %tensorboard --logdir runs\n# -------------------\n\n# Load pretrained model (small = more capacity than nano)\nmodel = YOLO(\"yolo11s-obb.pt\")\n\n# Train with drawing-safe augmentation\nresults = model.train(\n    data=os.path.join(DATASET_ROOT, \"dataset.yaml\"),\n    epochs=150,\n    batch=32,\n    imgsz=1024,\n    amp=True,\n    device=0,\n    project=os.path.join(os.getcwd(), \"runs\", \"obb\"),\n    name=\"callout_v2\",\n    exist_ok=True,\n    # Drawing-safe augmentation\n    flipud=0.0,        # No vertical flip\n    fliplr=0.0,        # No horizontal flip\n    hsv_h=0.0,         # No hue shift (monochrome)\n    hsv_s=0.0,         # No saturation shift\n    hsv_v=0.1,         # Tiny brightness variation\n    degrees=2.0,       # Minimal rotation (callouts are axis-aligned)\n    mosaic=0.3,        # Reduced mosaic (preserve spatial context)\n    scale=0.3,         # Moderate scale augmentation\n    translate=0.1,     # Small translation\n    # Performance\n    workers=4,\n    patience=30,       # Early stopping\n    save_period=25,    # Save checkpoint every 25 epochs\n    verbose=True,\n)\n\n# Store the actual weights path for subsequent cells\nimport glob\nBEST_WEIGHTS = glob.glob(os.path.join(os.getcwd(), \"runs\", \"obb\", \"**\", \"best.pt\"), recursive=True)[-1]\nprint(f\"\\nTraining complete!\")\nprint(f\"Best weights: {BEST_WEIGHTS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Validate & Print Per-Class mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model (use path found after training)\nbest_model = YOLO(BEST_WEIGHTS)\n\n# Run validation\nval_results = best_model.val(\n    data=os.path.join(DATASET_ROOT, \"dataset.yaml\"),\n    imgsz=1024,\n    batch=32,\n    device=0,\n)\n\n# Print per-class results\nprint(\"\\n\" + \"=\"*50)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"mAP50:     {val_results.box.map50:.4f}\")\nprint(f\"mAP50-95:  {val_results.box.map:.4f}\")\nprint()\n\n# Per-class mAP50\nfrom ai_inspector.detection.classes import IDX_TO_CLASS\nprint(\"Per-class mAP50:\")\nif hasattr(val_results.box, 'ap50') and val_results.box.ap50 is not None:\n    for i, ap in enumerate(val_results.box.ap50):\n        cls_name = IDX_TO_CLASS.get(i, f\"class_{i}\")\n        if ap > 0:\n            print(f\"  {cls_name:20s}: {ap:.4f}\")\nelse:\n    print(\"  (per-class AP not available in this format)\")\n\n# Pass/Fail check\ntarget_map = 0.5\nif val_results.box.map50 >= target_map:\n    print(f\"\\nPASS: mAP50 ({val_results.box.map50:.4f}) >= {target_map}\")\nelse:\n    print(f\"\\nBELOW TARGET: mAP50 ({val_results.box.map50:.4f}) < {target_map}\")\n    print(\"Consider: more annotations, more epochs, or larger model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8: Upload Best Weights to Hugging Face Hub\n\nThis uploads the trained model to your Hugging Face account for easy sharing and versioning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import HfApi, login\nimport shutil\nimport os\n\n# --- Hugging Face credentials ---\n# Option 1: Login interactively (will prompt for token)\nlogin()\n\n# Option 2: Use token directly (uncomment and add your token)\n# login(token=\"hf_YOUR_TOKEN_HERE\")\n# --------------------------------\n\n# Repository settings\nHF_REPO_ID = \"skaumbdoallsaws-coder/ai-inspector-callout-detection\"  # Change to your username/repo\nMODEL_NAME = \"callout_v2_yolo11s-obb\"\n\napi = HfApi()\n\n# Create repo if it doesn't exist\ntry:\n    api.create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n    print(f\"Repository: https://huggingface.co/{HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Repo creation note: {e}\")\n\n# Upload best weights\nprint(f\"\\nUploading {BEST_WEIGHTS}...\")\napi.upload_file(\n    path_or_fileobj=BEST_WEIGHTS,\n    path_in_repo=f\"{MODEL_NAME}_best.pt\",\n    repo_id=HF_REPO_ID,\n    repo_type=\"model\",\n)\nprint(f\"  Uploaded: {MODEL_NAME}_best.pt\")\n\n# Upload last weights as backup\nlast_weights = BEST_WEIGHTS.replace(\"best.pt\", \"last.pt\")\nif os.path.exists(last_weights):\n    api.upload_file(\n        path_or_fileobj=last_weights,\n        path_in_repo=f\"{MODEL_NAME}_last.pt\",\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"  Uploaded: {MODEL_NAME}_last.pt\")\n\n# Upload training results CSV\nrun_dir = os.path.dirname(os.path.dirname(BEST_WEIGHTS))\nresults_csv = os.path.join(run_dir, \"results.csv\")\nif os.path.exists(results_csv):\n    api.upload_file(\n        path_or_fileobj=results_csv,\n        path_in_repo=f\"{MODEL_NAME}_results.csv\",\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"  Uploaded: {MODEL_NAME}_results.csv\")\n\n# Create a simple model card\nmodel_card = f\"\"\"---\nlicense: apache-2.0\ntags:\n  - yolo\n  - object-detection\n  - obb\n  - engineering-drawings\n  - callout-detection\n---\n\n# {MODEL_NAME}\n\nYOLO11s-OBB model finetuned for engineering drawing callout detection.\n\n## Classes\n- **Hole** (idx 0) — diameter callouts\n- **TappedHole** (idx 1) — thread callouts  \n- **Fillet** (idx 4) — radius callouts\n- **Chamfer** (idx 5) — chamfer callouts\n\n## Training\n- Base model: yolo11s-obb.pt\n- Dataset: Roboflow ai-inspector-callout-detection v3\n- Epochs: 150 (with early stopping)\n- Image size: 1024\n- Augmentation: Drawing-safe (no flip, no hue/sat)\n\n## Usage\n```python\nfrom ultralytics import YOLO\nmodel = YOLO(\"hf://skaumbdoallsaws-coder/ai-inspector-callout-detection/{MODEL_NAME}_best.pt\")\nresults = model(\"drawing.png\")\n```\n\"\"\"\n\n# Save and upload model card\ncard_path = \"/content/README.md\"\nwith open(card_path, \"w\") as f:\n    f.write(model_card)\napi.upload_file(\n    path_or_fileobj=card_path,\n    path_in_repo=\"README.md\",\n    repo_id=HF_REPO_ID,\n    repo_type=\"model\",\n)\nprint(f\"  Uploaded: README.md (model card)\")\n\nprint(f\"\\nAll files uploaded to: https://huggingface.co/{HF_REPO_ID}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Pick a validation image\n",
    "val_img_dir = os.path.join(DATASET_ROOT, \"valid\", \"images\")\n",
    "test_images = sorted(os.listdir(val_img_dir))[:3]\n",
    "\n",
    "# Color map for classes\n",
    "CLASS_COLORS = {\n",
    "    0: \"#00FF00\",  # Hole - green\n",
    "    1: \"#FF6600\",  # TappedHole - orange\n",
    "    4: \"#0099FF\",  # Fillet - blue\n",
    "    5: \"#FF00FF\",  # Chamfer - magenta\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, min(3, len(test_images)), figsize=(20, 8))\n",
    "if len(test_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img_name in zip(axes, test_images):\n",
    "    img_path = os.path.join(val_img_dir, img_name)\n",
    "    \n",
    "    # Run inference\n",
    "    results = best_model(img_path, imgsz=1024, conf=0.25)\n",
    "    \n",
    "    # Plot\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(img_name[:30], fontsize=10)\n",
    "    \n",
    "    if results[0].obb is not None:\n",
    "        for obb in results[0].obb:\n",
    "            cls_id = int(obb.cls[0])\n",
    "            conf = float(obb.conf[0])\n",
    "            cls_name = IDX_TO_CLASS.get(cls_id, f\"cls{cls_id}\")\n",
    "            color = CLASS_COLORS.get(cls_id, \"#FFFFFF\")\n",
    "            \n",
    "            # Get OBB corners\n",
    "            if hasattr(obb, 'xyxyxyxy'):\n",
    "                corners = obb.xyxyxyxy[0].cpu().numpy()\n",
    "                polygon = patches.Polygon(\n",
    "                    corners, closed=True,\n",
    "                    linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(polygon)\n",
    "                ax.text(\n",
    "                    corners[0][0], corners[0][1] - 5,\n",
    "                    f\"{cls_name} {conf:.2f}\",\n",
    "                    color=color, fontsize=8,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"black\", alpha=0.7)\n",
    "                )\n",
    "    \n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"inference_preview.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: inference_preview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Integration Test with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ai_inspector.pipeline import YOLOPipeline\nfrom ai_inspector.config import Config\n\n# Use the finetuned model (use BEST_WEIGHTS from training)\nconfig = Config(yolo_model_path=BEST_WEIGHTS)\npipeline = YOLOPipeline(model_path=config.yolo_model_path, config=config)\n\n# Run on a validation image\ntest_img = os.path.join(val_img_dir, test_images[0])\nresult = pipeline.run(test_img)\n\nprint(f\"Image: {test_images[0]}\")\nprint(f\"Detections: {len(result.detections)}\")\nprint(f\"Callouts:   {len(result.callouts)}\")\nprint()\n\nfor i, callout in enumerate(result.callouts[:10]):\n    print(f\"  [{i}] {callout.get('calloutType', '?'):15s} | \"\n          f\"raw: {callout.get('raw_text', '?')[:40]}\")\n\nprint(\"\\nPipeline integration: OK\" if result.callouts else \"\\nNo callouts extracted\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}