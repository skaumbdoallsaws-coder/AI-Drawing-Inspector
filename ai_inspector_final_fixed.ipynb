{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/ai_inspector_final_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFrLGYPswHHZ"
   },
   "source": [
    "# AI Engineering Drawing Inspector (Final Version)\n",
    "\n",
    "A context-aware GD&T checker that uses:\n",
    "- Part context from BOM structure\n",
    "- RAG retrieval from ASME Y14.5 standard\n",
    "- Qwen2-VL for visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTvRp4rwHHa"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmkO97ziwHHa",
    "outputId": "70cea237-7eda-43a1-9957-45e86e37d96c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1A: Install Dependencies\n",
    "# ============================================================\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q qwen-vl-utils\n",
    "!pip install -q pdf2image\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "!pip install -q bitsandbytes\n",
    "!apt-get install -y poppler-utils > /dev/null 2>&1\n",
    "\n",
    "# Production Pipeline Dependencies\n",
    "!pip install -q pymupdf opencv-python-headless\n",
    "\n",
    "# Tesseract OCR (replaces PaddleOCR - more stable)\n",
    "!sudo apt-get install -y tesseract-ocr > /dev/null 2>&1\n",
    "!pip install -q pytesseract\n",
    "\n",
    "print(\"✅ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_VXcblCwHHa",
    "outputId": "7b119c0a-c42a-44ee-b2f9-cd74a716328c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1B: Import Libraries\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Production Pipeline Imports\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Tesseract OCR\n",
    "import pytesseract\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmqDweB4wHHa"
   },
   "source": [
    "## 2. Load Model (Qwen2-VL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "5d52cc773ce1405ca1fa53000a85a99d",
      "2c1d352483974fd1b7266a8a097f4472",
      "aceb4e55e5db4ca69eb957d406e4360c",
      "baf2d05be5d94ac3adc6c653f7751573",
      "ee995792c97e42f4b10f05ac4c1fd96f",
      "a0f0411651d34b5a87a74bade26cb91a",
      "17bbaa41ea7740dba581b8ec970ca46d",
      "3c80ee8f5be447959736b543e896d62c",
      "f7bfd4b8688c432986c11e46d00927c7",
      "705afc59cdbe464d88a5690cf84904f0",
      "bd63cf407a32497d84b53afba91203a9"
     ]
    },
    "id": "8fokt7vLwHHa",
    "outputId": "5da8b678-c1b8-4ae5-8f7d-30a7fcc0de21"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Qwen/Qwen2-VL-72B-Instruct in 4-bit (NF4)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d52cc773ce1405ca1fa53000a85a99d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Qwen2-VL-72B (4-bit) Loaded Successfully!\n",
      "Memory Footprint: 40.45 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Load Qwen2-VL Model (4-bit Quantized)\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-72B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID} in 4-bit (NF4)...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"✅ Qwen2-VL-72B (4-bit) Loaded Successfully!\")\n",
    "print(f\"Memory Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkOM8aOwHHa"
   },
   "source": [
    "## 3. Load Context Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK5ZAuphwHHa",
    "outputId": "68566461-fd5e-42fb-a7bc-570bb6d47447"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "STEP 1: Upload Configuration Files\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FILE STATUS:\n",
      "============================================================\n",
      "File Mapping:  ✅ OK\n",
      "Structure:     ✅ OK\n",
      "RAG Index:     ✅ OK\n",
      "\n",
      "Data directory: /content\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3A: Upload Configuration Files\n",
    "# ============================================================\n",
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
    "STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
    "RAG_INDEX_FILE = \"asme_visual_index.pkl\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Upload Configuration Files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def locate_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return os.path.abspath(filename)\n",
    "    nested_path = os.path.join(\"rag_data\", filename)\n",
    "    if os.path.exists(nested_path):\n",
    "        return os.path.abspath(nested_path)\n",
    "    return None\n",
    "\n",
    "FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
    "STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
    "RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
    "\n",
    "missing_files = []\n",
    "if not FILE_MAPPING_PATH:\n",
    "    missing_files.append(MAPPING_FILE)\n",
    "if not STRUCTURE_PATH:\n",
    "    missing_files.append(STRUCTURE_FILE)\n",
    "if not RAG_INDEX_PATH:\n",
    "    missing_files.append(RAG_INDEX_FILE)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n",
    "    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded:\n",
    "        if filename.lower().endswith('.zip'):\n",
    "            print(f\"\\nExtracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"rag_data\")\n",
    "            print(\"Extraction complete.\")\n",
    "            break\n",
    "\n",
    "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n",
    "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n",
    "    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
    "\n",
    "if FILE_MAPPING_PATH:\n",
    "    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\n",
    "else:\n",
    "    DATA_DIR = \"/content\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILE STATUS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"File Mapping:  {'✅ OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else '❌ MISSING'}\")\n",
    "print(f\"Structure:     {'✅ OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else '❌ MISSING'}\")\n",
    "print(f\"RAG Index:     {'✅ OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else '⚠️ MISSING'}\")\n",
    "print(f\"\\nData directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykrHElKlwHHb",
    "outputId": "09a5c2de-ed14-4182-92d5-5252cc53cd1d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading file mapping...\n",
      "  Loaded 260 file mappings\n",
      "Loading part structure...\n",
      "Building part context database...\n",
      "  Built context for 127 unique parts\n",
      "\n",
      "✅ Context databases loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3B: Load Part Context Databases\n",
    "# ============================================================\n",
    "\n",
    "def normalize_pn(pn):\n",
    "    \"\"\"Normalize part number for lookup.\"\"\"\n",
    "    return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
    "\n",
    "def load_context_databases():\n",
    "    \"\"\"Load and build all context databases.\"\"\"\n",
    "    print(\"Loading file mapping...\")\n",
    "    with open(FILE_MAPPING_PATH, 'r') as f:\n",
    "        file_mapping_list = json.load(f)\n",
    "\n",
    "    filename_to_pn = {}\n",
    "    for entry in file_mapping_list:\n",
    "        filename = entry['file']\n",
    "        pn = entry['pn']\n",
    "        if pn:\n",
    "            filename_to_pn[filename] = pn\n",
    "            filename_to_pn[filename + '.pdf'] = pn\n",
    "            filename_to_pn[filename + '.PDF'] = pn\n",
    "\n",
    "    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n",
    "\n",
    "    print(\"Loading part structure...\")\n",
    "    with open(STRUCTURE_PATH, 'r') as f:\n",
    "        structure_data = json.load(f)\n",
    "\n",
    "    print(\"Building part context database...\")\n",
    "    part_context_db = {}\n",
    "\n",
    "    for assembly_name, parts_list in structure_data.items():\n",
    "        for part in parts_list:\n",
    "            pn = part['pn']\n",
    "            desc = part['desc']\n",
    "\n",
    "            siblings_list = []\n",
    "            siblings_pns = []\n",
    "\n",
    "            for p_sibling in parts_list:\n",
    "                if p_sibling['pn'] != pn:\n",
    "                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n",
    "                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n",
    "                    siblings_pns.append(p_sibling['pn'])\n",
    "\n",
    "            siblings_str = \"; \".join(siblings_list[:12])\n",
    "            if len(siblings_list) > 12:\n",
    "                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n",
    "\n",
    "            lookup_key = normalize_pn(pn)\n",
    "\n",
    "            part_context_db[lookup_key] = {\n",
    "                'pn': pn,\n",
    "                'description': desc,\n",
    "                'assembly': assembly_name,\n",
    "                'siblings': siblings_str,\n",
    "                'siblings_list': siblings_pns\n",
    "            }\n",
    "            part_context_db[pn] = part_context_db[lookup_key]\n",
    "\n",
    "    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n",
    "    return filename_to_pn, part_context_db\n",
    "\n",
    "filename_to_pn, part_context_db = load_context_databases()\n",
    "print(\"\\n✅ Context databases loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fecfcc5a",
    "outputId": "a84ffd17-6387-4c8e-cf45-fa1aa88171c1"
   },
   "source": [
    "# ============================================================\n",
    "# CELL 3C-PREP: Verify Tesseract Installation\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "# Check if Tesseract is installed\n",
    "tesseract_path = shutil.which(\"tesseract\")\n",
    "if tesseract_path:\n",
    "    print(f\"✅ Tesseract found: {tesseract_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Tesseract not found. Installing...\")\n",
    "    !sudo apt-get install -y tesseract-ocr\n",
    "    print(\"✅ Tesseract installed!\")"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Tesseract found: /usr/bin/tesseract\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d605a04",
    "outputId": "e20bf7c6-5b8a-4df6-8e0e-aa847d0d19c1"
   },
   "source": [
    "# ============================================================\n",
    "# CELL 3C: Initialize Tesseract OCR\n",
    "# ============================================================\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "print(\"Loading Tesseract OCR Engine...\")\n",
    "\n",
    "def get_drawing_text_ocr(image_input):\n",
    "    \"\"\"\n",
    "    Runs Tesseract OCR on the drawing and returns a clean list of found text.\n",
    "    Uses PSM 11 (Sparse Text) mode which is best for engineering drawings.\n",
    "\n",
    "    Args:\n",
    "        image_input: PIL Image or numpy array\n",
    "\n",
    "    Returns:\n",
    "        List of unique text strings found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert numpy array to PIL Image if needed\n",
    "        if isinstance(image_input, np.ndarray):\n",
    "            img = Image.fromarray(image_input)\n",
    "        else:\n",
    "            img = image_input\n",
    "\n",
    "        # Run Tesseract with sparse text mode (PSM 11)\n",
    "        # PSM 11: Sparse text - Find as much text as possible in no particular order\n",
    "        raw_text = pytesseract.image_to_string(img, config='--psm 11')\n",
    "\n",
    "        # Split by newlines and clean\n",
    "        lines = raw_text.split('\\n')\n",
    "\n",
    "        text_set = set()\n",
    "        for line in lines:\n",
    "            # Strip whitespace\n",
    "            cleaned = line.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not cleaned:\n",
    "                continue\n",
    "\n",
    "            # Skip garbage (less than 2 alphanumeric characters)\n",
    "            alphanumeric_count = sum(1 for c in cleaned if c.isalnum())\n",
    "            if alphanumeric_count < 2:\n",
    "                continue\n",
    "\n",
    "            # Normalize engineering symbols\n",
    "            cleaned = cleaned.replace(\"Ø\", \"DIA \")\n",
    "            cleaned = cleaned.replace(\"ø\", \"DIA \")\n",
    "\n",
    "            text_set.add(cleaned)\n",
    "\n",
    "        return sorted(list(text_set))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ OCR Warning: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ Tesseract OCR Engine Ready!\")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Tesseract OCR Engine...\n",
      "✅ Tesseract OCR Engine Ready!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "AsY8eejBwHHb",
    "outputId": "2507d4a5-00df-4230-b77a-276165f5625d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "RAG SYSTEM SETUP\n",
      "============================================================\n",
      "\n",
      "[STEP 1/3] Loading CLIP model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ✅ CLIP model loaded!\n",
      "\n",
      "[STEP 2/3] Loading RAG Index...\n",
      "  ✅ Found: /content/asme_visual_index.pkl\n",
      "\n",
      "[STEP 3/3] Looking for RAG Visual Database...\n",
      "  ❌ No images found. Please upload rag_visual_db.zip:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-93f1574a-10fe-4fd6-aa07-c5f9a0f2021a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-93f1574a-10fe-4fd6-aa07-c5f9a0f2021a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving rag_visual_db.zip to rag_visual_db (2).zip\n",
      "  ✅ Extracted to /content/rag_visual_db\n",
      "\n",
      "============================================================\n",
      "✅ RAG SYSTEM: READY\n",
      "  Index: 294 entries\n",
      "  Images: /content/rag_visual_db\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3E: Load RAG Index & Visual Database\n",
    "# ============================================================\n",
    "import os\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "rag_data = []\n",
    "rag_embeddings = None\n",
    "rag_available = False\n",
    "RAG_IMAGE_DIR = None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RAG SYSTEM SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[STEP 1/3] Loading CLIP model...\")\n",
    "search_model = SentenceTransformer('clip-ViT-B-32')\n",
    "print(\"  ✅ CLIP model loaded!\")\n",
    "\n",
    "print(\"\\n[STEP 2/3] Loading RAG Index...\")\n",
    "index_loaded = False\n",
    "\n",
    "# Check multiple locations for the index file\n",
    "index_locations = [\n",
    "    \"/content/asme_visual_index.pkl\",\n",
    "    \"/content/rag_data/asme_visual_index.pkl\",\n",
    "    \"asme_visual_index.pkl\",\n",
    "]\n",
    "if 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH:\n",
    "    index_locations.insert(0, RAG_INDEX_PATH)\n",
    "\n",
    "for idx_path in index_locations:\n",
    "    if idx_path and os.path.exists(idx_path):\n",
    "        print(f\"  ✅ Found: {idx_path}\")\n",
    "        with open(idx_path, 'rb') as f:\n",
    "            rag_data = pickle.load(f)\n",
    "        RAG_INDEX_PATH = idx_path\n",
    "        index_loaded = True\n",
    "        break\n",
    "\n",
    "if not index_loaded:\n",
    "    print(\"  ❌ No index found. Please upload asme_visual_index.pkl:\")\n",
    "    from google.colab import files\n",
    "    try:\n",
    "        uploaded = files.upload()\n",
    "        for filename in uploaded:\n",
    "            if filename.endswith('.pkl'):\n",
    "                with open(filename, 'rb') as f:\n",
    "                    rag_data = pickle.load(f)\n",
    "                index_loaded = True\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n[STEP 3/3] Looking for RAG Visual Database...\")\n",
    "\n",
    "# Check multiple locations for the image folder\n",
    "image_locations = [\n",
    "    \"/content/rag_visual_db\",\n",
    "    \"/content/rag_data/rag_visual_db\",\n",
    "    \"rag_visual_db\",\n",
    "]\n",
    "if 'DATA_DIR' in dir() and DATA_DIR:\n",
    "    image_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n",
    "\n",
    "found_images = False\n",
    "for loc in image_locations:\n",
    "    if loc and os.path.exists(loc) and os.path.isdir(loc):\n",
    "        # Count images\n",
    "        img_files = [f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if len(img_files) > 0:\n",
    "            RAG_IMAGE_DIR = os.path.abspath(loc)\n",
    "            found_images = True\n",
    "            print(f\"  ✅ Found: {RAG_IMAGE_DIR} ({len(img_files)} images)\")\n",
    "            break\n",
    "\n",
    "if not found_images:\n",
    "    print(\"  ❌ No images found. Please upload rag_visual_db.zip:\")\n",
    "    from google.colab import files\n",
    "    import zipfile, shutil\n",
    "    try:\n",
    "        uploaded = files.upload()\n",
    "        for filename in uploaded:\n",
    "            if filename.lower().endswith('.zip'):\n",
    "                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
    "                if os.path.exists(RAG_IMAGE_DIR):\n",
    "                    shutil.rmtree(RAG_IMAGE_DIR)\n",
    "                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n",
    "                with zipfile.ZipFile(filename, 'r') as zf:\n",
    "                    zf.extractall(RAG_IMAGE_DIR)\n",
    "                found_images = True\n",
    "                print(f\"  ✅ Extracted to {RAG_IMAGE_DIR}\")\n",
    "                break\n",
    "    except:\n",
    "        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
    "\n",
    "# Build search index\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if index_loaded and len(rag_data) > 0:\n",
    "    embeddings_list = [item['embedding'] for item in rag_data]\n",
    "    rag_embeddings = np.array(embeddings_list).astype('float32')\n",
    "    rag_available = True\n",
    "    print(\"✅ RAG SYSTEM: READY\")\n",
    "    print(f\"  Index: {len(rag_data)} entries\")\n",
    "    print(f\"  Images: {RAG_IMAGE_DIR}\")\n",
    "else:\n",
    "    print(\"❌ RAG SYSTEM: NOT READY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD4nCWywHHb"
   },
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4Y7DliIwHHb",
    "outputId": "0ca1bc36-ba5a-4034-94aa-44de1f1eb38e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Core helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4A: Core Helper Functions\n",
    "# ============================================================\n",
    "import os\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_filename_key(filepath):\n",
    "    \"\"\"Extract filename key for lookup.\"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)  # Remove (1), (2) etc\n",
    "    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n",
    "    return name_cleaned.strip()\n",
    "\n",
    "def get_part_context(filepath):\n",
    "    \"\"\"Look up part context from filename.\"\"\"\n",
    "    filename_key = extract_filename_key(filepath)\n",
    "\n",
    "    if filename_key in filename_to_pn:\n",
    "        pn = filename_to_pn[filename_key]\n",
    "        lookup_key = normalize_pn(pn)\n",
    "        if lookup_key in part_context_db:\n",
    "            return pn, part_context_db[lookup_key]\n",
    "\n",
    "    for ext in ['.pdf', '.PDF']:\n",
    "        key = filename_key + ext\n",
    "        if key in filename_to_pn:\n",
    "            pn = filename_to_pn[key]\n",
    "            lookup_key = normalize_pn(pn)\n",
    "            if lookup_key in part_context_db:\n",
    "                return pn, part_context_db[lookup_key]\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def build_context_string(pn, context):\n",
    "    \"\"\"Build the context string for inspection prompt.\"\"\"\n",
    "    if context is None:\n",
    "        return \"CONTEXT: Unknown Part (General Syntax Check Only).\"\n",
    "\n",
    "    desc = context.get('description', 'Unknown')\n",
    "    assembly = context.get('assembly', 'Unknown Assembly')\n",
    "    siblings = context.get('siblings', 'None listed')\n",
    "\n",
    "    return f\"\"\"CONTEXT: This is Part {pn} ({desc}).\n",
    "It belongs to the {assembly}.\n",
    "It must assemble with these mating parts: {siblings}.\n",
    "CRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n",
    "\n",
    "def pdf_to_image(pdf_path, dpi=150):\n",
    "    \"\"\"Convert first page of PDF to PIL Image.\"\"\"\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n",
    "    return pages[0] if pages else None\n",
    "\n",
    "print(\"✅ Core helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3aLheGBwHHb",
    "outputId": "5791defc-8cf0-43d0-c61b-65af8a48aec6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Model query function defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4B: Model Query Function\n",
    "# ============================================================\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def query_model(messages, max_tokens=1024):\n",
    "    \"\"\"Send a query to Qwen2-VL and get response.\"\"\"\n",
    "    if 'model' not in globals() or 'processor' not in globals():\n",
    "        raise RuntimeError(\"⚠️ Model not loaded. Run Cell 2 first.\")\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "\n",
    "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "    response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response.strip()\n",
    "\n",
    "print(\"✅ Model query function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gETA6XD5wHHb",
    "outputId": "a8762ca9-674a-442f-e677-ae41b75aa637"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ RAG retrieval function defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4C: RAG Retrieval Function\n",
    "# ============================================================\n",
    "\n",
    "def retrieve_asme_pages(keywords, top_k=2):\n",
    "    \"\"\"Retrieve relevant ASME standard pages based on keywords.\"\"\"\n",
    "    global RAG_IMAGE_DIR\n",
    "\n",
    "    if not rag_available or rag_embeddings is None:\n",
    "        print(\"  ⚠️ RAG system not available\")\n",
    "        return []\n",
    "\n",
    "    if RAG_IMAGE_DIR is None:\n",
    "        print(\"  ⚠️ RAG_IMAGE_DIR not set\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = search_model.encode([keywords])\n",
    "        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "\n",
    "        retrieved_images = []\n",
    "        print(f\"  RAG Search: '{keywords[:50]}...'\")\n",
    "\n",
    "        for idx in top_indices:\n",
    "            item = rag_data[idx]\n",
    "            rel_path = item['path'].replace('\\\\', '/')\n",
    "\n",
    "            paths_to_try = [\n",
    "                os.path.join(RAG_IMAGE_DIR, rel_path),\n",
    "                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)),\n",
    "            ]\n",
    "\n",
    "            path_parts = rel_path.split('/')\n",
    "            if len(path_parts) > 1:\n",
    "                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n",
    "\n",
    "            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n",
    "\n",
    "            for try_path in paths_to_try:\n",
    "                if os.path.exists(try_path):\n",
    "                    try:\n",
    "                        img = Image.open(try_path).convert('RGB')\n",
    "                        retrieved_images.append(img)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"      Error: {e}\")\n",
    "\n",
    "        return retrieved_images\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  RAG error: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"✅ RAG retrieval function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# CELL 4D: Production Pipeline Helpers (Tesseract OCR + Tiling)\n",
    "# ============================================================\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import pytesseract\n",
    "\n",
    "print(\"⚙️ Initializing Production Pipeline...\")\n",
    "\n",
    "def render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n",
    "    \"\"\"Renders the first page of a PDF to a High-Res PIL Image using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        page = doc.load_page(0)\n",
    "        zoom = dpi / 72.0\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        doc.close()\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Rendering Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_tesseract_ocr(img: Image.Image) -> List[str]:\n",
    "    \"\"\"\n",
    "    Runs Tesseract OCR on the image and returns a sorted, unique list of text found.\n",
    "    Uses PSM 11 (Sparse Text) mode which is best for engineering drawings.\n",
    "    Normalizes common engineering symbols (Ø -> DIA).\n",
    "\n",
    "    Args:\n",
    "        img: PIL Image\n",
    "\n",
    "    Returns:\n",
    "        List of unique text strings found, sorted alphabetically\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run Tesseract with sparse text mode (PSM 11)\n",
    "        raw_text = pytesseract.image_to_string(img, config='--psm 11')\n",
    "\n",
    "        # Split by newlines and clean\n",
    "        lines = raw_text.split('\\n')\n",
    "\n",
    "        texts = []\n",
    "        for line in lines:\n",
    "            # Strip whitespace\n",
    "            cleaned = line.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not cleaned:\n",
    "                continue\n",
    "\n",
    "            # Skip garbage (less than 2 alphanumeric characters)\n",
    "            alphanumeric_count = sum(1 for c in cleaned if c.isalnum())\n",
    "            if alphanumeric_count < 2:\n",
    "                continue\n",
    "\n",
    "            # Normalize engineering symbols\n",
    "            cleaned = cleaned.replace(\"Ø\", \"DIA \")\n",
    "            cleaned = cleaned.replace(\"ø\", \"DIA \")\n",
    "\n",
    "            texts.append(cleaned)\n",
    "\n",
    "        # Deduplicate and sort\n",
    "        return sorted(list(set(texts)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Tesseract OCR Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n",
    "    \"\"\"Splits the image into 4 overlapping quadrants for better resolution.\"\"\"\n",
    "    w, h = full_img.size\n",
    "    tile_w, tile_h = w // 2, h // 2\n",
    "    overlap = int(min(w, h) * 0.15)\n",
    "\n",
    "    boxes = {\n",
    "        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n",
    "        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n",
    "        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n",
    "        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n",
    "    }\n",
    "\n",
    "    tiles = []\n",
    "    for name, box in boxes.items():\n",
    "        tiles.append((name, full_img.crop(box)))\n",
    "    return tiles\n",
    "\n",
    "print(\"✅ Production Pipeline Helpers Loaded (Tesseract OCR).\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJGxTszX-2XY",
    "outputId": "17cb872f-a525-4cd5-8d6c-df96ea023d0b"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "⚙️ Initializing Production Pipeline...\n",
      "✅ Production Pipeline Helpers Loaded (Tesseract OCR).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmd8w7bcwHHb"
   },
   "source": [
    "## 5. Main Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7BcVNR3hwHHb",
    "outputId": "bc4f44f1-c7cb-4682-e001-53a059ecf08a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ inspect_drawing_rag() defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5A: Main Inspection Function (RAG + OCR Hybrid)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def inspect_drawing_rag(drawing_path, verbose=True):\n",
    "    \"\"\"Main inspection function using Vision + OCR + RAG.\"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INSPECTING: {os.path.basename(drawing_path)}\")\n",
    "        print('='*60)\n",
    "\n",
    "    # Phase 0: Identify Part\n",
    "    if verbose:\n",
    "        print(\"\\n[1/5] Identifying part...\")\n",
    "\n",
    "    pn, context = get_part_context(drawing_path)\n",
    "\n",
    "    if not context:\n",
    "        if verbose:\n",
    "            print(f\"  ❌ Could not find context for '{drawing_path}'\")\n",
    "        return {'result': 'FAIL', 'part_number': None, 'description': None, 'details': 'Identity Unknown'}\n",
    "\n",
    "    context_str = build_context_string(pn, context)\n",
    "\n",
    "    if pn and verbose:\n",
    "        print(f\"  Part: {pn}\")\n",
    "        print(f\"  Description: {context.get('description', 'N/A')}\")\n",
    "        print(f\"  Assembly: {context.get('assembly', 'N/A')}\")\n",
    "\n",
    "    # Phase 1: Load Image + OCR\n",
    "    if verbose:\n",
    "        print(\"\\n[2/5] Loading drawing & OCR scan...\")\n",
    "\n",
    "    try:\n",
    "        drawing_image = pdf_to_image(drawing_path)\n",
    "        if drawing_image is None:\n",
    "            return {'result': 'ERROR', 'part_number': pn, 'details': 'Failed to load PDF'}\n",
    "        if verbose:\n",
    "            print(f\"  Drawing loaded: {drawing_image.size}\")\n",
    "    except Exception as e:\n",
    "        return {'result': 'ERROR', 'part_number': pn, 'details': f'PDF Error: {e}'}\n",
    "\n",
    "    ocr_text_list = []\n",
    "    ocr_text_block = \"\"\n",
    "    try:\n",
    "        ocr_input = np.array(drawing_image)\n",
    "        ocr_text_list = get_drawing_text_ocr(ocr_input)\n",
    "        ocr_text_block = \"\\n\".join(ocr_text_list)\n",
    "        if verbose:\n",
    "            print(f\"  OCR Found {len(ocr_text_list)} elements: {ocr_text_list[:5]}...\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"  OCR Warning: {e}\")\n",
    "\n",
    "    # Phase 2: Vision + OCR Extraction\n",
    "    if verbose:\n",
    "        print(\"\\n[3/5] CoT Step 1: Extraction...\")\n",
    "\n",
    "    if ocr_text_block:\n",
    "        extraction_prompt = f\"\"\"You are an Expert Engineering Drawing Scanner.\n",
    "\n",
    "OCR Data found:\n",
    "--- OCR DATA ---\n",
    "{ocr_text_block}\n",
    "--- END ---\n",
    "\n",
    "Extract: 1. Thread Callouts 2. Bore/Hole Dimensions 3. Material Note 4. GD&T Symbols\n",
    "Trust the OCR data. Output a clean list.\"\"\"\n",
    "    else:\n",
    "        extraction_prompt = \"\"\"Scan this drawing and extract:\n",
    "1. Thread Callouts (e.g., '1/4-20 UNC')\n",
    "2. Bore/Hole Dimensions with tolerances\n",
    "3. Material Note\n",
    "4. GD&T Symbols\n",
    "List them exactly as written.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": drawing_image}, {\"type\": \"text\", \"text\": extraction_prompt}]}]\n",
    "    extraction_text = query_model(messages, max_tokens=512)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": extraction_text}]})\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Extracted:\\n{extraction_text[:300]}...\")\n",
    "\n",
    "    # Phase 3: RAG Retrieval\n",
    "    if verbose:\n",
    "        print(\"\\n[4/5] Retrieving ASME references...\")\n",
    "\n",
    "    asme_images = []\n",
    "    if rag_available:\n",
    "        asme_images = retrieve_asme_pages(extraction_text, top_k=2)\n",
    "    if verbose:\n",
    "        print(f\"  ASME pages: {len(asme_images)}\")\n",
    "\n",
    "    # Phase 4: Audit\n",
    "    if verbose:\n",
    "        print(\"\\n[5/5] CoT Step 2: Audit...\")\n",
    "\n",
    "    mating_parts_str = context.get('siblings', 'None') if context else 'None'\n",
    "\n",
    "    audit_prompt = f\"\"\"You are a Strict Logic Comparator.\n",
    "\n",
    "REQUIREMENTS: {context_str}\n",
    "ACTUALS: {extraction_text}\n",
    "\n",
    "RULES:\n",
    "- Verify dimensions match mating parts\n",
    "- If Mating Part is '3/4-16' and ACTUALS shows 'M10' -> FAIL\n",
    "- Missing features -> CANNOT VERIFY\n",
    "\n",
    "OUTPUT: Line 1: PASS or FAIL\n",
    "Then: Tier 1 (General), Tier 2 (GD&T), Tier 3 (Assembly Fit), Recommendations\"\"\"\n",
    "\n",
    "    content_2 = [{\"type\": \"image\", \"image\": img} for img in asme_images]\n",
    "    content_2.append({\"type\": \"text\", \"text\": audit_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": content_2})\n",
    "\n",
    "    audit_response = query_model(messages, max_tokens=1500)\n",
    "\n",
    "    # Parse result\n",
    "    first_line = audit_response.split('\\n')[0].upper()\n",
    "    if 'PASS' in first_line and 'FAIL' not in first_line:\n",
    "        result = 'PASS'\n",
    "    elif 'FAIL' in first_line:\n",
    "        result = 'FAIL'\n",
    "    else:\n",
    "        result = 'REVIEW'\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nRESULT: {result}\\n{'='*60}\")\n",
    "        print(audit_response)\n",
    "\n",
    "    return {\n",
    "        'result': result, 'part_number': pn,\n",
    "        'description': context.get('description') if context else None,\n",
    "        'assembly': context.get('assembly') if context else None,\n",
    "        'mating_parts': mating_parts_str,\n",
    "        'ocr_text_count': len(ocr_text_list),\n",
    "        'asme_pages_used': len(asme_images),\n",
    "        'details': audit_response\n",
    "    }\n",
    "\n",
    "print(\"✅ inspect_drawing_rag() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5B: UNIVERSAL MISMATCH INSPECTOR (Batch-Ready)\n# ============================================================\n\ndef inspect_drawing_universal(pdf_path):\n    \"\"\"\n    Universal inspector that dynamically loads context for each file.\n    Batch-ready with robust error handling.\n    \"\"\"\n    # 1. Get Dynamic Context for THIS specific file\n    pn = None\n    try:\n        pn, ctx = get_part_context(pdf_path)\n        context_str = ctx['siblings']  # Fixed: was 'siblings_str'\n        part_name = ctx['description']  # Fixed: was 'desc'\n    except:\n        print(f\"⚠️ Context Lookup Failed for {pdf_path}\")\n        context_str = \"No Mating Parts Found in Database.\"\n        part_name = \"Unknown Part\"\n\n    print(f\"\\n{'='*60}\\nUNIVERSAL INSPECTION: {part_name}\\n{'='*60}\")\n    \n    # --- Phase A: Perception ---\n    print(\"[1/3] Reading Drawing (Tesseract OCR)...\")\n    full_img = render_pdf_page(pdf_path, dpi=300)\n    if not full_img:\n        return {'error': 'Failed to render PDF', 'part_number': pn}\n    \n    # Run Tesseract\n    ocr_texts = run_tesseract_ocr(full_img)\n    # Filter for relevant engineering text (numbers/dims)\n    filtered_ocr = [t for t in ocr_texts if any(char.isdigit() for char in t)]\n    ocr_block = \"\\n\".join([f\"- {t}\" for t in filtered_ocr[:120]])\n    \n    print(f\"  > Evidence: {len(filtered_ocr)} relevant lines found.\")\n\n    # --- Phase B: Reasoning ---\n    print(\"[2/3] Generating Dynamic Truth Table...\")\n    \n    system_prompt = \"\"\"You are a Universal Engineering Auditor.\n\n**YOUR GOAL:**\nCross-reference \"LIST A\" (Requirements) against \"LIST B\" (Drawing Evidence).\n\n**LOGIC PROTOCOL:**\n1. Read LIST A to identify the Mating Parts.\n2. For EACH Mating Part, search LIST B for a corresponding feature (Thread, Hole, Diameter).\n3. **STRICTLY** compare dimensions. \n   - If List A says \"3/4-16\" and List B says \"M10\", output NO.\n   - If List A says \"0.750\" and List B says \"0.500\", output NO.\n4. If the Mating Part is generic (e.g. \"WASHER\"), and you see *any* washer dimension, you may output \"LIKELY MATCH\".\n5. If you cannot find any matching text in List B, output \"NOT FOUND\".\"\"\"\n\n    user_text = f\"\"\"**LIST A (THE REQUIREMENTS for Part {pn}):**\n{context_str}\n\n**LIST B (THE DRAWING TEXT):**\n{ocr_block}\n\n**TASK:**\nCreate a Truth Table checking the compatibility of the Mating Parts in List A against the Evidence in List B.\n\n| Mating Part (from List A) | Found Feature in List B | Compatible? (YES/NO/NOT FOUND) |\n| :--- | :--- | :--- |\n| [Name/Spec of Part 1] | [Text from Drawing] | [Verdict] |\n| [Name/Spec of Part 2] | [Text from Drawing] | [Verdict] |\n\n**FINAL VERDICT:**\nPASS if all critical features match.\nFAIL if there is a direct contradiction (Metric vs Imperial).\"\"\"\n\n    content_payload = [\n        {'type': 'image', 'image': full_img},\n        {'type': 'text', 'text': user_text}\n    ]\n\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': content_payload}\n    ]\n\n    # Inference\n    print(\"[3/3] Running inference...\")\n    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(\n        text=[text_input],\n        images=[full_img],\n        return_tensors=\"pt\",\n        padding=True\n    ).to(model.device)\n\n    token_count = inputs.input_ids.shape[1]\n    print(f\"  Token count: {token_count}\")\n\n    generated_ids = model.generate(**inputs, max_new_tokens=600)\n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n    \n    print(f\"\\n{'='*60}\\nRESULT:\\n{'='*60}\")\n    print(response)\n    \n    return {\n        'response': response,\n        'part_number': pn,\n        'part_name': part_name,\n        'ocr_total': len(ocr_texts),\n        'ocr_filtered': len(filtered_ocr),\n        'token_count': token_count\n    }\n\n# Aliases for compatibility\ninspect_drawing_strict_optimized = inspect_drawing_universal\ninspect_drawing_production = inspect_drawing_universal\n\nprint(\"✅ Universal Inspector Loaded (Batch-Ready).\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peirUbTi-2XY",
    "outputId": "6b150bf9-a0e9-43a0-9577-f796e3c256ce"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5C: STAGE 2 - Engineering Consultant (Improvement Suggestions)\n# ============================================================\n\ndef generate_smart_rag_queries(part_description, mating_parts_str):\n    \"\"\"\n    Generate intelligent RAG search queries based on part type and mating parts.\n    Returns a list of targeted search queries for ASME standards.\n    \"\"\"\n    queries = []\n    desc_lower = part_description.lower() if part_description else \"\"\n    mating_lower = mating_parts_str.lower() if mating_parts_str else \"\"\n    \n    # Shaft-related queries\n    if any(kw in desc_lower for kw in ['shaft', 'spindle', 'axle', 'arbor']):\n        queries.append(\"Shaft Runout Tolerances Cylindricity\")\n        queries.append(\"Concentricity Total Runout Control\")\n    \n    # Bearing-related queries\n    if any(kw in desc_lower + mating_lower for kw in ['bearing', 'bushing', 'sleeve']):\n        queries.append(\"Bearing Fits Interference Clearance\")\n        queries.append(\"Cylindrical Tolerance Zones\")\n    \n    # Housing/Casting-related queries\n    if any(kw in desc_lower for kw in ['housing', 'casting', 'body', 'block', 'head']):\n        queries.append(\"Flatness Parallelism Datum Surfaces\")\n        queries.append(\"Position Tolerance Hole Patterns\")\n    \n    # Thread-related queries\n    if any(kw in desc_lower + mating_lower for kw in ['screw', 'bolt', 'thread', 'tap']):\n        queries.append(\"Thread Tolerances Pitch Diameter\")\n        queries.append(\"Perpendicularity Threaded Holes\")\n    \n    # Gear-related queries\n    if any(kw in desc_lower for kw in ['gear', 'pinion', 'sprocket', 'pulley']):\n        queries.append(\"Runout Profile Tolerances Gears\")\n        queries.append(\"Concentricity Bore Tolerances\")\n    \n    # Default engineering queries if nothing specific found\n    if not queries:\n        queries.append(\"General Dimensioning Tolerancing GD&T\")\n        queries.append(\"Datum Feature Selection\")\n    \n    # Always add position tolerance query\n    queries.append(\"Position Tolerance MMC LMC\")\n    \n    return queries[:3]  # Limit to top 3 queries\n\n\ndef suggest_improvements_stage2(pdf_path, context_data, ocr_text_list, drawing_image=None):\n    \"\"\"\n    Stage 2: Senior GD&T Consultant - Suggests qualitative improvements.\n    \n    This runs ONLY after Stage 1 passes. It analyzes the drawing in context\n    of its mating parts and proposes engineering improvements.\n    \n    Args:\n        pdf_path: Path to the PDF file\n        context_data: Dict with 'description', 'siblings', 'assembly', 'pn'\n        ocr_text_list: List of OCR text strings (from Stage 1, not re-run)\n        drawing_image: Optional PIL Image (if already loaded)\n    \n    Returns:\n        Dict with improvement suggestions\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(\"STAGE 2: ENGINEERING CONSULTANT\")\n    print(f\"{'='*60}\")\n    \n    # Extract context\n    part_desc = context_data.get('description', 'Unknown Part')\n    pn = context_data.get('pn', 'Unknown')\n    assembly = context_data.get('assembly', 'Unknown Assembly')\n    mating_parts = context_data.get('siblings', 'No mating parts')\n    \n    print(f\"Part: {pn} ({part_desc})\")\n    print(f\"Assembly: {assembly}\")\n    \n    # --- Phase 1: Smart RAG Retrieval ---\n    print(\"\\n[1/3] Performing Smart Library Lookup...\")\n    \n    # Generate intelligent search queries\n    search_queries = generate_smart_rag_queries(part_desc, mating_parts)\n    print(f\"  Search Queries: {search_queries}\")\n    \n    # Retrieve ASME standard pages (max 3 for token safety)\n    asme_images = []\n    if rag_available:\n        for query in search_queries:\n            retrieved = retrieve_asme_pages(query, top_k=1)\n            asme_images.extend(retrieved)\n            if len(asme_images) >= 3:\n                break\n    \n    asme_images = asme_images[:3]  # Hard limit\n    print(f\"  Retrieved {len(asme_images)} ASME reference pages\")\n    \n    # --- Phase 2: Load Drawing (if not provided) ---\n    print(\"\\n[2/3] Preparing Analysis...\")\n    \n    if drawing_image is None:\n        drawing_image = render_pdf_page(pdf_path, dpi=200)  # Lower DPI for Stage 2\n        if not drawing_image:\n            return {'error': 'Failed to render PDF for Stage 2'}\n    \n    # Prepare OCR summary (compact for token efficiency)\n    ocr_summary = \"\\n\".join([f\"- {t}\" for t in ocr_text_list[:50]])\n    \n    # --- Phase 3: Consultant Analysis ---\n    print(\"\\n[3/3] Consulting Senior GD&T Engineer...\")\n    \n    system_prompt = \"\"\"You are a Senior GD&T Consultant with 25+ years of experience in precision manufacturing.\n\n**YOUR ROLE:**\nReview this engineering drawing and provide constructive improvement suggestions based on:\n1. The part's function and its mating parts\n2. Industry best practices from ASME Y14.5\n3. Common manufacturing and inspection considerations\n\n**ANALYSIS FRAMEWORK:**\n- Missing Critical Specs: Does a rotating part have Runout? Does a mating surface have Flatness?\n- Tolerance Adequacy: Are tolerances tight enough for the assembly fit?\n- Datum Selection: Are datums properly chosen for the part's function?\n- Inspection Feasibility: Can the specified tolerances be measured?\n\n**TONE:** Be constructive, educational, and specific. Explain WHY each suggestion matters.\"\"\"\n\n    user_text = f\"\"\"**PART CONTEXT:**\n- Part Number: {pn}\n- Description: {part_desc}\n- Assembly: {assembly}\n- Mating Parts: {mating_parts}\n\n**DRAWING TEXT (OCR Evidence):**\n{ocr_summary}\n\n**TASK:**\nBased on the drawing, the part's function, and the ASME reference pages provided:\n\n1. **MISSING TOLERANCES:** List any GD&T controls that SHOULD be present but are missing.\n   - Example: \"A shaft mating with a bearing should have Runout or Total Runout specified.\"\n\n2. **IMPROVEMENT SUGGESTIONS:** Propose 2-3 specific improvements with reasoning.\n   - Example: \"Add Flatness to Datum A (0.001) to ensure proper seating of the bearing.\"\n\n3. **ASSEMBLY FIT NOTES:** Any concerns about fit with the mating parts?\n\n4. **PRIORITY RANKING:** Rank your suggestions by importance (Critical / Recommended / Nice-to-Have).\n\nKeep your response focused and actionable.\"\"\"\n\n    # Build message payload\n    content = [{'type': 'image', 'image': drawing_image}]\n    \n    # Add ASME reference images\n    for i, asme_img in enumerate(asme_images):\n        content.append({'type': 'image', 'image': asme_img})\n        content.append({'type': 'text', 'text': f\"[ASME Reference {i+1}]\"})\n    \n    content.append({'type': 'text', 'text': user_text})\n    \n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': content}\n    ]\n    \n    # Inference\n    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \n    all_images = [drawing_image] + asme_images\n    \n    inputs = processor(\n        text=[text_input],\n        images=all_images,\n        return_tensors=\"pt\",\n        padding=True\n    ).to(model.device)\n    \n    token_count = inputs.input_ids.shape[1]\n    print(f\"  Token count: {token_count}\")\n    \n    if token_count > 28000:\n        print(\"  ⚠️ Warning: High token count, reducing ASME images...\")\n        # Retry with fewer images if needed\n        asme_images = asme_images[:1]\n        all_images = [drawing_image] + asme_images\n        content = [{'type': 'image', 'image': drawing_image}]\n        if asme_images:\n            content.append({'type': 'image', 'image': asme_images[0]})\n            content.append({'type': 'text', 'text': \"[ASME Reference]\"})\n        content.append({'type': 'text', 'text': user_text})\n        messages = [{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': content}]\n        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = processor(text=[text_input], images=all_images, return_tensors=\"pt\", padding=True).to(model.device)\n        token_count = inputs.input_ids.shape[1]\n        print(f\"  Reduced token count: {token_count}\")\n    \n    generated_ids = model.generate(**inputs, max_new_tokens=800)\n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n    \n    print(f\"\\n{'='*60}\")\n    print(\"CONSULTANT RECOMMENDATIONS:\")\n    print(f\"{'='*60}\")\n    print(response)\n    \n    return {\n        'suggestions': response,\n        'asme_pages_used': len(asme_images),\n        'search_queries': search_queries,\n        'token_count': token_count\n    }\n\nprint(\"✅ Stage 2: suggest_improvements_stage2() defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5D: MASTER PIPELINE - Full Inspection (Stage 1 + Stage 2)\n# ============================================================\nfrom datetime import datetime\n\ndef run_full_inspection(pdf_path, skip_stage2_on_fail=True):\n    \"\"\"\n    Master pipeline that runs both inspection stages:\n    \n    Stage 1 (Gatekeeper): Strict mismatch detection (Imperial vs Metric, missing specs)\n    Stage 2 (Consultant): Improvement suggestions (only runs if Stage 1 passes)\n    \n    Args:\n        pdf_path: Path to the PDF drawing\n        skip_stage2_on_fail: If True, skip Stage 2 when Stage 1 fails (default)\n    \n    Returns:\n        Comprehensive JSON report with both stages' results\n    \"\"\"\n    print(f\"\\n{'#'*60}\")\n    print(f\"# FULL ENGINEERING INSPECTION PIPELINE\")\n    print(f\"# File: {os.path.basename(pdf_path)}\")\n    print(f\"# Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"{'#'*60}\")\n    \n    # Initialize result structure\n    result = {\n        'file': os.path.basename(pdf_path),\n        'path': pdf_path,\n        'timestamp': datetime.now().isoformat(),\n        'stage1': None,\n        'stage2': None,\n        'final_verdict': None\n    }\n    \n    # =========================================================\n    # STAGE 1: THE GATEKEEPER (Strict Mismatch Detection)\n    # =========================================================\n    print(f\"\\n{'='*60}\")\n    print(\"STAGE 1: THE GATEKEEPER (Mismatch Detection)\")\n    print(f\"{'='*60}\")\n    \n    # Get context first (we'll reuse it for Stage 2)\n    pn, ctx = get_part_context(pdf_path)\n    \n    if not ctx:\n        print(f\"⚠️ No context found for {pdf_path}\")\n        result['stage1'] = {'verdict': 'ERROR', 'reason': 'Part not in database'}\n        result['final_verdict'] = 'ERROR'\n        return result\n    \n    # Store context for later\n    context_data = {\n        'pn': pn,\n        'description': ctx.get('description', 'Unknown'),\n        'assembly': ctx.get('assembly', 'Unknown'),\n        'siblings': ctx.get('siblings', 'No mating parts')\n    }\n    \n    # Run Stage 1 inspection\n    stage1_result = inspect_drawing_universal(pdf_path)\n    \n    # Parse Stage 1 verdict\n    if isinstance(stage1_result, dict) and 'error' in stage1_result:\n        stage1_verdict = 'ERROR'\n        stage1_reason = stage1_result['error']\n    else:\n        response = stage1_result.get('response', '') if isinstance(stage1_result, dict) else str(stage1_result)\n        stage1_verdict, stage1_reason = parse_verdict(response)\n    \n    result['stage1'] = {\n        'verdict': stage1_verdict,\n        'reason': stage1_reason,\n        'part_number': pn,\n        'part_name': context_data['description'],\n        'ocr_count': stage1_result.get('ocr_filtered', 0) if isinstance(stage1_result, dict) else 0,\n        'response': response[:1000] if 'response' in dir() else ''\n    }\n    \n    print(f\"\\n>>> STAGE 1 VERDICT: {stage1_verdict} - {stage1_reason}\")\n    \n    # =========================================================\n    # STAGE 2: THE CONSULTANT (Improvement Suggestions)\n    # =========================================================\n    if stage1_verdict == 'FAIL' and skip_stage2_on_fail:\n        print(f\"\\n{'='*60}\")\n        print(\"STAGE 2: SKIPPED (Stage 1 Failed)\")\n        print(f\"{'='*60}\")\n        print(\"Fix the Stage 1 issues before requesting improvement suggestions.\")\n        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 failed'}\n        result['final_verdict'] = 'FAIL'\n        \n    elif stage1_verdict == 'ERROR':\n        print(f\"\\n{'='*60}\")\n        print(\"STAGE 2: SKIPPED (Stage 1 Error)\")\n        print(f\"{'='*60}\")\n        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 error'}\n        result['final_verdict'] = 'ERROR'\n        \n    else:\n        # Stage 1 passed - proceed to Stage 2\n        print(f\"\\n{'='*60}\")\n        print(\"STAGE 2: THE CONSULTANT (Proceeding...)\")\n        print(f\"{'='*60}\")\n        \n        # Get OCR text from Stage 1 result\n        ocr_text_list = []\n        if isinstance(stage1_result, dict):\n            # We need to re-extract OCR for Stage 2 since it wasn't stored\n            # But we can use a lower DPI image\n            full_img = render_pdf_page(pdf_path, dpi=200)\n            if full_img:\n                ocr_text_list = run_tesseract_ocr(full_img)\n                ocr_text_list = [t for t in ocr_text_list if any(c.isdigit() for c in t)]\n        \n        try:\n            stage2_result = suggest_improvements_stage2(\n                pdf_path=pdf_path,\n                context_data=context_data,\n                ocr_text_list=ocr_text_list\n            )\n            \n            result['stage2'] = {\n                'status': 'COMPLETED',\n                'suggestions': stage2_result.get('suggestions', ''),\n                'asme_pages_used': stage2_result.get('asme_pages_used', 0),\n                'search_queries': stage2_result.get('search_queries', [])\n            }\n            \n        except Exception as e:\n            print(f\"⚠️ Stage 2 Error: {e}\")\n            result['stage2'] = {'status': 'ERROR', 'reason': str(e)}\n        \n        result['final_verdict'] = 'PASS_WITH_SUGGESTIONS' if stage1_verdict == 'PASS' else 'REVIEW'\n    \n    # =========================================================\n    # FINAL SUMMARY\n    # =========================================================\n    print(f\"\\n{'#'*60}\")\n    print(\"# FINAL INSPECTION SUMMARY\")\n    print(f\"{'#'*60}\")\n    print(f\"Part: {pn} ({context_data['description']})\")\n    print(f\"Stage 1 (Gatekeeper): {result['stage1']['verdict']}\")\n    print(f\"Stage 2 (Consultant): {result['stage2'].get('status', 'N/A')}\")\n    print(f\"Final Verdict: {result['final_verdict']}\")\n    print(f\"{'#'*60}\\n\")\n    \n    return result\n\n\ndef run_full_inspection_batch(drawing_folder, output_file=None, limit=None):\n    \"\"\"\n    Run full inspection (Stage 1 + Stage 2) on all PDFs in a folder.\n    \"\"\"\n    from tqdm.notebook import tqdm\n    \n    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n    pdf_files = sorted(list(set(pdf_files)))\n    \n    if limit:\n        pdf_files = pdf_files[:limit]\n    \n    print(f\"{'#'*60}\")\n    print(f\"FULL INSPECTION BATCH\")\n    print(f\"Files: {len(pdf_files)}\")\n    print(f\"{'#'*60}\\n\")\n    \n    results = []\n    \n    for pdf_path in tqdm(pdf_files, desc=\"Full Inspection\"):\n        try:\n            result = run_full_inspection(pdf_path)\n            results.append(result)\n        except Exception as e:\n            results.append({\n                'file': os.path.basename(pdf_path),\n                'final_verdict': 'ERROR',\n                'error': str(e)\n            })\n    \n    # Save results\n    if not output_file:\n        output_file = f\"full_inspection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n    \n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    # Summary\n    verdicts = [r.get('final_verdict', 'ERROR') for r in results]\n    print(f\"\\n{'='*60}\")\n    print(\"BATCH SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Total: {len(results)}\")\n    print(f\"PASS_WITH_SUGGESTIONS: {verdicts.count('PASS_WITH_SUGGESTIONS')}\")\n    print(f\"FAIL: {verdicts.count('FAIL')}\")\n    print(f\"REVIEW: {verdicts.count('REVIEW')}\")\n    print(f\"ERROR: {verdicts.count('ERROR')}\")\n    print(f\"Results saved to: {output_file}\")\n    \n    return results\n\nprint(\"✅ Master Pipeline: run_full_inspection() defined.\")\nprint(\"✅ Batch Pipeline: run_full_inspection_batch() defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc7S5QTzwHHb"
   },
   "source": [
    "## 6. Batch Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-tlYl9GwHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7A: Full Inspection Test (Stage 1 + Stage 2)\n# ============================================================\nfrom google.colab import files\n\nprint(\"Upload a PDF drawing for FULL INSPECTION (Stage 1 + Stage 2):\")\nprint(\"  Stage 1: Gatekeeper (Mismatch Detection)\")\nprint(\"  Stage 2: Consultant (Improvement Suggestions + RAG)\")\nprint()\n\nuploaded = files.upload()\n\nif uploaded:\n    test_drawing = list(uploaded.keys())[0]\n    print(f\"\\n🔍 Running FULL INSPECTION on {test_drawing}...\")\n    \n    # Run both stages\n    result = run_full_inspection(test_drawing)\n    \n    # Save result to JSON\n    output_file = f\"inspection_{test_drawing.replace('.pdf', '')}.json\"\n    with open(output_file, 'w') as f:\n        json.dump(result, f, indent=2)\n    print(f\"\\n📄 Result saved to: {output_file}\")\nelse:\n    print(\"No file uploaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM1_6wuZwHHb"
   },
   "source": [
    "## 7. Test the Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "0hzx2gEywHHb",
    "outputId": "f327b009-125f-4e4c-c263-6d39ff284c90"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload a PDF drawing to inspect (Strict Mismatch Mode):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-29cdbb65-4249-4c8a-a192-748957ebe736\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-29cdbb65-4249-4c8a-a192-748957ebe736\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving 51754201_03.pdf to 51754201_03.pdf\n",
      "\n",
      "Running STRICT MISMATCH inspection on 51754201_03.pdf...\n",
      "  Part: 517542 (HEAD CASTING)\n",
      "\n",
      "============================================================\n",
      "STRICT INSPECTION (OPTIMIZED): 51754201_03.pdf\n",
      "============================================================\n",
      "[1/3] Reading Drawing (Tesseract OCR)...\n",
      "  > Evidence: 90 strings found. (Filtered to 54 relevant lines)\n",
      "[2/3] Generating Logic Truth Table...\n",
      "[3/3] Running inference...\n",
      "  Token count: 17267\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7A: Single File Test (STRICT MISMATCH MODE)\n",
    "# ============================================================\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload a PDF drawing to inspect (Strict Mismatch Mode):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    test_drawing = list(uploaded.keys())[0]\n",
    "    print(f\"\\nRunning STRICT MISMATCH inspection on {test_drawing}...\")\n",
    "    result = inspect_drawing_production(test_drawing)  # <-- Uses Strict Mismatch Mode\n",
    "else:\n",
    "    print(\"No file uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "00c88766"
   },
   "source": [
    "# ============================================================\n",
    "# CELL 7B: Recovery - Reload Context (After Restart)\n",
    "# ============================================================\n",
    "import os, json, re\n",
    "\n",
    "if 'filename_to_pn' not in globals() or 'part_context_db' not in globals():\n",
    "    print(\"🔄 Reloading Context Databases...\")\n",
    "\n",
    "    MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
    "    STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
    "\n",
    "    def locate_file(filename):\n",
    "        if os.path.exists(filename): return os.path.abspath(filename)\n",
    "        if os.path.exists(os.path.join(\"rag_data\", filename)): return os.path.abspath(os.path.join(\"rag_data\", filename))\n",
    "        return None\n",
    "\n",
    "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
    "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
    "\n",
    "    if not FILE_MAPPING_PATH or not STRUCTURE_PATH:\n",
    "        print(\"❌ Config files not found. Please re-upload.\")\n",
    "    else:\n",
    "        with open(FILE_MAPPING_PATH, 'r') as f:\n",
    "            file_mapping_list = json.load(f)\n",
    "\n",
    "        filename_to_pn = {}\n",
    "        for entry in file_mapping_list:\n",
    "            pn, fname = entry.get('pn'), entry.get('file')\n",
    "            if pn and fname:\n",
    "                filename_to_pn[fname] = pn\n",
    "                filename_to_pn[fname + '.pdf'] = pn\n",
    "                filename_to_pn[fname + '.PDF'] = pn\n",
    "\n",
    "        with open(STRUCTURE_PATH, 'r') as f:\n",
    "            structure_data = json.load(f)\n",
    "\n",
    "        part_context_db = {}\n",
    "        def normalize_pn(pn): return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
    "\n",
    "        for assembly_name, parts_list in structure_data.items():\n",
    "            for part in parts_list:\n",
    "                pn, desc = part['pn'], part['desc']\n",
    "                siblings = [f\"{p['pn']} ({p['desc']})\" for p in parts_list if p['pn'] != pn]\n",
    "                key = normalize_pn(pn)\n",
    "                ctx = {'pn': pn, 'description': desc, 'assembly': assembly_name, 'siblings': \"; \".join(siblings[:12])}\n",
    "                part_context_db[key] = ctx\n",
    "                part_context_db[pn] = ctx\n",
    "\n",
    "        print(f\"✅ Restored: {len(filename_to_pn)} mappings, {len(part_context_db)//2} parts\")\n",
    "else:\n",
    "    print(\"✅ Context already loaded.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fc691493"
   },
   "source": [
    "# ============================================================\n",
    "# CELL 7C: OCR Libraries (Backup Install)\n",
    "# ============================================================\n",
    "!sudo apt-get install -y tesseract-ocr > /dev/null 2>&1\n",
    "!pip install -q pytesseract\n",
    "print(\"✅ Tesseract OCR Libraries Installed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "666f60f1"
   },
   "source": [
    "# ============================================================\n",
    "# CELL 7D: Verify RAG Database\n",
    "# ============================================================\n",
    "import os, glob\n",
    "\n",
    "if 'DATA_DIR' not in globals():\n",
    "    DATA_DIR = \"/content\"\n",
    "\n",
    "rag_db_path = os.path.join(DATA_DIR, \"rag_visual_db\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"RAG path: {rag_db_path}\")\n",
    "\n",
    "if os.path.exists(rag_db_path):\n",
    "    images = glob.glob(os.path.join(rag_db_path, \"**/*.png\"), recursive=True)\n",
    "    images += glob.glob(os.path.join(rag_db_path, \"**/*.jpg\"), recursive=True)\n",
    "    print(f\"✅ Found {len(images)} images\")\n",
    "else:\n",
    "    print(\"❌ Folder NOT found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JB5KJmE_wHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7E: Batch Test (Upload ZIP) - Strict Mismatch Mode\n# ============================================================\nfrom google.colab import files\nimport shutil, zipfile, glob\n\nprint(\"Upload a ZIP file with PDF drawings for batch inspection:\")\nuploaded = files.upload()\n\nif uploaded:\n    zip_file = next((f for f in uploaded if f.lower().endswith('.zip')), None)\n    if zip_file:\n        batch_dir = \"batch_drawings\"\n        if os.path.exists(batch_dir): \n            shutil.rmtree(batch_dir)\n        os.makedirs(batch_dir, exist_ok=True)\n\n        print(f\"Extracting {zip_file}...\")\n        with zipfile.ZipFile(zip_file, 'r') as zf:\n            zf.extractall(batch_dir)\n\n        pdfs = glob.glob(os.path.join(batch_dir, \"**/*.pdf\"), recursive=True)\n        pdfs += glob.glob(os.path.join(batch_dir, \"**/*.PDF\"), recursive=True)\n        print(f\"Found {len(pdfs)} PDFs\\n\")\n\n        # Run batch with Strict Mismatch Mode\n        results = inspect_batch_strict(batch_dir, \"inspection_results.json\")\n        \n        # Download results\n        print(\"\\nDownloading results...\")\n        files.download(\"inspection_results.json\")\n    else:\n        print(\"No ZIP file found\")\nelse:\n    print(\"No files uploaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG8mlYNMwHHb"
   },
   "source": [
    "## 8. View Failed Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DgLE5trwHHb"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: View Failed Inspections\n",
    "# ============================================================\n",
    "\n",
    "def show_failures(results):\n",
    "    \"\"\"Display failed inspections.\"\"\"\n",
    "    failures = [r for r in results if r.get('result') == 'FAIL']\n",
    "    print(f\"\\nFAILED: {len(failures)}\")\n",
    "    print('='*60)\n",
    "\n",
    "    for i, fail in enumerate(failures, 1):\n",
    "        print(f\"\\n[{i}] {fail.get('file', 'Unknown')}\")\n",
    "        print(f\"    Part: {fail.get('part_number', 'N/A')} - {fail.get('description', 'N/A')}\")\n",
    "        print(f\"    Details: {fail.get('details', 'N/A')[:300]}...\")\n",
    "\n",
    "# Usage: show_failures(results)\n",
    "print(\"✅ show_failures() defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5d52cc773ce1405ca1fa53000a85a99d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c1d352483974fd1b7266a8a097f4472",
       "IPY_MODEL_aceb4e55e5db4ca69eb957d406e4360c",
       "IPY_MODEL_baf2d05be5d94ac3adc6c653f7751573"
      ],
      "layout": "IPY_MODEL_ee995792c97e42f4b10f05ac4c1fd96f"
     }
    },
    "2c1d352483974fd1b7266a8a097f4472": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0f0411651d34b5a87a74bade26cb91a",
      "placeholder": "​",
      "style": "IPY_MODEL_17bbaa41ea7740dba581b8ec970ca46d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "aceb4e55e5db4ca69eb957d406e4360c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c80ee8f5be447959736b543e896d62c",
      "max": 38,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7bfd4b8688c432986c11e46d00927c7",
      "value": 38
     }
    },
    "baf2d05be5d94ac3adc6c653f7751573": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_705afc59cdbe464d88a5690cf84904f0",
      "placeholder": "​",
      "style": "IPY_MODEL_bd63cf407a32497d84b53afba91203a9",
      "value": " 38/38 [11:21&lt;00:00, 13.15s/it]"
     }
    },
    "ee995792c97e42f4b10f05ac4c1fd96f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0f0411651d34b5a87a74bade26cb91a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17bbaa41ea7740dba581b8ec970ca46d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c80ee8f5be447959736b543e896d62c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7bfd4b8688c432986c11e46d00927c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "705afc59cdbe464d88a5690cf84904f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd63cf407a32497d84b53afba91203a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}