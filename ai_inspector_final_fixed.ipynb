{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MkNLQ8UCt-u"
      },
      "source": [
        "# AI Engineering Drawing Inspector (Single File)\n\nStreamlined inspector using:\n- **Qwen2-VL-7B** for visual reasoning (smaller, faster)\n- **LightOnOCR-2** for structured text extraction\n- **RAG** retrieval from ASME Y14.5 standard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ZVsEFCCt-u"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lDC-SvrCt-v",
        "outputId": "f6544c51-5c31-4956-8dad-ea704822ded5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 1A: Install Dependencies & HuggingFace Login\n# ============================================================\n# Core ML dependencies\n!pip install -q accelerate\n!pip install -q qwen-vl-utils\n!pip install -q pdf2image\n!pip install -q faiss-cpu sentence-transformers\n!pip install -q bitsandbytes\n!apt-get install -y poppler-utils > /dev/null 2>&1\n\n# Production Pipeline Dependencies\n!pip install -q pymupdf opencv-python-headless\n\n# LightOnOCR-2 requires transformers from source\n!pip install -q git+https://github.com/huggingface/transformers\n!pip install -q pillow pypdfium2 huggingface_hub\n\nprint(\"All packages installed!\")\n\n# HuggingFace Authentication\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\ntry:\n    hf_token = userdata.get('HF_TOKEN')\n    login(token=hf_token)\n    print(\"Logged in to HuggingFace!\")\nexcept Exception as e:\n    print(f\"HF Login failed: {e}\")\n    print(\"Set HF_TOKEN in Colab Secrets (key icon in left sidebar)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsRtdoY0Ct-v",
        "outputId": "58512207-dd38-4349-98fe-7cf87c1e5745"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 1B: Import Libraries\n# ============================================================\nimport os\nimport json\nimport re\nimport pickle\nimport gc\nimport torch\nfrom pathlib import Path\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nfrom qwen_vl_utils import process_vision_info\n\n# Production Pipeline Imports\nimport fitz  # PyMuPDF\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Dict, Any\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory cache.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMeLfbuMCt-v"
      },
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572,
          "referenced_widgets": [
            "020f33f5c58e4bb480ea684db2a2193c",
            "cef6447101674122912aa4583b77ef78",
            "1643e1df0f2e4db080ec4722b2e7b2d0",
            "ce1962e42dd04a1287fd7c0bc8445cd0",
            "cc980b74b2624998a9b3ec2e764aa531",
            "8166006315674851b84e2d319ddad762",
            "8ce22d2a1bbd49189512a8f09a58b1e8",
            "fc72e43a75584ed5809b1afb87941f90",
            "f4c911fdab15445ea715242f461aef40",
            "4475bffe493440c3b6a55c728c430aa0",
            "5ef06f11a1094a55a6918706b0d2e801",
            "e60fcd5ac14143228eec0412fc467c20",
            "4bb90998a4d14c2dbaacc84a459e3083",
            "90bcfb8d72c7433faddcb4cb630f97ab",
            "aa886a489ef44f62bd173e13efbe582e",
            "c94941ec28f24fd3a07dad70a8f4f98d",
            "66bff5531f084bc2afaec845bebafba4",
            "816de2f310424733a156a987ee70d610",
            "fc6dc22e5fd841f391e16fe9fd682b02",
            "9fb1c899397d493095eb98dfb862a6dd",
            "3667e8d05f2c4a2894fdcf98c8e61dcb",
            "940a1f6a73aa411fb4df5011db472b2f",
            "feea3fb1703b4029901d79d0f40f5af1",
            "cf8220469fe548e182f84159b5c1a4fa",
            "2325a92b4e164c8c951d3e56bf930320",
            "d43fb493a505474aba6ef9c22ef6fdaf",
            "d403353bd6374c049b64bdf1dd4994ca",
            "57743d87b4334f0490adc56f8a569a42",
            "4f9c9e62ac4b40d38a6df70f9e089703",
            "4925377c36524422b648ebb0463928f5",
            "2ab3a34b258b464ebbd365a902c2f5a1",
            "abe5be41caf44a32bb37b667cb9dc127",
            "ad9e9543d20047469ec8142694e667fe"
          ]
        },
        "id": "ZDxMZNRfCt-w",
        "outputId": "dd7f5264-f528-46b4-fb22-2913cd333c41"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 2A: Load LightOnOCR-2 (Small OCR Model - Load First)\n# ============================================================\nimport torch\nfrom transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\nfrom PIL import Image\nimport numpy as np\nfrom google.colab import userdata\n\n# Clear any leftover GPU memory\nclear_gpu_memory()\n\n# Get HF token\ntry:\n    hf_token = userdata.get('HF_TOKEN')\nexcept:\n    hf_token = None\n\nprint(\"Loading LightOnOCR-2-1B (VLM-based OCR)...\")\n\nocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n\nocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n    \"lightonai/LightOnOCR-2-1B\",\n    torch_dtype=ocr_dtype,\n    token=hf_token\n).to(ocr_device)\n\nocr_processor = LightOnOcrProcessor.from_pretrained(\n    \"lightonai/LightOnOCR-2-1B\",\n    token=hf_token\n)\n\nprint(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n\ndef get_drawing_text_ocr(image_input):\n    \"\"\"Run LightOnOCR-2 on the drawing and return structured text.\"\"\"\n    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n    try:\n        if isinstance(image_input, np.ndarray):\n            img = Image.fromarray(image_input).convert(\"RGB\")\n        else:\n            img = image_input.convert(\"RGB\")\n\n        conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n        inputs = ocr_processor.apply_chat_template(\n            conversation, add_generation_prompt=True, tokenize=True,\n            return_dict=True, return_tensors=\"pt\",\n        )\n        inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n        generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n        output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n        return [line.strip() for line in output_text.split(\"\\n\") if line.strip()]\n    except Exception as e:\n        print(f\"LightOnOCR Error: {e}\")\n        return []\n\nprint(\"LightOnOCR-2 Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 2B: Load Qwen2-VL Model (7B - Faster & Lighter)\n# ============================================================\nimport torch\nimport gc\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nfrom google.colab import userdata\n\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()\n\ntry:\n    hf_token = userdata.get('HF_TOKEN')\nexcept:\n    hf_token = None\n\n# Using 7B model - much faster to download and load\nMODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n\nprint(f\"Loading {MODEL_ID}...\")\nprint(f\"GPU memory before: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    token=hf_token,\n    torch_dtype=torch.float16\n)\n\nprocessor = AutoProcessor.from_pretrained(\n    MODEL_ID,\n    trust_remote_code=True,\n    token=hf_token\n)\n\nprint(f\"Qwen2-VL-7B loaded!\")\nprint(f\"GPU memory after: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_pqau_Ct-w"
      },
      "source": [
        "## 3. Load Context Databases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMch-5vdCt-w"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3A: Upload Configuration Files\n",
        "# ============================================================\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
        "STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
        "RAG_INDEX_FILE = \"asme_visual_index.pkl\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Upload Configuration Files\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def locate_file(filename):\n",
        "    if os.path.exists(filename):\n",
        "        return os.path.abspath(filename)\n",
        "    nested_path = os.path.join(\"rag_data\", filename)\n",
        "    if os.path.exists(nested_path):\n",
        "        return os.path.abspath(nested_path)\n",
        "    return None\n",
        "\n",
        "FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
        "STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
        "RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "missing_files = []\n",
        "if not FILE_MAPPING_PATH:\n",
        "    missing_files.append(MAPPING_FILE)\n",
        "if not STRUCTURE_PATH:\n",
        "    missing_files.append(STRUCTURE_FILE)\n",
        "if not RAG_INDEX_PATH:\n",
        "    missing_files.append(RAG_INDEX_FILE)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n",
        "    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded:\n",
        "        if filename.lower().endswith('.zip'):\n",
        "            print(f\"\\nExtracting {filename}...\")\n",
        "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall(\"rag_data\")\n",
        "            print(\"Extraction complete.\")\n",
        "            break\n",
        "\n",
        "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n",
        "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n",
        "    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "if FILE_MAPPING_PATH:\n",
        "    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\n",
        "else:\n",
        "    DATA_DIR = \"/content\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FILE STATUS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"File Mapping:  {'✅ OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else '❌ MISSING'}\")\n",
        "print(f\"Structure:     {'✅ OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else '❌ MISSING'}\")\n",
        "print(f\"RAG Index:     {'✅ OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else '⚠️ MISSING'}\")\n",
        "print(f\"\\nData directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvOVrCT6Ct-w"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3B: Load Part Context Databases\n",
        "# ============================================================\n",
        "\n",
        "def normalize_pn(pn):\n",
        "    \"\"\"Normalize part number for lookup.\"\"\"\n",
        "    return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
        "\n",
        "def load_context_databases():\n",
        "    \"\"\"Load and build all context databases.\"\"\"\n",
        "    print(\"Loading file mapping...\")\n",
        "    with open(FILE_MAPPING_PATH, 'r') as f:\n",
        "        file_mapping_list = json.load(f)\n",
        "\n",
        "    filename_to_pn = {}\n",
        "    for entry in file_mapping_list:\n",
        "        filename = entry['file']\n",
        "        pn = entry['pn']\n",
        "        if pn:\n",
        "            filename_to_pn[filename] = pn\n",
        "            filename_to_pn[filename + '.pdf'] = pn\n",
        "            filename_to_pn[filename + '.PDF'] = pn\n",
        "\n",
        "    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n",
        "\n",
        "    print(\"Loading part structure...\")\n",
        "    with open(STRUCTURE_PATH, 'r') as f:\n",
        "        structure_data = json.load(f)\n",
        "\n",
        "    print(\"Building part context database...\")\n",
        "    part_context_db = {}\n",
        "\n",
        "    for assembly_name, parts_list in structure_data.items():\n",
        "        for part in parts_list:\n",
        "            pn = part['pn']\n",
        "            desc = part['desc']\n",
        "\n",
        "            siblings_list = []\n",
        "            siblings_pns = []\n",
        "\n",
        "            for p_sibling in parts_list:\n",
        "                if p_sibling['pn'] != pn:\n",
        "                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n",
        "                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n",
        "                    siblings_pns.append(p_sibling['pn'])\n",
        "\n",
        "            siblings_str = \"; \".join(siblings_list[:12])\n",
        "            if len(siblings_list) > 12:\n",
        "                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n",
        "\n",
        "            lookup_key = normalize_pn(pn)\n",
        "\n",
        "            part_context_db[lookup_key] = {\n",
        "                'pn': pn,\n",
        "                'description': desc,\n",
        "                'assembly': assembly_name,\n",
        "                'siblings': siblings_str,\n",
        "                'siblings_list': siblings_pns\n",
        "            }\n",
        "            part_context_db[pn] = part_context_db[lookup_key]\n",
        "\n",
        "    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n",
        "    return filename_to_pn, part_context_db\n",
        "\n",
        "filename_to_pn, part_context_db = load_context_databases()\n",
        "print(\"\\n✅ Context databases loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rxvhSvxCt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3E: Load RAG Index & Visual Database\n",
        "# ============================================================\n",
        "import os\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "rag_data = []\n",
        "rag_embeddings = None\n",
        "rag_available = False\n",
        "RAG_IMAGE_DIR = None\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RAG SYSTEM SETUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n[STEP 1/3] Loading CLIP model...\")\n",
        "search_model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"  ✅ CLIP model loaded!\")\n",
        "\n",
        "print(\"\\n[STEP 2/3] Loading RAG Index...\")\n",
        "index_loaded = False\n",
        "\n",
        "# Check multiple locations for the index file\n",
        "index_locations = [\n",
        "    \"/content/asme_visual_index.pkl\",\n",
        "    \"/content/rag_data/asme_visual_index.pkl\",\n",
        "    \"asme_visual_index.pkl\",\n",
        "]\n",
        "if 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH:\n",
        "    index_locations.insert(0, RAG_INDEX_PATH)\n",
        "\n",
        "for idx_path in index_locations:\n",
        "    if idx_path and os.path.exists(idx_path):\n",
        "        print(f\"  ✅ Found: {idx_path}\")\n",
        "        with open(idx_path, 'rb') as f:\n",
        "            rag_data = pickle.load(f)\n",
        "        RAG_INDEX_PATH = idx_path\n",
        "        index_loaded = True\n",
        "        break\n",
        "\n",
        "if not index_loaded:\n",
        "    print(\"  ❌ No index found. Please upload asme_visual_index.pkl:\")\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filename, 'rb') as f:\n",
        "                    rag_data = pickle.load(f)\n",
        "                index_loaded = True\n",
        "                break\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\n[STEP 3/3] Looking for RAG Visual Database...\")\n",
        "\n",
        "# Check multiple locations for the image folder\n",
        "image_locations = [\n",
        "    \"/content/rag_visual_db\",\n",
        "    \"/content/rag_data/rag_visual_db\",\n",
        "    \"rag_visual_db\",\n",
        "]\n",
        "if 'DATA_DIR' in dir() and DATA_DIR:\n",
        "    image_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n",
        "\n",
        "found_images = False\n",
        "for loc in image_locations:\n",
        "    if loc and os.path.exists(loc) and os.path.isdir(loc):\n",
        "        # Count images\n",
        "        img_files = [f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if len(img_files) > 0:\n",
        "            RAG_IMAGE_DIR = os.path.abspath(loc)\n",
        "            found_images = True\n",
        "            print(f\"  ✅ Found: {RAG_IMAGE_DIR} ({len(img_files)} images)\")\n",
        "            break\n",
        "\n",
        "if not found_images:\n",
        "    print(\"  ❌ No images found. Please upload rag_visual_db.zip:\")\n",
        "    from google.colab import files\n",
        "    import zipfile, shutil\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded:\n",
        "            if filename.lower().endswith('.zip'):\n",
        "                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "                if os.path.exists(RAG_IMAGE_DIR):\n",
        "                    shutil.rmtree(RAG_IMAGE_DIR)\n",
        "                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n",
        "                with zipfile.ZipFile(filename, 'r') as zf:\n",
        "                    zf.extractall(RAG_IMAGE_DIR)\n",
        "                found_images = True\n",
        "                print(f\"  ✅ Extracted to {RAG_IMAGE_DIR}\")\n",
        "                break\n",
        "    except:\n",
        "        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "\n",
        "# Build search index\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if index_loaded and len(rag_data) > 0:\n",
        "    embeddings_list = [item['embedding'] for item in rag_data]\n",
        "    rag_embeddings = np.array(embeddings_list).astype('float32')\n",
        "    rag_available = True\n",
        "    print(\"✅ RAG SYSTEM: READY\")\n",
        "    print(f\"  Index: {len(rag_data)} entries\")\n",
        "    print(f\"  Images: {RAG_IMAGE_DIR}\")\n",
        "else:\n",
        "    print(\"❌ RAG SYSTEM: NOT READY\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgprR5vDCt-x"
      },
      "source": [
        "## 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjzPIwxACt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4A: Core Helper Functions\n",
        "# ============================================================\n",
        "import os\n",
        "import re\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def extract_filename_key(filepath):\n",
        "    \"\"\"Extract filename key for lookup.\"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    name_no_ext = os.path.splitext(filename)[0]\n",
        "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)  # Remove (1), (2) etc\n",
        "    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n",
        "    return name_cleaned.strip()\n",
        "\n",
        "def get_part_context(filepath):\n",
        "    \"\"\"Look up part context from filename.\"\"\"\n",
        "    filename_key = extract_filename_key(filepath)\n",
        "\n",
        "    if filename_key in filename_to_pn:\n",
        "        pn = filename_to_pn[filename_key]\n",
        "        lookup_key = normalize_pn(pn)\n",
        "        if lookup_key in part_context_db:\n",
        "            return pn, part_context_db[lookup_key]\n",
        "\n",
        "    for ext in ['.pdf', '.PDF']:\n",
        "        key = filename_key + ext\n",
        "        if key in filename_to_pn:\n",
        "            pn = filename_to_pn[key]\n",
        "            lookup_key = normalize_pn(pn)\n",
        "            if lookup_key in part_context_db:\n",
        "                return pn, part_context_db[lookup_key]\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def build_context_string(pn, context):\n",
        "    \"\"\"Build the context string for inspection prompt.\"\"\"\n",
        "    if context is None:\n",
        "        return \"CONTEXT: Unknown Part (General Syntax Check Only).\"\n",
        "\n",
        "    desc = context.get('description', 'Unknown')\n",
        "    assembly = context.get('assembly', 'Unknown Assembly')\n",
        "    siblings = context.get('siblings', 'None listed')\n",
        "\n",
        "    return f\"\"\"CONTEXT: This is Part {pn} ({desc}).\n",
        "It belongs to the {assembly}.\n",
        "It must assemble with these mating parts: {siblings}.\n",
        "CRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n",
        "\n",
        "def pdf_to_image(pdf_path, dpi=150):\n",
        "    \"\"\"Convert first page of PDF to PIL Image.\"\"\"\n",
        "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n",
        "    return pages[0] if pages else None\n",
        "\n",
        "print(\"✅ Core helper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NCtYUaBCt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4B: Model Query Function\n",
        "# ============================================================\n",
        "import torch\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def query_model(messages, max_tokens=1024):\n",
        "    \"\"\"Send a query to Qwen2-VL and get response.\"\"\"\n",
        "    if 'model' not in globals() or 'processor' not in globals():\n",
        "        raise RuntimeError(\"⚠️ Model not loaded. Run Cell 2 first.\")\n",
        "\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
        "\n",
        "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "    response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return response.strip()\n",
        "\n",
        "print(\"✅ Model query function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlGu3_l1Ct-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4C: RAG Retrieval Function\n",
        "# ============================================================\n",
        "\n",
        "def retrieve_asme_pages(keywords, top_k=2):\n",
        "    \"\"\"Retrieve relevant ASME standard pages based on keywords.\"\"\"\n",
        "    global RAG_IMAGE_DIR\n",
        "\n",
        "    if not rag_available or rag_embeddings is None:\n",
        "        print(\"  ⚠️ RAG system not available\")\n",
        "        return []\n",
        "\n",
        "    if RAG_IMAGE_DIR is None:\n",
        "        print(\"  ⚠️ RAG_IMAGE_DIR not set\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        query_vector = search_model.encode([keywords])\n",
        "        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "        retrieved_images = []\n",
        "        print(f\"  RAG Search: '{keywords[:50]}...'\")\n",
        "\n",
        "        for idx in top_indices:\n",
        "            item = rag_data[idx]\n",
        "            rel_path = item['path'].replace('\\\\', '/')\n",
        "\n",
        "            paths_to_try = [\n",
        "                os.path.join(RAG_IMAGE_DIR, rel_path),\n",
        "                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)),\n",
        "            ]\n",
        "\n",
        "            path_parts = rel_path.split('/')\n",
        "            if len(path_parts) > 1:\n",
        "                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n",
        "\n",
        "            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n",
        "\n",
        "            for try_path in paths_to_try:\n",
        "                if os.path.exists(try_path):\n",
        "                    try:\n",
        "                        img = Image.open(try_path).convert('RGB')\n",
        "                        retrieved_images.append(img)\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error: {e}\")\n",
        "\n",
        "        return retrieved_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  RAG error: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"✅ RAG retrieval function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OpGuvn5Ct-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 4D: Production Pipeline Helpers\n# ============================================================\nimport fitz  # PyMuPDF\nfrom PIL import Image\nimport numpy as np\nfrom typing import List, Tuple\n\nprint(\"Initializing Production Pipeline...\")\n\ndef render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n    \"\"\"Render first page of PDF to PIL Image.\"\"\"\n    try:\n        doc = fitz.open(pdf_path)\n        page = doc.load_page(0)\n        zoom = dpi / 72.0\n        mat = fitz.Matrix(zoom, zoom)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        doc.close()\n        return img\n    except Exception as e:\n        print(f\"Rendering Error: {e}\")\n        return None\n\ndef run_lighton_ocr(img: Image.Image) -> List[str]:\n    \"\"\"Run LightOnOCR-2 on image.\"\"\"\n    return get_drawing_text_ocr(img)\n\n# Alias for backwards compatibility\nrun_tesseract_ocr = run_lighton_ocr\n\ndef make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n    \"\"\"Split image into 4 overlapping quadrants.\"\"\"\n    w, h = full_img.size\n    tile_w, tile_h = w // 2, h // 2\n    overlap = int(min(w, h) * 0.15)\n    boxes = {\n        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n    }\n    return [(name, full_img.crop(box)) for name, box in boxes.items()]\n\nprint(\"Production Pipeline Ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUrqrO0eCt-y"
      },
      "source": [
        "## 5. Main Inspection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVw0CRCBCt-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n# CELL 5B: UNIVERSAL MISMATCH INSPECTOR (Batch-Ready)\n# ============================================================\n\ndef inspect_drawing_universal(pdf_path):\n    \"\"\"\n    Universal inspector that dynamically loads context for each file.\n    Batch-ready with robust error handling.\n    \"\"\"\n    # 1. Get Dynamic Context for THIS specific file\n    pn = None\n    try:\n        pn, ctx = get_part_context(pdf_path)\n        context_str = ctx['siblings']  # Fixed: was 'siblings_str'\n        part_name = ctx['description']  # Fixed: was 'desc'\n    except:\n        print(f\"⚠️ Context Lookup Failed for {pdf_path}\")\n        context_str = \"No Mating Parts Found in Database.\"\n        part_name = \"Unknown Part\"\n\n    print(f\"\\n{'='*60}\\nUNIVERSAL INSPECTION: {part_name}\\n{'='*60}\")\n\n    # --- Phase A: Perception ---\n    print(\"[1/3] Reading Drawing (LightOnOCR-2)...\")\n    full_img = render_pdf_page(pdf_path, dpi=300)\n    if not full_img:\n        return {'error': 'Failed to render PDF', 'part_number': pn}\n\n    # Run Tesseract\n    ocr_texts = run_tesseract_ocr(full_img)\n    # Filter for relevant engineering text (numbers/dims)\n    filtered_ocr = [t for t in ocr_texts if any(char.isdigit() for char in t)]\n    ocr_block = \"\\n\".join([f\"- {t}\" for t in filtered_ocr[:120]])\n\n    print(f\"  > Evidence: {len(filtered_ocr)} relevant lines found.\")\n\n    # --- Phase B: Reasoning ---\n    print(\"[2/3] Generating Dynamic Truth Table...\")\n\n    system_prompt = \"\"\"You are a Universal Engineering Auditor.\n\n**YOUR GOAL:**\nCross-reference \"LIST A\" (Requirements) against \"LIST B\" (Drawing Evidence).\n\n**LOGIC PROTOCOL:**\n1. Read LIST A to identify the Mating Parts.\n2. For EACH Mating Part, search LIST B for a corresponding feature (Thread, Hole, Diameter).\n3. **STRICTLY** compare dimensions.\n   - If List A says \"3/4-16\" and List B says \"M10\", output NO.\n   - If List A says \"0.750\" and List B says \"0.500\", output NO.\n4. If the Mating Part is generic (e.g. \"WASHER\"), and you see *any* washer dimension, you may output \"LIKELY MATCH\".\n5. If you cannot find any matching text in List B, output \"NOT FOUND\".\"\"\"\n\n    user_text = f\"\"\"**LIST A (THE REQUIREMENTS for Part {pn}):**\n{context_str}\n\n**LIST B (THE DRAWING TEXT):**\n{ocr_block}\n\n**TASK:**\nCreate a Truth Table checking the compatibility of the Mating Parts in List A against the Evidence in List B.\n\n| Mating Part (from List A) | Found Feature in List B | Compatible? (YES/NO/NOT FOUND) |\n| :--- | :--- | :--- |\n| [Name/Spec of Part 1] | [Text from Drawing] | [Verdict] |\n| [Name/Spec of Part 2] | [Text from Drawing] | [Verdict] |\n\n**FINAL VERDICT:**\nPASS if all critical features match.\nFAIL if there is a direct contradiction (Metric vs Imperial).\"\"\"\n\n    content_payload = [\n        {'type': 'image', 'image': full_img},\n        {'type': 'text', 'text': user_text}\n    ]\n\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': content_payload}\n    ]\n\n    # Inference\n    print(\"[3/3] Running inference...\")\n    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(\n        text=[text_input],\n        images=[full_img],\n        return_tensors=\"pt\",\n        padding=True\n    ).to(model.device)\n\n    token_count = inputs.input_ids.shape[1]\n    print(f\"  Token count: {token_count}\")\n\n    generated_ids = model.generate(**inputs, max_new_tokens=600)\n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n\n    print(f\"\\n{'='*60}\\nRESULT:\\n{'='*60}\")\n    print(response)\n\n    return {\n        'response': response,\n        'part_number': pn,\n        'part_name': part_name,\n        'ocr_total': len(ocr_texts),\n        'ocr_filtered': len(filtered_ocr),\n        'token_count': token_count\n    }\n\n# Aliases for compatibility\ninspect_drawing_strict_optimized = inspect_drawing_universal\ninspect_drawing_production = inspect_drawing_universal\n\nprint(\"✅ Universal Inspector Loaded (Batch-Ready).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aoMhHjzCt-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5D: MASTER PIPELINE - Full Inspection (Stage 1 + Stage 2)\n",
        "# ============================================================\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def parse_verdict(response_text):\n",
        "    \"\"\"\n",
        "    Parse the truth table response to determine PASS/FAIL.\n",
        "    Returns FAIL if any row has 'NO', PASS if all rows have 'YES'.\n",
        "    \"\"\"\n",
        "    if not response_text:\n",
        "        return 'ERROR', 'No response'\n",
        "\n",
        "    text_upper = response_text.upper()\n",
        "\n",
        "    # Count YES and NO in the response\n",
        "    yes_count = len(re.findall(r'\\|\\s*YES\\s*\\|', text_upper))\n",
        "    no_count = len(re.findall(r'\\|\\s*NO\\s*\\|', text_upper))\n",
        "\n",
        "    # Also check for standalone YES/NO at end of lines\n",
        "    yes_count += len(re.findall(r'YES\\s*$', text_upper, re.MULTILINE))\n",
        "    no_count += len(re.findall(r'NO\\s*$', text_upper, re.MULTILINE))\n",
        "\n",
        "    # Check for NOT FOUND\n",
        "    not_found = 'NOT FOUND' in text_upper\n",
        "\n",
        "    # Determine verdict\n",
        "    if no_count > 0 or not_found:\n",
        "        issues = []\n",
        "        if no_count > 0:\n",
        "            issues.append(f\"{no_count} mismatches\")\n",
        "        if not_found:\n",
        "            issues.append(\"missing specs\")\n",
        "        return 'FAIL', '; '.join(issues)\n",
        "    elif yes_count > 0:\n",
        "        return 'PASS', f\"{yes_count} specs verified\"\n",
        "    else:\n",
        "        return 'REVIEW', 'Could not parse verdict'\n",
        "\n",
        "\n",
        "def run_full_inspection(pdf_path, skip_stage2_on_fail=True):\n",
        "    \"\"\"\n",
        "    Master pipeline that runs both inspection stages:\n",
        "\n",
        "    Stage 1 (Gatekeeper): Strict mismatch detection\n",
        "    Stage 2 (Consultant): Improvement suggestions (only if Stage 1 passes)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"# FULL ENGINEERING INSPECTION PIPELINE\")\n",
        "    print(f\"# File: {os.path.basename(pdf_path)}\")\n",
        "    print(f\"# Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'#'*60}\")\n",
        "\n",
        "    result = {\n",
        "        'file': os.path.basename(pdf_path),\n",
        "        'path': pdf_path,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'stage1': None,\n",
        "        'stage2': None,\n",
        "        'final_verdict': None\n",
        "    }\n",
        "\n",
        "    # STAGE 1: THE GATEKEEPER\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"STAGE 1: THE GATEKEEPER (Mismatch Detection)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    pn, ctx = get_part_context(pdf_path)\n",
        "\n",
        "    if not ctx:\n",
        "        print(f\"⚠️ No context found for {pdf_path}\")\n",
        "        result['stage1'] = {'verdict': 'ERROR', 'reason': 'Part not in database'}\n",
        "        result['final_verdict'] = 'ERROR'\n",
        "        return result\n",
        "\n",
        "    context_data = {\n",
        "        'pn': pn,\n",
        "        'description': ctx.get('description', 'Unknown'),\n",
        "        'assembly': ctx.get('assembly', 'Unknown'),\n",
        "        'siblings': ctx.get('siblings', 'No mating parts')\n",
        "    }\n",
        "\n",
        "    stage1_result = inspect_drawing_universal(pdf_path)\n",
        "\n",
        "    if isinstance(stage1_result, dict) and 'error' in stage1_result:\n",
        "        stage1_verdict = 'ERROR'\n",
        "        stage1_reason = stage1_result['error']\n",
        "        response = ''\n",
        "    else:\n",
        "        response = stage1_result.get('response', '') if isinstance(stage1_result, dict) else str(stage1_result)\n",
        "        stage1_verdict, stage1_reason = parse_verdict(response)\n",
        "\n",
        "    result['stage1'] = {\n",
        "        'verdict': stage1_verdict,\n",
        "        'reason': stage1_reason,\n",
        "        'part_number': pn,\n",
        "        'part_name': context_data['description'],\n",
        "        'ocr_count': stage1_result.get('ocr_filtered', 0) if isinstance(stage1_result, dict) else 0,\n",
        "        'response': response[:1000]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n>>> STAGE 1 VERDICT: {stage1_verdict} - {stage1_reason}\")\n",
        "\n",
        "    # STAGE 2: THE CONSULTANT\n",
        "    if stage1_verdict == 'FAIL' and skip_stage2_on_fail:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: SKIPPED (Stage 1 Failed)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(\"Fix the Stage 1 issues before requesting improvement suggestions.\")\n",
        "        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 failed'}\n",
        "        result['final_verdict'] = 'FAIL'\n",
        "\n",
        "    elif stage1_verdict == 'ERROR':\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: SKIPPED (Stage 1 Error)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 error'}\n",
        "        result['final_verdict'] = 'ERROR'\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: THE CONSULTANT (Proceeding...)\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        ocr_text_list = []\n",
        "        if isinstance(stage1_result, dict):\n",
        "            full_img = render_pdf_page(pdf_path, dpi=200)\n",
        "            if full_img:\n",
        "                ocr_text_list = run_tesseract_ocr(full_img)\n",
        "                ocr_text_list = [t for t in ocr_text_list if any(c.isdigit() for c in t)]\n",
        "\n",
        "        try:\n",
        "            stage2_result = suggest_improvements_stage2(\n",
        "                pdf_path=pdf_path,\n",
        "                context_data=context_data,\n",
        "                ocr_text_list=ocr_text_list\n",
        "            )\n",
        "\n",
        "            result['stage2'] = {\n",
        "                'status': 'COMPLETED',\n",
        "                'suggestions': stage2_result.get('suggestions', ''),\n",
        "                'asme_pages_used': stage2_result.get('asme_pages_used', 0),\n",
        "                'search_queries': stage2_result.get('search_queries', [])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Stage 2 Error: {e}\")\n",
        "            result['stage2'] = {'status': 'ERROR', 'reason': str(e)}\n",
        "\n",
        "        result['final_verdict'] = 'PASS_WITH_SUGGESTIONS' if stage1_verdict == 'PASS' else 'REVIEW'\n",
        "\n",
        "    # FINAL SUMMARY\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(\"# FINAL INSPECTION SUMMARY\")\n",
        "    print(f\"{'#'*60}\")\n",
        "    print(f\"Part: {pn} ({context_data['description']})\")\n",
        "    print(f\"Stage 1 (Gatekeeper): {result['stage1']['verdict']}\")\n",
        "    print(f\"Stage 2 (Consultant): {result['stage2'].get('status', 'N/A')}\")\n",
        "    print(f\"Final Verdict: {result['final_verdict']}\")\n",
        "    print(f\"{'#'*60}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_full_inspection_batch(drawing_folder, output_file=None, limit=None):\n",
        "    \"\"\"Run full inspection on all PDFs in a folder.\"\"\"\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n",
        "    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n",
        "    pdf_files = sorted(list(set(pdf_files)))\n",
        "\n",
        "    if limit:\n",
        "        pdf_files = pdf_files[:limit]\n",
        "\n",
        "    print(f\"{'#'*60}\")\n",
        "    print(f\"FULL INSPECTION BATCH: {len(pdf_files)} files\")\n",
        "    print(f\"{'#'*60}\\n\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for pdf_path in tqdm(pdf_files, desc=\"Full Inspection\"):\n",
        "        try:\n",
        "            result = run_full_inspection(pdf_path)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            results.append({'file': os.path.basename(pdf_path), 'final_verdict': 'ERROR', 'error': str(e)})\n",
        "\n",
        "    if not output_file:\n",
        "        output_file = f\"full_inspection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    verdicts = [r.get('final_verdict', 'ERROR') for r in results]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BATCH SUMMARY: {len(results)} files\")\n",
        "    print(f\"PASS: {verdicts.count('PASS_WITH_SUGGESTIONS')} | FAIL: {verdicts.count('FAIL')} | ERROR: {verdicts.count('ERROR')}\")\n",
        "    print(f\"Results: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"✅ parse_verdict() defined.\")\n",
        "print(\"✅ run_full_inspection() defined.\")\n",
        "print(\"✅ run_full_inspection_batch() defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0MRiihCt-y"
      },
      "source": [
        "## 6. Inspect a Drawing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEJbc1XtCt-y"
      },
      "outputs": [],
      "source": [
        "# Upload and inspect a single PDF drawing\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload a PDF drawing to inspect:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "    print(f\"\n",
        "Inspecting: {pdf_path}\")\n",
        "    result = run_full_inspection(pdf_path)\n",
        "\n",
        "    # Display result summary\n",
        "    print(\"\n",
        "\" + \"=\"*60)\n",
        "    print(\"INSPECTION COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    if 'verdict' in result:\n",
        "        print(f\"Verdict: {result['verdict']}\")\n",
        "    if 'reason' in result:\n",
        "        print(f\"Reason: {result['reason']}\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n"
      ]
    }
  ]
}