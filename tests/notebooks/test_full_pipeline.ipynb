{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Full Pipeline: End-to-End (M11)\n",
        "\n",
        "This notebook runs the complete YOLO-OBB pipeline end-to-end:\n",
        "\n",
        "```\n",
        "image -> detect -> crop -> rotate+OCR -> parse -> normalize -> validate -> expand -> match -> score\n",
        "```\n",
        "\n",
        "Uses `YOLOPipeline` which orchestrates all 10 stages in a single `run()` call.\n",
        "\n",
        "**Runtime requirement:** GPU with at least 8 GB VRAM (A100 preferred).\n",
        "\n",
        "**Required files on Google Drive:**\n",
        "- YOLO-OBB model weights (`best.pt`)\n",
        "- Sample page image (PNG/JPG)\n",
        "- SolidWorks JSON export\n",
        "- HuggingFace token for LightOnOCR-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Install all dependencies\n",
        "# NOTE: Set your runtime to GPU before running (Runtime > Change runtime type > A100)\n",
        "%pip install ultralytics transformers torch pillow accelerate --quiet\n",
        "\n",
        "# Clone the repo\n",
        "!git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git /content/AI-Drawing-Inspector 2>/dev/null || \\\n",
        "    (cd /content/AI-Drawing-Inspector && git pull)\n",
        "\n",
        "print('Dependencies installed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Mount Drive, set all paths\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "sys.path.insert(0, '/content/AI-Drawing-Inspector')\n",
        "\n",
        "# ---- HuggingFace token ----\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "except Exception:\n",
        "    HF_TOKEN = None\n",
        "# HF_TOKEN = 'hf_YOUR_TOKEN_HERE'  # Uncomment for manual entry\n",
        "\n",
        "if HF_TOKEN:\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(f'HF_TOKEN set (length={len(HF_TOKEN)})')\n",
        "else:\n",
        "    print('WARNING: HF_TOKEN not set.')\n",
        "\n",
        "# ---- Paths (edit to match your Drive layout) ----\n",
        "MODEL_PATH = '/content/drive/MyDrive/ai_inspector_models/best.pt'\n",
        "SAMPLE_IMAGE = '/content/drive/MyDrive/ai_inspector_data/sample_pages/test_page.png'\n",
        "SW_JSON_PATH = '/content/drive/MyDrive/ai_inspector_data/sw_json/test_part.json'\n",
        "OUTPUT_DIR = '/content/debug/pipeline_run'\n",
        "\n",
        "# Title block text (paste from OCR or manual inspection)\n",
        "TITLE_BLOCK_TEXT = 'UNLESS OTHERWISE SPECIFIED DIMENSIONS ARE IN INCHES'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f'Model:       {MODEL_PATH}  exists={os.path.exists(MODEL_PATH)}')\n",
        "print(f'Image:       {SAMPLE_IMAGE}  exists={os.path.exists(SAMPLE_IMAGE)}')\n",
        "print(f'SW JSON:     {SW_JSON_PATH}  exists={os.path.exists(SW_JSON_PATH)}')\n",
        "print(f'Output dir:  {OUTPUT_DIR}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Import YOLOPipeline\n",
        "from ai_inspector.pipeline.yolo_pipeline import YOLOPipeline, PipelineResult\n",
        "\n",
        "print('YOLOPipeline imported successfully.')\n",
        "print(f'PipelineResult fields: {[f.name for f in PipelineResult.__dataclass_fields__.values()]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Create and load pipeline (show GPU memory usage)\n",
        "import torch\n",
        "\n",
        "def gpu_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
        "        return f'{allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.1f} GB total'\n",
        "    return 'No CUDA'\n",
        "\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')\n",
        "print(f'Before load: {gpu_mem()}')\n",
        "\n",
        "pipeline = YOLOPipeline(\n",
        "    model_path=MODEL_PATH,\n",
        "    hf_token=HF_TOKEN,\n",
        "    confidence_threshold=0.25,\n",
        "    device='cuda',\n",
        ")\n",
        "\n",
        "pipeline.load()\n",
        "print(f'Pipeline loaded: {pipeline.is_loaded}')\n",
        "print(f'After load:  {gpu_mem()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Run pipeline on a test page\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "result = pipeline.run(\n",
        "    image_path=SAMPLE_IMAGE,\n",
        "    sw_json_path=SW_JSON_PATH,\n",
        "    title_block_text=TITLE_BLOCK_TEXT,\n",
        "    page_id='test_page_0',\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    save_crops=True,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f'Pipeline completed in {elapsed:.1f}s')\n",
        "print(f'Packets: {len(result.packets)}')\n",
        "print(f'Match results: {len(result.match_results)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Display results: scores, expansion summary, validation stats\n",
        "import json\n",
        "\n",
        "print('=== SCORES ===')\n",
        "for key, val in result.scores.items():\n",
        "    print(f'  {key:25s}: {val}')\n",
        "\n",
        "print('\\n=== EXPANSION SUMMARY ===')\n",
        "print(json.dumps(result.expansion_summary, indent=2))\n",
        "\n",
        "print('\\n=== VALIDATION STATS ===')\n",
        "print(json.dumps(result.validation_stats, indent=2))\n",
        "\n",
        "print('\\n=== PACKET SUMMARY ===')\n",
        "print(json.dumps(result.packet_summary, indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Display match results table\n",
        "from ai_inspector.comparison.matcher import MatchStatus\n",
        "\n",
        "print(f'{\"#\":>3s} {\"Status\":15s} {\"Type\":18s} {\"Delta\":>10s} {\"Notes\"}')\n",
        "print('=' * 90)\n",
        "\n",
        "for i, r in enumerate(result.match_results):\n",
        "    # Determine type from whichever side is present\n",
        "    callout_type = ''\n",
        "    if r.drawing_callout:\n",
        "        callout_type = r.drawing_callout.get('calloutType', '')\n",
        "    elif r.sw_feature:\n",
        "        callout_type = r.sw_feature.feature_type\n",
        "\n",
        "    delta_str = f'{r.delta:+.4f}' if r.delta is not None else 'N/A'\n",
        "\n",
        "    # Color coding via emoji-free markers\n",
        "    status_marker = {\n",
        "        MatchStatus.MATCHED: '[OK]',\n",
        "        MatchStatus.MISSING: '[MISS]',\n",
        "        MatchStatus.EXTRA: '[EXTRA]',\n",
        "        MatchStatus.TOLERANCE_FAIL: '[TOL]',\n",
        "        MatchStatus.SKIPPED: '[SKIP]',\n",
        "    }.get(r.status, '[?]')\n",
        "\n",
        "    print(f'{i:3d} {status_marker + \" \" + r.status.value:15s} '\n",
        "          f'{callout_type:18s} {delta_str:>10s} {r.notes[:50]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: Display packet provenance for first 5 detections\n",
        "from ai_inspector.schemas.callout_packet import packet_to_dict\n",
        "\n",
        "print('=== Packet Provenance (first 5) ===')\n",
        "print()\n",
        "\n",
        "for i, pkt in enumerate(result.packets[:5]):\n",
        "    print(f'--- Packet {i}: {pkt.det_id} ---')\n",
        "\n",
        "    # Detection\n",
        "    if pkt.detection:\n",
        "        print(f'  Detection: class={pkt.detection.class_name}, '\n",
        "              f'conf={pkt.detection.confidence:.3f}')\n",
        "\n",
        "    # Crop\n",
        "    if pkt.crop:\n",
        "        meta = pkt.crop.meta\n",
        "        print(f'  Crop: {meta.get(\"crop_w\", \"?\")}x{meta.get(\"crop_h\", \"?\")}px, '\n",
        "              f'angle={meta.get(\"rotation_angle\", 0):.1f}deg')\n",
        "\n",
        "    # Rotation\n",
        "    if pkt.rotation:\n",
        "        print(f'  Rotation: {pkt.rotation.rotation_used}deg, '\n",
        "              f'quality={pkt.rotation.quality_score:.2f}')\n",
        "\n",
        "    # Reader\n",
        "    if pkt.reader:\n",
        "        print(f'  Reader: type={pkt.reader.callout_type}, '\n",
        "              f'source={pkt.reader.source}, '\n",
        "              f'ocr_conf={pkt.reader.ocr_confidence:.2f}')\n",
        "        print(f'  Raw: \"{pkt.reader.raw[:60]}\"')\n",
        "        parsed_keys = [k for k in pkt.reader.parsed.keys() if not k.startswith('_')]\n",
        "        print(f'  Parsed fields: {parsed_keys}')\n",
        "\n",
        "    # Normalization\n",
        "    if pkt.normalized:\n",
        "        method = pkt.normalized.get('_normalization_method', '?')\n",
        "        units = pkt.normalized.get('_detected_units', '?')\n",
        "        print(f'  Normalization: method={method}, detected_units={units}')\n",
        "\n",
        "    # Validation\n",
        "    print(f'  Validated: {pkt.validated}'\n",
        "          + (f', error=\"{pkt.validation_error}\"' if pkt.validation_error else ''))\n",
        "\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 9: Save all artifacts to output dir\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "out = Path(OUTPUT_DIR)\n",
        "print(f'Artifacts saved to: {OUTPUT_DIR}/')\n",
        "print()\n",
        "\n",
        "# List saved files\n",
        "for f in sorted(out.rglob('*')):\n",
        "    if f.is_file():\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f'  {f.relative_to(out)}  ({size_kb:.1f} KB)')\n",
        "\n",
        "# Save the full result dict as well\n",
        "result_dict = result.to_dict()\n",
        "summary_path = out / 'pipeline_summary.json'\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(result_dict, f, indent=2, ensure_ascii=False)\n",
        "print(f'\\nPipeline summary saved to: {summary_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 10: Unload pipeline, show memory freed\n",
        "import torch\n",
        "\n",
        "def gpu_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
        "        return f'{allocated:.2f} GB allocated / {reserved:.2f} GB reserved / {total:.1f} GB total'\n",
        "    return 'No CUDA'\n",
        "\n",
        "print(f'Before unload: {gpu_mem()}')\n",
        "\n",
        "pipeline.unload()\n",
        "\n",
        "# Force garbage collection\n",
        "import gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'After unload:  {gpu_mem()}')\n",
        "print(f'Pipeline loaded: {pipeline.is_loaded}')\n",
        "print('\\nDone. Pipeline unloaded and memory freed.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}