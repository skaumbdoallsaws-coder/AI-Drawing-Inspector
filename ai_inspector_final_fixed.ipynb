{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f2da10a7b3d40c789b105b65dee12ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fe0984995be49f9bd7a67bf5d71b53c",
              "IPY_MODEL_5eb0ee8482c942f0a72d2a3c90962b19",
              "IPY_MODEL_4b191a0f669347a2aabd5b0659fb712c"
            ],
            "layout": "IPY_MODEL_e99239bb839d42a0a5ab004ffffd5f69"
          }
        },
        "5fe0984995be49f9bd7a67bf5d71b53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6ab057d83f476ba35767ead92cc7b2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b3cb7d653ccc4bd4a8b585098e030d9c",
            "value": "Downloadâ€‡complete:â€‡"
          }
        },
        "5eb0ee8482c942f0a72d2a3c90962b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7858c7b198eb493c8dbed5d2584a23fa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc6c93f7ff384911aa905c0a54fb3378",
            "value": 0
          }
        },
        "4b191a0f669347a2aabd5b0659fb712c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d92d9b0f62ce4f80a36440586c4e083f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_838a1091053a4f419f5c3feb4a928d45",
            "value": "â€‡0.00/0.00â€‡[00:00&lt;?,â€‡?B/s]"
          }
        },
        "e99239bb839d42a0a5ab004ffffd5f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6ab057d83f476ba35767ead92cc7b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3cb7d653ccc4bd4a8b585098e030d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7858c7b198eb493c8dbed5d2584a23fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fc6c93f7ff384911aa905c0a54fb3378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d92d9b0f62ce4f80a36440586c4e083f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "838a1091053a4f419f5c3feb4a928d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03041b6920334683b60f2c4c11c8ad69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bca44aa036154f2abdd9e97abac0da04",
              "IPY_MODEL_3425e86e088a4e388ad2f835117922ff",
              "IPY_MODEL_f3050eaaa1b64e48a149feaac5930071"
            ],
            "layout": "IPY_MODEL_3f2a6388e1f04677be80fdf1e1310413"
          }
        },
        "bca44aa036154f2abdd9e97abac0da04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c454416e4ae54a75878fead6e89e8483",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_084c7397d04346d88c513bd0cc1acd69",
            "value": "Fetchingâ€‡38â€‡files:â€‡100%"
          }
        },
        "3425e86e088a4e388ad2f835117922ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36274a66729e4ef6add6d01ae3ef189a",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8c324841f7048c3ae0afc4465c6c64d",
            "value": 38
          }
        },
        "f3050eaaa1b64e48a149feaac5930071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29806971c88f4564bf88b86a3b9c6440",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2d36f22c05a94b25b8ef9b4005ea1ca2",
            "value": "â€‡38/38â€‡[00:00&lt;00:00,â€‡3597.09it/s]"
          }
        },
        "3f2a6388e1f04677be80fdf1e1310413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c454416e4ae54a75878fead6e89e8483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084c7397d04346d88c513bd0cc1acd69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36274a66729e4ef6add6d01ae3ef189a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8c324841f7048c3ae0afc4465c6c64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29806971c88f4564bf88b86a3b9c6440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d36f22c05a94b25b8ef9b4005ea1ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8651db820c88436383de26c80d77237b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79f8ff413d96452181c1fe2c19ba00f0",
              "IPY_MODEL_9ac98884cf764763a1421ca515d8fad0",
              "IPY_MODEL_7b2313def8294db89b578e5c80db1e6d"
            ],
            "layout": "IPY_MODEL_4c95abae637245978dbd01ab5438fd16"
          }
        },
        "79f8ff413d96452181c1fe2c19ba00f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_580b02f7376e423dade17dfdee6bb14a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_134d2b6bc7cd40a9b2ce144156a0a019",
            "value": "Loadingâ€‡weights:â€‡â€‡42%"
          }
        },
        "9ac98884cf764763a1421ca515d8fad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7213dd9027414aa0bbbd52aaabf816",
            "max": 1354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f60eff3490434a26addf324839e28cc5",
            "value": 568
          }
        },
        "7b2313def8294db89b578e5c80db1e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a160bcca4a984ebf938d3f4ee644bfa4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef5b77f22d3f4822a6207a6b98caba54",
            "value": "â€‡568/1354â€‡[00:30&lt;00:01,â€‡417.90it/s,â€‡Materializingâ€‡param=model.language_model.layers.47.mlp.down_proj.weight]"
          }
        },
        "4c95abae637245978dbd01ab5438fd16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "580b02f7376e423dade17dfdee6bb14a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134d2b6bc7cd40a9b2ce144156a0a019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a7213dd9027414aa0bbbd52aaabf816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60eff3490434a26addf324839e28cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a160bcca4a984ebf938d3f4ee644bfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5b77f22d3f4822a6207a6b98caba54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MkNLQ8UCt-u"
      },
      "source": [
        "# AI Engineering Drawing Inspector (Single File)\n",
        "\n",
        "Streamlined inspector using:\n",
        "- **Qwen2-VL-72B** for visual reasoning\n",
        "- **LightOnOCR-2** for structured text extraction (replaces Tesseract)\n",
        "- **RAG** retrieval from ASME Y14.5 standard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ZVsEFCCt-u"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lDC-SvrCt-v",
        "outputId": "1ee81f0f-83b6-4269-ec06-8bd519eabc0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "All packages installed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 1A: Install Dependencies\n",
        "# ============================================================\n",
        "# Core ML dependencies\n",
        "!pip install -q accelerate\n",
        "!pip install -q qwen-vl-utils\n",
        "!pip install -q pdf2image\n",
        "!pip install -q faiss-cpu sentence-transformers\n",
        "!pip install -q bitsandbytes\n",
        "!apt-get install -y poppler-utils > /dev/null 2>&1\n",
        "\n",
        "# Production Pipeline Dependencies\n",
        "!pip install -q pymupdf opencv-python-headless\n",
        "\n",
        "# LightOnOCR-2 (VLM-based OCR - better than Tesseract for complex layouts)\n",
        "# Requires transformers from source (not yet in stable release)\n",
        "!pip install -q git+https://github.com/huggingface/transformers\n",
        "!pip install -q pillow pypdfium2\n",
        "\n",
        "print(\"All packages installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsRtdoY0Ct-v",
        "outputId": "046a403e-ee36-46a3-8907-58a95d4afd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 1B: Import Libraries\n",
        "# ============================================================\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Production Pipeline Imports\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMeLfbuMCt-v"
      },
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772,
          "referenced_widgets": [
            "0f2da10a7b3d40c789b105b65dee12ee",
            "5fe0984995be49f9bd7a67bf5d71b53c",
            "5eb0ee8482c942f0a72d2a3c90962b19",
            "4b191a0f669347a2aabd5b0659fb712c",
            "e99239bb839d42a0a5ab004ffffd5f69",
            "9a6ab057d83f476ba35767ead92cc7b2",
            "b3cb7d653ccc4bd4a8b585098e030d9c",
            "7858c7b198eb493c8dbed5d2584a23fa",
            "fc6c93f7ff384911aa905c0a54fb3378",
            "d92d9b0f62ce4f80a36440586c4e083f",
            "838a1091053a4f419f5c3feb4a928d45",
            "03041b6920334683b60f2c4c11c8ad69",
            "bca44aa036154f2abdd9e97abac0da04",
            "3425e86e088a4e388ad2f835117922ff",
            "f3050eaaa1b64e48a149feaac5930071",
            "3f2a6388e1f04677be80fdf1e1310413",
            "c454416e4ae54a75878fead6e89e8483",
            "084c7397d04346d88c513bd0cc1acd69",
            "36274a66729e4ef6add6d01ae3ef189a",
            "c8c324841f7048c3ae0afc4465c6c64d",
            "29806971c88f4564bf88b86a3b9c6440",
            "2d36f22c05a94b25b8ef9b4005ea1ca2",
            "8651db820c88436383de26c80d77237b",
            "79f8ff413d96452181c1fe2c19ba00f0",
            "9ac98884cf764763a1421ca515d8fad0",
            "7b2313def8294db89b578e5c80db1e6d",
            "4c95abae637245978dbd01ab5438fd16",
            "580b02f7376e423dade17dfdee6bb14a",
            "134d2b6bc7cd40a9b2ce144156a0a019",
            "1a7213dd9027414aa0bbbd52aaabf816",
            "f60eff3490434a26addf324839e28cc5",
            "a160bcca4a984ebf938d3f4ee644bfa4",
            "ef5b77f22d3f4822a6207a6b98caba54"
          ]
        },
        "id": "ZDxMZNRfCt-w",
        "outputId": "2572e8fa-ea5f-4a2a-f61a-34a2ae00dc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¹ Cleaning up GPU memory...\n",
            "GPU Memory Status: 0.00 GB allocated\n",
            "\n",
            "ðŸ”‘ Token detected: hf_*******************************IsJ (Length: 37)\n",
            "âœ… Logged in to Hugging Face Hub\n",
            "\n",
            "Loading Qwen/Qwen2-VL-72B-Instruct in 4-bit (NF4)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f2da10a7b3d40c789b105b65dee12ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 38 files:   0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03041b6920334683b60f2c4c11c8ad69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/1354 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8651db820c88436383de26c80d77237b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ›‘ CUDA OUT OF MEMORY\n",
            "The GPU is full. Please try: Runtime > Restart Session, then run the cells again.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 353.88 MiB is free. Process 112515 has 78.96 GiB memory in use. Of the allocated memory 78.43 GiB is allocated by PyTorch, and 51.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1268733904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nðŸ›‘ CUDA OUT OF MEMORY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The GPU is full. Please try: Runtime > Restart Session, then run the cells again.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1268733904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     model = Qwen2VLForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m         model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(\n\u001b[0m\u001b[1;32m   4043\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_buffers, dtype, hf_quantizer, device_mesh, weights_only, weight_mapping)\u001b[0m\n\u001b[1;32m   4184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m             missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n\u001b[0;32m-> 4186\u001b[0;31m                 convert_and_load_state_dict_in_model(\n\u001b[0m\u001b[1;32m   4187\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4188\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert_and_load_state_dict_in_model\u001b[0;34m(model, state_dict, weight_mapping, tp_plan, hf_quantizer, dtype, device_map, dtype_plan, device_mesh, disk_offload_index, disk_offload_folder, offload_buffers)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                     realized_value, conversion_errors = mapping.convert(\n\u001b[0m\u001b[1;32m   1105\u001b[0m                         \u001b[0mfirst_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, layer_name, model, config, hf_quantizer, missing_keys, conversion_errors)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# attribute during the whole process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mcollected_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mmaterialize_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;31m# Async loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Sync loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36m_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_materialize_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthread_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36m_materialize_copy\u001b[0;34m(tensor, device, dtype)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 353.88 MiB is free. Process 112515 has 78.96 GiB memory in use. Of the allocated memory 78.43 GiB is allocated by PyTorch, and 51.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 2: Load Qwen2-VL Model (4-bit Quantized)\n",
        "# ============================================================\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "# --- AGGRESSIVE MEMORY CLEANUP ---\n",
        "print(\"ðŸ§¹ Cleaning up GPU memory...\")\n",
        "# Clear exception hooks that might hold references to old variables\n",
        "if hasattr(sys, 'last_traceback'):\n",
        "    del sys.last_traceback\n",
        "if hasattr(sys, 'last_value'):\n",
        "    del sys.last_value\n",
        "if hasattr(sys, 'last_type'):\n",
        "    del sys.last_type\n",
        "\n",
        "# Delete variables if they exist\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "if 'processor' in globals():\n",
        "    del processor\n",
        "\n",
        "# Force garbage collection multiple times\n",
        "for _ in range(3):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "allocated_gb = torch.cuda.memory_allocated() / 1e9\n",
        "print(f\"GPU Memory Status: {allocated_gb:.2f} GB allocated\")\n",
        "\n",
        "if allocated_gb > 2.0:\n",
        "    print(\"\\nâš ï¸ CRITICAL WARNING: GPU memory is not empty ({} GB in use).\".format(allocated_gb))\n",
        "    print(\"If this cell crashes, you MUST restart the runtime:\")\n",
        "    print(\"ðŸ‘‰ Click 'Runtime' > 'Restart Session' in the top menu, then run the cells again.\")\n",
        "# ------------------------------\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-72B-Instruct\"\n",
        "\n",
        "# --- AUTHENTICATION DEBUG ---\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        clean_token = hf_token.strip()\n",
        "        # Show masked token for verification\n",
        "        print(f\"\\nðŸ”‘ Token detected: {clean_token[:3]}{'*' * (len(clean_token)-6)}{clean_token[-3:]} (Length: {len(clean_token)})\")\n",
        "        login(token=clean_token)\n",
        "        print(\"âœ… Logged in to Hugging Face Hub\")\n",
        "    else:\n",
        "        print(\"âš ï¸ HF_TOKEN secret not found. Proceeding with anonymous access.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Authentication failed: {e}\")\n",
        "    print(\"Tip: Ensure your token in 'Secrets' is a 'Write' token from huggingface.co/settings/tokens\")\n",
        "# --------------------------\n",
        "\n",
        "print(f\"\\nLoading {MODEL_ID} in 4-bit (NF4)...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        attn_implementation=\"sdpa\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "    print(\"âœ… Qwen2-VL-72B (4-bit) Loaded Successfully!\")\n",
        "    print(f\"Memory Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(\"\\nðŸ›‘ CUDA OUT OF MEMORY\")\n",
        "        print(\"The GPU is full. Please try: Runtime > Restart Session, then run the cells again.\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_pqau_Ct-w"
      },
      "source": [
        "## 3. Load Context Databases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMch-5vdCt-w"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3A: Upload Configuration Files\n",
        "# ============================================================\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "MAPPING_FILE = \"400S_file_part_mapping.json\"\n",
        "STRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\n",
        "RAG_INDEX_FILE = \"asme_visual_index.pkl\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STEP 1: Upload Configuration Files\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def locate_file(filename):\n",
        "    if os.path.exists(filename):\n",
        "        return os.path.abspath(filename)\n",
        "    nested_path = os.path.join(\"rag_data\", filename)\n",
        "    if os.path.exists(nested_path):\n",
        "        return os.path.abspath(nested_path)\n",
        "    return None\n",
        "\n",
        "FILE_MAPPING_PATH = locate_file(MAPPING_FILE)\n",
        "STRUCTURE_PATH = locate_file(STRUCTURE_FILE)\n",
        "RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "missing_files = []\n",
        "if not FILE_MAPPING_PATH:\n",
        "    missing_files.append(MAPPING_FILE)\n",
        "if not STRUCTURE_PATH:\n",
        "    missing_files.append(STRUCTURE_FILE)\n",
        "if not RAG_INDEX_PATH:\n",
        "    missing_files.append(RAG_INDEX_FILE)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n",
        "    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded:\n",
        "        if filename.lower().endswith('.zip'):\n",
        "            print(f\"\\nExtracting {filename}...\")\n",
        "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall(\"rag_data\")\n",
        "            print(\"Extraction complete.\")\n",
        "            break\n",
        "\n",
        "    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n",
        "    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n",
        "    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n",
        "\n",
        "if FILE_MAPPING_PATH:\n",
        "    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\n",
        "else:\n",
        "    DATA_DIR = \"/content\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FILE STATUS:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"File Mapping:  {'âœ… OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else 'âŒ MISSING'}\")\n",
        "print(f\"Structure:     {'âœ… OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else 'âŒ MISSING'}\")\n",
        "print(f\"RAG Index:     {'âœ… OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else 'âš ï¸ MISSING'}\")\n",
        "print(f\"\\nData directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvOVrCT6Ct-w"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3B: Load Part Context Databases\n",
        "# ============================================================\n",
        "\n",
        "def normalize_pn(pn):\n",
        "    \"\"\"Normalize part number for lookup.\"\"\"\n",
        "    return re.sub(r'[-\\s]', '', str(pn)).lower()\n",
        "\n",
        "def load_context_databases():\n",
        "    \"\"\"Load and build all context databases.\"\"\"\n",
        "    print(\"Loading file mapping...\")\n",
        "    with open(FILE_MAPPING_PATH, 'r') as f:\n",
        "        file_mapping_list = json.load(f)\n",
        "\n",
        "    filename_to_pn = {}\n",
        "    for entry in file_mapping_list:\n",
        "        filename = entry['file']\n",
        "        pn = entry['pn']\n",
        "        if pn:\n",
        "            filename_to_pn[filename] = pn\n",
        "            filename_to_pn[filename + '.pdf'] = pn\n",
        "            filename_to_pn[filename + '.PDF'] = pn\n",
        "\n",
        "    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n",
        "\n",
        "    print(\"Loading part structure...\")\n",
        "    with open(STRUCTURE_PATH, 'r') as f:\n",
        "        structure_data = json.load(f)\n",
        "\n",
        "    print(\"Building part context database...\")\n",
        "    part_context_db = {}\n",
        "\n",
        "    for assembly_name, parts_list in structure_data.items():\n",
        "        for part in parts_list:\n",
        "            pn = part['pn']\n",
        "            desc = part['desc']\n",
        "\n",
        "            siblings_list = []\n",
        "            siblings_pns = []\n",
        "\n",
        "            for p_sibling in parts_list:\n",
        "                if p_sibling['pn'] != pn:\n",
        "                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n",
        "                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n",
        "                    siblings_pns.append(p_sibling['pn'])\n",
        "\n",
        "            siblings_str = \"; \".join(siblings_list[:12])\n",
        "            if len(siblings_list) > 12:\n",
        "                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n",
        "\n",
        "            lookup_key = normalize_pn(pn)\n",
        "\n",
        "            part_context_db[lookup_key] = {\n",
        "                'pn': pn,\n",
        "                'description': desc,\n",
        "                'assembly': assembly_name,\n",
        "                'siblings': siblings_str,\n",
        "                'siblings_list': siblings_pns\n",
        "            }\n",
        "            part_context_db[pn] = part_context_db[lookup_key]\n",
        "\n",
        "    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n",
        "    return filename_to_pn, part_context_db\n",
        "\n",
        "filename_to_pn, part_context_db = load_context_databases()\n",
        "print(\"\\nâœ… Context databases loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJjEH2yzCt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3C: Initialize LightOnOCR-2 (VLM-based OCR)\n",
        "# ============================================================\n",
        "import torch\n",
        "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading LightOnOCR-2-1B (VLM-based OCR)...\")\n",
        "\n",
        "# Determine device and dtype\n",
        "ocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n",
        "\n",
        "# Load LightOnOCR-2 model\n",
        "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
        "    \"lightonai/LightOnOCR-2-1B\",\n",
        "    torch_dtype=ocr_dtype\n",
        ").to(ocr_device)\n",
        "\n",
        "ocr_processor = LightOnOcrProcessor.from_pretrained(\"lightonai/LightOnOCR-2-1B\")\n",
        "\n",
        "print(f\"LightOnOCR-2 loaded on {ocr_device} ({ocr_dtype})\")\n",
        "print(f\"Memory: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "\n",
        "def get_drawing_text_ocr(image_input):\n",
        "    \"\"\"\n",
        "    Runs LightOnOCR-2 on the drawing and returns structured text.\n",
        "\n",
        "    LightOnOCR-2 is a Vision-Language Model that understands document structure,\n",
        "    outputting clean Markdown with tables and headers - ideal for engineering drawings.\n",
        "\n",
        "    Args:\n",
        "        image_input: PIL Image or numpy array\n",
        "\n",
        "    Returns:\n",
        "        List of text strings found (split by newlines)\n",
        "    \"\"\"\n",
        "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
        "\n",
        "    try:\n",
        "        # Convert numpy array to PIL Image if needed\n",
        "        if isinstance(image_input, np.ndarray):\n",
        "            img = Image.fromarray(image_input).convert(\"RGB\")\n",
        "        else:\n",
        "            img = image_input.convert(\"RGB\")\n",
        "\n",
        "        # Prepare conversation for LightOnOCR\n",
        "        conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n",
        "\n",
        "        inputs = ocr_processor.apply_chat_template(\n",
        "            conversation,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate OCR output\n",
        "        with torch.no_grad():\n",
        "            output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
        "\n",
        "        generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
        "        output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Split into lines and filter empty\n",
        "        lines = [line.strip() for line in output_text.split(\"\n",
        "\") if line.strip()]\n",
        "\n",
        "        return lines\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LightOnOCR Error: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"LightOnOCR-2 Engine Ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rxvhSvxCt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3E: Load RAG Index & Visual Database\n",
        "# ============================================================\n",
        "import os\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "rag_data = []\n",
        "rag_embeddings = None\n",
        "rag_available = False\n",
        "RAG_IMAGE_DIR = None\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RAG SYSTEM SETUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n[STEP 1/3] Loading CLIP model...\")\n",
        "search_model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"  âœ… CLIP model loaded!\")\n",
        "\n",
        "print(\"\\n[STEP 2/3] Loading RAG Index...\")\n",
        "index_loaded = False\n",
        "\n",
        "# Check multiple locations for the index file\n",
        "index_locations = [\n",
        "    \"/content/asme_visual_index.pkl\",\n",
        "    \"/content/rag_data/asme_visual_index.pkl\",\n",
        "    \"asme_visual_index.pkl\",\n",
        "]\n",
        "if 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH:\n",
        "    index_locations.insert(0, RAG_INDEX_PATH)\n",
        "\n",
        "for idx_path in index_locations:\n",
        "    if idx_path and os.path.exists(idx_path):\n",
        "        print(f\"  âœ… Found: {idx_path}\")\n",
        "        with open(idx_path, 'rb') as f:\n",
        "            rag_data = pickle.load(f)\n",
        "        RAG_INDEX_PATH = idx_path\n",
        "        index_loaded = True\n",
        "        break\n",
        "\n",
        "if not index_loaded:\n",
        "    print(\"  âŒ No index found. Please upload asme_visual_index.pkl:\")\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filename, 'rb') as f:\n",
        "                    rag_data = pickle.load(f)\n",
        "                index_loaded = True\n",
        "                break\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\n[STEP 3/3] Looking for RAG Visual Database...\")\n",
        "\n",
        "# Check multiple locations for the image folder\n",
        "image_locations = [\n",
        "    \"/content/rag_visual_db\",\n",
        "    \"/content/rag_data/rag_visual_db\",\n",
        "    \"rag_visual_db\",\n",
        "]\n",
        "if 'DATA_DIR' in dir() and DATA_DIR:\n",
        "    image_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n",
        "\n",
        "found_images = False\n",
        "for loc in image_locations:\n",
        "    if loc and os.path.exists(loc) and os.path.isdir(loc):\n",
        "        # Count images\n",
        "        img_files = [f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if len(img_files) > 0:\n",
        "            RAG_IMAGE_DIR = os.path.abspath(loc)\n",
        "            found_images = True\n",
        "            print(f\"  âœ… Found: {RAG_IMAGE_DIR} ({len(img_files)} images)\")\n",
        "            break\n",
        "\n",
        "if not found_images:\n",
        "    print(\"  âŒ No images found. Please upload rag_visual_db.zip:\")\n",
        "    from google.colab import files\n",
        "    import zipfile, shutil\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded:\n",
        "            if filename.lower().endswith('.zip'):\n",
        "                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "                if os.path.exists(RAG_IMAGE_DIR):\n",
        "                    shutil.rmtree(RAG_IMAGE_DIR)\n",
        "                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n",
        "                with zipfile.ZipFile(filename, 'r') as zf:\n",
        "                    zf.extractall(RAG_IMAGE_DIR)\n",
        "                found_images = True\n",
        "                print(f\"  âœ… Extracted to {RAG_IMAGE_DIR}\")\n",
        "                break\n",
        "    except:\n",
        "        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n",
        "\n",
        "# Build search index\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if index_loaded and len(rag_data) > 0:\n",
        "    embeddings_list = [item['embedding'] for item in rag_data]\n",
        "    rag_embeddings = np.array(embeddings_list).astype('float32')\n",
        "    rag_available = True\n",
        "    print(\"âœ… RAG SYSTEM: READY\")\n",
        "    print(f\"  Index: {len(rag_data)} entries\")\n",
        "    print(f\"  Images: {RAG_IMAGE_DIR}\")\n",
        "else:\n",
        "    print(\"âŒ RAG SYSTEM: NOT READY\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgprR5vDCt-x"
      },
      "source": [
        "## 4. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjzPIwxACt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4A: Core Helper Functions\n",
        "# ============================================================\n",
        "import os\n",
        "import re\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "def extract_filename_key(filepath):\n",
        "    \"\"\"Extract filename key for lookup.\"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    name_no_ext = os.path.splitext(filename)[0]\n",
        "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)  # Remove (1), (2) etc\n",
        "    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n",
        "    return name_cleaned.strip()\n",
        "\n",
        "def get_part_context(filepath):\n",
        "    \"\"\"Look up part context from filename.\"\"\"\n",
        "    filename_key = extract_filename_key(filepath)\n",
        "\n",
        "    if filename_key in filename_to_pn:\n",
        "        pn = filename_to_pn[filename_key]\n",
        "        lookup_key = normalize_pn(pn)\n",
        "        if lookup_key in part_context_db:\n",
        "            return pn, part_context_db[lookup_key]\n",
        "\n",
        "    for ext in ['.pdf', '.PDF']:\n",
        "        key = filename_key + ext\n",
        "        if key in filename_to_pn:\n",
        "            pn = filename_to_pn[key]\n",
        "            lookup_key = normalize_pn(pn)\n",
        "            if lookup_key in part_context_db:\n",
        "                return pn, part_context_db[lookup_key]\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def build_context_string(pn, context):\n",
        "    \"\"\"Build the context string for inspection prompt.\"\"\"\n",
        "    if context is None:\n",
        "        return \"CONTEXT: Unknown Part (General Syntax Check Only).\"\n",
        "\n",
        "    desc = context.get('description', 'Unknown')\n",
        "    assembly = context.get('assembly', 'Unknown Assembly')\n",
        "    siblings = context.get('siblings', 'None listed')\n",
        "\n",
        "    return f\"\"\"CONTEXT: This is Part {pn} ({desc}).\n",
        "It belongs to the {assembly}.\n",
        "It must assemble with these mating parts: {siblings}.\n",
        "CRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n",
        "\n",
        "def pdf_to_image(pdf_path, dpi=150):\n",
        "    \"\"\"Convert first page of PDF to PIL Image.\"\"\"\n",
        "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n",
        "    return pages[0] if pages else None\n",
        "\n",
        "print(\"âœ… Core helper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NCtYUaBCt-x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4B: Model Query Function\n",
        "# ============================================================\n",
        "import torch\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def query_model(messages, max_tokens=1024):\n",
        "    \"\"\"Send a query to Qwen2-VL and get response.\"\"\"\n",
        "    if 'model' not in globals() or 'processor' not in globals():\n",
        "        raise RuntimeError(\"âš ï¸ Model not loaded. Run Cell 2 first.\")\n",
        "\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
        "\n",
        "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "    response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return response.strip()\n",
        "\n",
        "print(\"âœ… Model query function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlGu3_l1Ct-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4C: RAG Retrieval Function\n",
        "# ============================================================\n",
        "\n",
        "def retrieve_asme_pages(keywords, top_k=2):\n",
        "    \"\"\"Retrieve relevant ASME standard pages based on keywords.\"\"\"\n",
        "    global RAG_IMAGE_DIR\n",
        "\n",
        "    if not rag_available or rag_embeddings is None:\n",
        "        print(\"  âš ï¸ RAG system not available\")\n",
        "        return []\n",
        "\n",
        "    if RAG_IMAGE_DIR is None:\n",
        "        print(\"  âš ï¸ RAG_IMAGE_DIR not set\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        query_vector = search_model.encode([keywords])\n",
        "        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "        retrieved_images = []\n",
        "        print(f\"  RAG Search: '{keywords[:50]}...'\")\n",
        "\n",
        "        for idx in top_indices:\n",
        "            item = rag_data[idx]\n",
        "            rel_path = item['path'].replace('\\\\', '/')\n",
        "\n",
        "            paths_to_try = [\n",
        "                os.path.join(RAG_IMAGE_DIR, rel_path),\n",
        "                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)),\n",
        "            ]\n",
        "\n",
        "            path_parts = rel_path.split('/')\n",
        "            if len(path_parts) > 1:\n",
        "                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n",
        "\n",
        "            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n",
        "\n",
        "            for try_path in paths_to_try:\n",
        "                if os.path.exists(try_path):\n",
        "                    try:\n",
        "                        img = Image.open(try_path).convert('RGB')\n",
        "                        retrieved_images.append(img)\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        print(f\"      Error: {e}\")\n",
        "\n",
        "        return retrieved_images\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  RAG error: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"âœ… RAG retrieval function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OpGuvn5Ct-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4D: Production Pipeline Helpers (LightOnOCR-2 + Tiling)\n",
        "# ============================================================\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "\n",
        "print(\"Initializing Production Pipeline...\")\n",
        "\n",
        "def render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n",
        "    \"\"\"Renders the first page of a PDF to a High-Res PIL Image using PyMuPDF.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        page = doc.load_page(0)\n",
        "        zoom = dpi / 72.0\n",
        "        mat = fitz.Matrix(zoom, zoom)\n",
        "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        doc.close()\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Rendering Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_lighton_ocr(img: Image.Image) -> List[str]:\n",
        "    \"\"\"\n",
        "    Runs LightOnOCR-2 on the image and returns structured text.\n",
        "\n",
        "    LightOnOCR-2 is a VLM that understands document structure and can output\n",
        "    Markdown tables - ideal for engineering drawings with title blocks.\n",
        "\n",
        "    Args:\n",
        "        img: PIL Image\n",
        "\n",
        "    Returns:\n",
        "        List of text strings found\n",
        "    \"\"\"\n",
        "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
        "\n",
        "    try:\n",
        "        img_rgb = img.convert(\"RGB\")\n",
        "\n",
        "        # Prepare conversation\n",
        "        conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img_rgb}]}]\n",
        "\n",
        "        inputs = ocr_processor.apply_chat_template(\n",
        "            conversation,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_dict=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
        "\n",
        "        generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
        "        output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Split and filter\n",
        "        lines = [line.strip() for line in output_text.split(\"\n",
        "\") if line.strip()]\n",
        "        return lines\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"LightOnOCR Error: {e}\")\n",
        "        return []\n",
        "\n",
        "# Alias for backwards compatibility\n",
        "run_tesseract_ocr = run_lighton_ocr\n",
        "\n",
        "def make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n",
        "    \"\"\"Splits the image into 4 overlapping quadrants for better resolution.\"\"\"\n",
        "    w, h = full_img.size\n",
        "    tile_w, tile_h = w // 2, h // 2\n",
        "    overlap = int(min(w, h) * 0.15)\n",
        "\n",
        "    boxes = {\n",
        "        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n",
        "        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n",
        "        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n",
        "        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n",
        "    }\n",
        "\n",
        "    tiles = []\n",
        "    for name, box in boxes.items():\n",
        "        tiles.append((name, full_img.crop(box)))\n",
        "    return tiles\n",
        "\n",
        "print(\"Production Pipeline Helpers Loaded (LightOnOCR-2).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUrqrO0eCt-y"
      },
      "source": [
        "## 5. Main Inspection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVw0CRCBCt-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5B: UNIVERSAL MISMATCH INSPECTOR (Batch-Ready)\n",
        "# ============================================================\n",
        "\n",
        "def inspect_drawing_universal(pdf_path):\n",
        "    \"\"\"\n",
        "    Universal inspector that dynamically loads context for each file.\n",
        "    Batch-ready with robust error handling.\n",
        "    \"\"\"\n",
        "    # 1. Get Dynamic Context for THIS specific file\n",
        "    pn = None\n",
        "    try:\n",
        "        pn, ctx = get_part_context(pdf_path)\n",
        "        context_str = ctx['siblings']  # Fixed: was 'siblings_str'\n",
        "        part_name = ctx['description']  # Fixed: was 'desc'\n",
        "    except:\n",
        "        print(f\"âš ï¸ Context Lookup Failed for {pdf_path}\")\n",
        "        context_str = \"No Mating Parts Found in Database.\"\n",
        "        part_name = \"Unknown Part\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nUNIVERSAL INSPECTION: {part_name}\\n{'='*60}\")\n",
        "\n",
        "    # --- Phase A: Perception ---\n",
        "    print(\"[1/3] Reading Drawing (Tesseract OCR)...\")\n",
        "    full_img = render_pdf_page(pdf_path, dpi=300)\n",
        "    if not full_img:\n",
        "        return {'error': 'Failed to render PDF', 'part_number': pn}\n",
        "\n",
        "    # Run Tesseract\n",
        "    ocr_texts = run_tesseract_ocr(full_img)\n",
        "    # Filter for relevant engineering text (numbers/dims)\n",
        "    filtered_ocr = [t for t in ocr_texts if any(char.isdigit() for char in t)]\n",
        "    ocr_block = \"\\n\".join([f\"- {t}\" for t in filtered_ocr[:120]])\n",
        "\n",
        "    print(f\"  > Evidence: {len(filtered_ocr)} relevant lines found.\")\n",
        "\n",
        "    # --- Phase B: Reasoning ---\n",
        "    print(\"[2/3] Generating Dynamic Truth Table...\")\n",
        "\n",
        "    system_prompt = \"\"\"You are a Universal Engineering Auditor.\n",
        "\n",
        "**YOUR GOAL:**\n",
        "Cross-reference \"LIST A\" (Requirements) against \"LIST B\" (Drawing Evidence).\n",
        "\n",
        "**LOGIC PROTOCOL:**\n",
        "1. Read LIST A to identify the Mating Parts.\n",
        "2. For EACH Mating Part, search LIST B for a corresponding feature (Thread, Hole, Diameter).\n",
        "3. **STRICTLY** compare dimensions.\n",
        "   - If List A says \"3/4-16\" and List B says \"M10\", output NO.\n",
        "   - If List A says \"0.750\" and List B says \"0.500\", output NO.\n",
        "4. If the Mating Part is generic (e.g. \"WASHER\"), and you see *any* washer dimension, you may output \"LIKELY MATCH\".\n",
        "5. If you cannot find any matching text in List B, output \"NOT FOUND\".\"\"\"\n",
        "\n",
        "    user_text = f\"\"\"**LIST A (THE REQUIREMENTS for Part {pn}):**\n",
        "{context_str}\n",
        "\n",
        "**LIST B (THE DRAWING TEXT):**\n",
        "{ocr_block}\n",
        "\n",
        "**TASK:**\n",
        "Create a Truth Table checking the compatibility of the Mating Parts in List A against the Evidence in List B.\n",
        "\n",
        "| Mating Part (from List A) | Found Feature in List B | Compatible? (YES/NO/NOT FOUND) |\n",
        "| :--- | :--- | :--- |\n",
        "| [Name/Spec of Part 1] | [Text from Drawing] | [Verdict] |\n",
        "| [Name/Spec of Part 2] | [Text from Drawing] | [Verdict] |\n",
        "\n",
        "**FINAL VERDICT:**\n",
        "PASS if all critical features match.\n",
        "FAIL if there is a direct contradiction (Metric vs Imperial).\"\"\"\n",
        "\n",
        "    content_payload = [\n",
        "        {'type': 'image', 'image': full_img},\n",
        "        {'type': 'text', 'text': user_text}\n",
        "    ]\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_prompt},\n",
        "        {'role': 'user', 'content': content_payload}\n",
        "    ]\n",
        "\n",
        "    # Inference\n",
        "    print(\"[3/3] Running inference...\")\n",
        "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=[full_img],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    token_count = inputs.input_ids.shape[1]\n",
        "    print(f\"  Token count: {token_count}\")\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=600)\n",
        "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nRESULT:\\n{'='*60}\")\n",
        "    print(response)\n",
        "\n",
        "    return {\n",
        "        'response': response,\n",
        "        'part_number': pn,\n",
        "        'part_name': part_name,\n",
        "        'ocr_total': len(ocr_texts),\n",
        "        'ocr_filtered': len(filtered_ocr),\n",
        "        'token_count': token_count\n",
        "    }\n",
        "\n",
        "# Aliases for compatibility\n",
        "inspect_drawing_strict_optimized = inspect_drawing_universal\n",
        "inspect_drawing_production = inspect_drawing_universal\n",
        "\n",
        "print(\"âœ… Universal Inspector Loaded (Batch-Ready).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aoMhHjzCt-y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5D: MASTER PIPELINE - Full Inspection (Stage 1 + Stage 2)\n",
        "# ============================================================\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def parse_verdict(response_text):\n",
        "    \"\"\"\n",
        "    Parse the truth table response to determine PASS/FAIL.\n",
        "    Returns FAIL if any row has 'NO', PASS if all rows have 'YES'.\n",
        "    \"\"\"\n",
        "    if not response_text:\n",
        "        return 'ERROR', 'No response'\n",
        "\n",
        "    text_upper = response_text.upper()\n",
        "\n",
        "    # Count YES and NO in the response\n",
        "    yes_count = len(re.findall(r'\\|\\s*YES\\s*\\|', text_upper))\n",
        "    no_count = len(re.findall(r'\\|\\s*NO\\s*\\|', text_upper))\n",
        "\n",
        "    # Also check for standalone YES/NO at end of lines\n",
        "    yes_count += len(re.findall(r'YES\\s*$', text_upper, re.MULTILINE))\n",
        "    no_count += len(re.findall(r'NO\\s*$', text_upper, re.MULTILINE))\n",
        "\n",
        "    # Check for NOT FOUND\n",
        "    not_found = 'NOT FOUND' in text_upper\n",
        "\n",
        "    # Determine verdict\n",
        "    if no_count > 0 or not_found:\n",
        "        issues = []\n",
        "        if no_count > 0:\n",
        "            issues.append(f\"{no_count} mismatches\")\n",
        "        if not_found:\n",
        "            issues.append(\"missing specs\")\n",
        "        return 'FAIL', '; '.join(issues)\n",
        "    elif yes_count > 0:\n",
        "        return 'PASS', f\"{yes_count} specs verified\"\n",
        "    else:\n",
        "        return 'REVIEW', 'Could not parse verdict'\n",
        "\n",
        "\n",
        "def run_full_inspection(pdf_path, skip_stage2_on_fail=True):\n",
        "    \"\"\"\n",
        "    Master pipeline that runs both inspection stages:\n",
        "\n",
        "    Stage 1 (Gatekeeper): Strict mismatch detection\n",
        "    Stage 2 (Consultant): Improvement suggestions (only if Stage 1 passes)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(f\"# FULL ENGINEERING INSPECTION PIPELINE\")\n",
        "    print(f\"# File: {os.path.basename(pdf_path)}\")\n",
        "    print(f\"# Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'#'*60}\")\n",
        "\n",
        "    result = {\n",
        "        'file': os.path.basename(pdf_path),\n",
        "        'path': pdf_path,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'stage1': None,\n",
        "        'stage2': None,\n",
        "        'final_verdict': None\n",
        "    }\n",
        "\n",
        "    # STAGE 1: THE GATEKEEPER\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"STAGE 1: THE GATEKEEPER (Mismatch Detection)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    pn, ctx = get_part_context(pdf_path)\n",
        "\n",
        "    if not ctx:\n",
        "        print(f\"âš ï¸ No context found for {pdf_path}\")\n",
        "        result['stage1'] = {'verdict': 'ERROR', 'reason': 'Part not in database'}\n",
        "        result['final_verdict'] = 'ERROR'\n",
        "        return result\n",
        "\n",
        "    context_data = {\n",
        "        'pn': pn,\n",
        "        'description': ctx.get('description', 'Unknown'),\n",
        "        'assembly': ctx.get('assembly', 'Unknown'),\n",
        "        'siblings': ctx.get('siblings', 'No mating parts')\n",
        "    }\n",
        "\n",
        "    stage1_result = inspect_drawing_universal(pdf_path)\n",
        "\n",
        "    if isinstance(stage1_result, dict) and 'error' in stage1_result:\n",
        "        stage1_verdict = 'ERROR'\n",
        "        stage1_reason = stage1_result['error']\n",
        "        response = ''\n",
        "    else:\n",
        "        response = stage1_result.get('response', '') if isinstance(stage1_result, dict) else str(stage1_result)\n",
        "        stage1_verdict, stage1_reason = parse_verdict(response)\n",
        "\n",
        "    result['stage1'] = {\n",
        "        'verdict': stage1_verdict,\n",
        "        'reason': stage1_reason,\n",
        "        'part_number': pn,\n",
        "        'part_name': context_data['description'],\n",
        "        'ocr_count': stage1_result.get('ocr_filtered', 0) if isinstance(stage1_result, dict) else 0,\n",
        "        'response': response[:1000]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n>>> STAGE 1 VERDICT: {stage1_verdict} - {stage1_reason}\")\n",
        "\n",
        "    # STAGE 2: THE CONSULTANT\n",
        "    if stage1_verdict == 'FAIL' and skip_stage2_on_fail:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: SKIPPED (Stage 1 Failed)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(\"Fix the Stage 1 issues before requesting improvement suggestions.\")\n",
        "        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 failed'}\n",
        "        result['final_verdict'] = 'FAIL'\n",
        "\n",
        "    elif stage1_verdict == 'ERROR':\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: SKIPPED (Stage 1 Error)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        result['stage2'] = {'status': 'SKIPPED', 'reason': 'Stage 1 error'}\n",
        "        result['final_verdict'] = 'ERROR'\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"STAGE 2: THE CONSULTANT (Proceeding...)\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        ocr_text_list = []\n",
        "        if isinstance(stage1_result, dict):\n",
        "            full_img = render_pdf_page(pdf_path, dpi=200)\n",
        "            if full_img:\n",
        "                ocr_text_list = run_tesseract_ocr(full_img)\n",
        "                ocr_text_list = [t for t in ocr_text_list if any(c.isdigit() for c in t)]\n",
        "\n",
        "        try:\n",
        "            stage2_result = suggest_improvements_stage2(\n",
        "                pdf_path=pdf_path,\n",
        "                context_data=context_data,\n",
        "                ocr_text_list=ocr_text_list\n",
        "            )\n",
        "\n",
        "            result['stage2'] = {\n",
        "                'status': 'COMPLETED',\n",
        "                'suggestions': stage2_result.get('suggestions', ''),\n",
        "                'asme_pages_used': stage2_result.get('asme_pages_used', 0),\n",
        "                'search_queries': stage2_result.get('search_queries', [])\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Stage 2 Error: {e}\")\n",
        "            result['stage2'] = {'status': 'ERROR', 'reason': str(e)}\n",
        "\n",
        "        result['final_verdict'] = 'PASS_WITH_SUGGESTIONS' if stage1_verdict == 'PASS' else 'REVIEW'\n",
        "\n",
        "    # FINAL SUMMARY\n",
        "    print(f\"\\n{'#'*60}\")\n",
        "    print(\"# FINAL INSPECTION SUMMARY\")\n",
        "    print(f\"{'#'*60}\")\n",
        "    print(f\"Part: {pn} ({context_data['description']})\")\n",
        "    print(f\"Stage 1 (Gatekeeper): {result['stage1']['verdict']}\")\n",
        "    print(f\"Stage 2 (Consultant): {result['stage2'].get('status', 'N/A')}\")\n",
        "    print(f\"Final Verdict: {result['final_verdict']}\")\n",
        "    print(f\"{'#'*60}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_full_inspection_batch(drawing_folder, output_file=None, limit=None):\n",
        "    \"\"\"Run full inspection on all PDFs in a folder.\"\"\"\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n",
        "    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n",
        "    pdf_files = sorted(list(set(pdf_files)))\n",
        "\n",
        "    if limit:\n",
        "        pdf_files = pdf_files[:limit]\n",
        "\n",
        "    print(f\"{'#'*60}\")\n",
        "    print(f\"FULL INSPECTION BATCH: {len(pdf_files)} files\")\n",
        "    print(f\"{'#'*60}\\n\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for pdf_path in tqdm(pdf_files, desc=\"Full Inspection\"):\n",
        "        try:\n",
        "            result = run_full_inspection(pdf_path)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            results.append({'file': os.path.basename(pdf_path), 'final_verdict': 'ERROR', 'error': str(e)})\n",
        "\n",
        "    if not output_file:\n",
        "        output_file = f\"full_inspection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    verdicts = [r.get('final_verdict', 'ERROR') for r in results]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BATCH SUMMARY: {len(results)} files\")\n",
        "    print(f\"PASS: {verdicts.count('PASS_WITH_SUGGESTIONS')} | FAIL: {verdicts.count('FAIL')} | ERROR: {verdicts.count('ERROR')}\")\n",
        "    print(f\"Results: {output_file}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… parse_verdict() defined.\")\n",
        "print(\"âœ… run_full_inspection() defined.\")\n",
        "print(\"âœ… run_full_inspection_batch() defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0MRiihCt-y"
      },
      "source": [
        "## 6. Inspect a Drawing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEJbc1XtCt-y"
      },
      "outputs": [],
      "source": [
        "# Upload and inspect a single PDF drawing\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload a PDF drawing to inspect:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "    print(f\"\n",
        "Inspecting: {pdf_path}\")\n",
        "    result = run_full_inspection(pdf_path)\n",
        "\n",
        "    # Display result summary\n",
        "    print(\"\n",
        "\" + \"=\"*60)\n",
        "    print(\"INSPECTION COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    if 'verdict' in result:\n",
        "        print(f\"Verdict: {result['verdict']}\")\n",
        "    if 'reason' in result:\n",
        "        print(f\"Reason: {result['reason']}\")\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n"
      ]
    }
  ]
}