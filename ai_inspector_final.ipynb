{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFrLGYPswHHZ"
   },
   "source": [
    "# AI Engineering Drawing Inspector (Final Version)\n",
    "\n",
    "A context-aware GD&T checker that uses:\n",
    "- Part context from BOM structure\n",
    "- RAG retrieval from ASME Y14.5 standard\n",
    "- Qwen2-VL for visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTvRp4rwHHa"
   },
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmkO97ziwHHa",
    "outputId": "86f11636-b817-4b1d-cb87-b3c70b0cd294"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 1A: Install Dependencies\n# ============================================================\n# Install required packages\n!pip install -q transformers accelerate\n!pip install -q qwen-vl-utils\n!pip install -q pdf2image\n!pip install -q faiss-cpu sentence-transformers\n!pip install -q bitsandbytes\n!apt-get install -y poppler-utils > /dev/null 2>&1\n\n# Production Pipeline Dependencies (OCR + High-Res Rendering)\n!pip install -q pymupdf paddleocr paddlepaddle opencv-python-headless\n\nprint(\"‚úÖ All packages installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_VXcblCwHHa",
    "outputId": "9a2dc86c-8d12-4b1c-f9c7-4268d0161a9c"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 1B: Import Libraries\n# ============================================================\nimport os\nimport json\nimport re\nimport pickle\nimport torch\nfrom pathlib import Path\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# Production Pipeline Imports\nimport fitz  # PyMuPDF\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Dict, Any\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmqDweB4wHHa"
   },
   "source": [
    "## 2. Load Model (Qwen2-VL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "8024a37d19d149cd8d5fcebb04443799",
      "b59f6d5995594d909cd39f10e8a3fa61",
      "49bf137ce3594f5fad2c04f000dd8148",
      "f52e2c702ae84250b1f5208bad2fa7ec",
      "5b662ad0551d4ac4b9bfac9d9e3bc511",
      "0181fba481e34d48b0afe547f430d500",
      "b613b9eb5cd94dca83d6dea13ae21285",
      "3d53fd4269fc4e5292d7d6fd6b17df93",
      "fd7fee055d724bf38dc9b903c990c052",
      "fce5f0a8a2a242879992d551f3378a17",
      "df4f872ed2544b6bb60f515ec1fc6b07"
     ]
    },
    "id": "8fokt7vLwHHa",
    "outputId": "da45d8f8-4174-4df3-90a3-5c7af547242d"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Load Qwen2-VL Model (4-bit Quantized)\n# ============================================================\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n\nMODEL_ID = \"Qwen/Qwen2-VL-72B-Instruct\"\n\nprint(f\"Loading {MODEL_ID} in 4-bit (NF4)...\")\n\n# Define 4-bit configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load Model with SDPA (Native Flash Attention for PyTorch 2.0+)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    MODEL_ID,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n    attn_implementation=\"sdpa\",\n    trust_remote_code=True\n)\n\nprocessor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n\nprint(\"‚úÖ Qwen2-VL-72B (4-bit) Loaded Successfully!\")\nprint(f\"Memory Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YkOM8aOwHHa"
   },
   "source": [
    "## 3. Load Context Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OK5ZAuphwHHa",
    "outputId": "c141b3be-e1f7-4408-d803-bb4deea724e5"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3A: Upload/Locate Configuration Files\n# ============================================================\nimport os\nimport zipfile\nfrom google.colab import files\n\n# Expected config filenames\nMAPPING_FILE = \"400S_file_part_mapping.json\"\nSTRUCTURE_FILE = \"400S_detailed_structure_fixed.json\"\nRAG_INDEX_FILE = \"asme_visual_index.pkl\"\n\nprint(\"=\"*60)\nprint(\"STEP 1: Upload Configuration Files\")\nprint(\"=\"*60)\n\n# Helper to locate a file (cwd or inside rag_data)\ndef locate_file(filename):\n    if os.path.exists(filename):\n        return os.path.abspath(filename)\n    nested_path = os.path.join(\"rag_data\", filename)\n    if os.path.exists(nested_path):\n        return os.path.abspath(nested_path)\n    return None\n\n# Check for existing files first\nFILE_MAPPING_PATH = locate_file(MAPPING_FILE)\nSTRUCTURE_PATH = locate_file(STRUCTURE_FILE)\nRAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n\n# Upload missing files\nmissing_files = []\nif not FILE_MAPPING_PATH:\n    missing_files.append(MAPPING_FILE)\nif not STRUCTURE_PATH:\n    missing_files.append(STRUCTURE_FILE)\nif not RAG_INDEX_PATH:\n    missing_files.append(RAG_INDEX_FILE)\n\nif missing_files:\n    print(f\"\\nMissing files: {', '.join(missing_files)}\")\n    print(\"\\nPlease upload the required files (or a ZIP containing them):\")\n    uploaded = files.upload()\n\n    for filename in uploaded:\n        if filename.lower().endswith('.zip'):\n            print(f\"\\nExtracting {filename}...\")\n            with zipfile.ZipFile(filename, 'r') as zip_ref:\n                zip_ref.extractall(\"rag_data\")\n            print(\"Extraction complete.\")\n            break\n\n    FILE_MAPPING_PATH = locate_file(MAPPING_FILE) or os.path.abspath(MAPPING_FILE)\n    STRUCTURE_PATH = locate_file(STRUCTURE_FILE) or os.path.abspath(STRUCTURE_FILE)\n    RAG_INDEX_PATH = locate_file(RAG_INDEX_FILE)\n\n# Set DATA_DIR\nif FILE_MAPPING_PATH:\n    DATA_DIR = os.path.dirname(FILE_MAPPING_PATH)\nelse:\n    DATA_DIR = \"/content\"\n\n# Print status\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILE STATUS:\")\nprint(\"=\"*60)\nprint(f\"File Mapping:  {'‚úÖ OK' if FILE_MAPPING_PATH and os.path.exists(FILE_MAPPING_PATH) else '‚ùå MISSING'}\")\nprint(f\"Structure:     {'‚úÖ OK' if STRUCTURE_PATH and os.path.exists(STRUCTURE_PATH) else '‚ùå MISSING'}\")\nprint(f\"RAG Index:     {'‚úÖ OK' if RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH) else '‚ö†Ô∏è MISSING (optional)'}\")\nprint(f\"\\nData directory: {DATA_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykrHElKlwHHb",
    "outputId": "62c1d377-4283-46e8-a395-b45bfb23bfa4"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3B: Load Part Context Databases\n# ============================================================\n\ndef normalize_pn(pn):\n    \"\"\"Normalize part number for lookup (remove dashes, spaces, lowercase).\"\"\"\n    return re.sub(r'[-\\s]', '', str(pn)).lower()\n\n\ndef load_context_databases():\n    \"\"\"\n    Load and build all context databases:\n    1. filename_to_pn: Maps filenames to part numbers\n    2. part_context_db: Full context for each part\n    \"\"\"\n    print(\"Loading file mapping...\")\n    with open(FILE_MAPPING_PATH, 'r') as f:\n        file_mapping_list = json.load(f)\n\n    filename_to_pn = {}\n    for entry in file_mapping_list:\n        filename = entry['file']\n        pn = entry['pn']\n        if pn:\n            filename_to_pn[filename] = pn\n            filename_to_pn[filename + '.pdf'] = pn\n            filename_to_pn[filename + '.PDF'] = pn\n\n    print(f\"  Loaded {len(file_mapping_list)} file mappings\")\n\n    print(\"Loading part structure...\")\n    with open(STRUCTURE_PATH, 'r') as f:\n        structure_data = json.load(f)\n\n    print(\"Building part context database...\")\n    part_context_db = {}\n\n    for assembly_name, parts_list in structure_data.items():\n        for part in parts_list:\n            pn = part['pn']\n            desc = part['desc']\n\n            siblings_list = []\n            siblings_pns = []\n\n            for p_sibling in parts_list:\n                if p_sibling['pn'] != pn:\n                    safe_desc = str(p_sibling['desc']).replace('\"', \"'\")\n                    siblings_list.append(f\"{p_sibling['pn']} ({safe_desc})\")\n                    siblings_pns.append(p_sibling['pn'])\n\n            siblings_str = \"; \".join(siblings_list[:12])\n            if len(siblings_list) > 12:\n                siblings_str += f\"... and {len(siblings_list) - 12} more\"\n\n            lookup_key = normalize_pn(pn)\n\n            part_context_db[lookup_key] = {\n                'pn': pn,\n                'description': desc,\n                'assembly': assembly_name,\n                'siblings': siblings_str,\n                'siblings_list': siblings_pns\n            }\n            part_context_db[pn] = part_context_db[lookup_key]\n\n    print(f\"  Built context for {len(part_context_db) // 2} unique parts\")\n\n    return filename_to_pn, part_context_db\n\n\n# Load databases\nfilename_to_pn, part_context_db = load_context_databases()\nprint(\"\\n‚úÖ Context databases loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d605a04",
    "outputId": "e5e2acbf-af33-4d93-eb89-60905f4ad972"
   },
   "source": "# ============================================================\n# CELL 3C: Initialize OCR Engine (PaddleOCR)\n# ============================================================\nimport sys\nimport subprocess\n\nprint(\"Loading OCR Engine...\")\n\n# Fix langchain dependency issue BEFORE importing PaddleOCR\nprint(\"  Installing langchain dependencies...\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"langchain\", \"langchain-community\"], \n               capture_output=True)\n\nocr_engine = None\n\ntry:\n    from paddleocr import PaddleOCR\n    ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n    print(\"‚úÖ OCR Engine Ready!\")\n\nexcept RuntimeError as e:\n    if \"PDX has already been initialized\" in str(e):\n        print(\"\\n\" + \"=\"*60)\n        print(\"üõë RUNTIME RESTART REQUIRED\")\n        print(\"=\"*60)\n        print(\"The PaddleOCR library cannot be re-initialized in the same session.\")\n        print(\"Please go to the menu bar:\")\n        print(\"   Runtime > Restart session\")\n        print(\"Then run the cells again.\")\n    else:\n        print(f\"‚ùå Runtime Error: {e}\")\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")\n\ndef get_drawing_text_ocr(image_input):\n    \"\"\"\n    Runs robust OCR on the drawing and returns a clean list of found text.\n    \"\"\"\n    if ocr_engine is None:\n        print(\"‚ö†Ô∏è OCR Engine not initialized. Please restart runtime.\")\n        return []\n\n    try:\n        result = ocr_engine.ocr(image_input, cls=True)\n        text_set = set()\n\n        if result and result[0]:\n            for line in result[0]:\n                text_content = line[1][0]\n                confidence = line[1][1]\n                if confidence > 0.85:\n                    text_set.add(text_content.strip())\n\n        return sorted(list(text_set))\n    except Exception as e:\n        print(f\"‚ö†Ô∏è OCR Warning: {e}\")\n        return []",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fecfcc5a",
    "outputId": "e0d3cbd3-40bf-4a5f-8f57-e5db079397a3"
   },
   "source": "# ============================================================\n# CELL 3D: LangChain Dependencies (Backup Install)\n# ============================================================\n# This cell is a backup in case cell 3C didn't install langchain properly\n!pip install -q langchain langchain-community\nprint(\"‚úÖ LangChain dependencies installed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "AsY8eejBwHHb",
    "outputId": "104cd431-bf3b-480b-8cbb-2e3d82b072a4"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3E: Load RAG Index & Visual Database\n# ============================================================\nfrom sentence_transformers import SentenceTransformer\nfrom google.colab import files\nimport numpy as np\nimport zipfile\nimport shutil\n\n# Initialize globals\nrag_data = []\nrag_embeddings = None\nrag_available = False\nRAG_IMAGE_DIR = None\n\nprint(\"=\"*60)\nprint(\"RAG SYSTEM SETUP\")\nprint(\"=\"*60)\n\n# STEP 1: Load CLIP Model\nprint(\"\\n[STEP 1/3] Loading CLIP model for semantic search...\")\nsearch_model = SentenceTransformer('clip-ViT-B-32')\nprint(\"  CLIP model loaded!\")\n\n# STEP 2: Load or Upload RAG Index (.pkl file)\nprint(\"\\n[STEP 2/3] Loading RAG Index...\")\n\nindex_loaded = False\n\nif 'RAG_INDEX_PATH' in dir() and RAG_INDEX_PATH and os.path.exists(RAG_INDEX_PATH):\n    print(f\"  Found existing index: {RAG_INDEX_PATH}\")\n    with open(RAG_INDEX_PATH, 'rb') as f:\n        rag_data = pickle.load(f)\n    index_loaded = True\nelse:\n    print(\"  No RAG index found.\")\n    print(\"\\n  >> Please upload your RAG index file (asme_visual_index.pkl):\")\n\n    try:\n        uploaded_index = files.upload()\n\n        for filename in uploaded_index:\n            if filename.endswith('.pkl'):\n                RAG_INDEX_PATH = os.path.abspath(filename)\n                with open(RAG_INDEX_PATH, 'rb') as f:\n                    rag_data = pickle.load(f)\n                index_loaded = True\n                print(f\"\\n  Loaded index: {filename} ({len(rag_data)} entries)\")\n                break\n\n        if not index_loaded:\n            print(\"  WARNING: No .pkl file was uploaded!\")\n    except Exception as e:\n        print(f\"  Upload error: {e}\")\n\n# STEP 3: Load or Upload RAG Visual Database (images folder)\nprint(\"\\n[STEP 3/3] Setting up RAG Visual Database (ASME page images)...\")\n\nexisting_locations = [\n    \"/content/rag_visual_db\",\n    \"/content/rag_data/rag_visual_db\",\n    \"/content/data/rag_visual_db\",\n    \"rag_visual_db\",\n    \"rag_data/rag_visual_db\",\n]\n\nif 'DATA_DIR' in dir() and DATA_DIR:\n    existing_locations.insert(0, os.path.join(DATA_DIR, \"rag_visual_db\"))\n\nfound_images = False\nfor loc in existing_locations:\n    if loc and os.path.exists(loc) and os.path.isdir(loc):\n        img_count = len([f for f in os.listdir(loc) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        if img_count > 0:\n            RAG_IMAGE_DIR = os.path.abspath(loc)\n            found_images = True\n            print(f\"  Found existing images: {RAG_IMAGE_DIR} ({img_count} files)\")\n            break\n\nif not found_images:\n    print(\"  No existing image database found.\")\n    print(\"\\n  >> Please upload your RAG visual database as a ZIP file:\")\n\n    try:\n        uploaded_zip = files.upload()\n\n        for filename in uploaded_zip:\n            if filename.lower().endswith('.zip'):\n                RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n                if os.path.exists(RAG_IMAGE_DIR):\n                    shutil.rmtree(RAG_IMAGE_DIR)\n                os.makedirs(RAG_IMAGE_DIR, exist_ok=True)\n\n                print(f\"\\n  Extracting {filename}...\")\n                with zipfile.ZipFile(filename, 'r') as zf:\n                    zf.extractall(RAG_IMAGE_DIR)\n\n                all_images = []\n                for root, dirs, fls in os.walk(RAG_IMAGE_DIR):\n                    for f in fls:\n                        if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n                            all_images.append(os.path.join(root, f))\n\n                print(f\"  Extracted {len(all_images)} images\")\n\n                if all_images:\n                    common_dir = os.path.commonpath(all_images)\n                    if os.path.isdir(common_dir) and common_dir != RAG_IMAGE_DIR:\n                        RAG_IMAGE_DIR = common_dir\n\n                found_images = True\n                break\n\n        if not found_images:\n            print(\"  WARNING: No ZIP file uploaded. RAG retrieval will not work.\")\n            RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n\n    except Exception as e:\n        print(f\"  Upload error: {e}\")\n        RAG_IMAGE_DIR = \"/content/rag_visual_db\"\n\n# FINALIZE: Build search index\nprint(\"\\n\" + \"=\"*60)\n\nif index_loaded and isinstance(rag_data, list) and len(rag_data) > 0:\n    print(\"Optimizing search index...\")\n    embeddings_list = [item['embedding'] for item in rag_data]\n    rag_embeddings = np.array(embeddings_list).astype('float32')\n    rag_available = True\n\n    print(\"\\n‚úÖ RAG SYSTEM STATUS: READY\")\n    print(f\"  Index entries:    {len(rag_data)}\")\n    print(f\"  Image directory:  {RAG_IMAGE_DIR}\")\n    print(f\"  Images exist:     {os.path.exists(RAG_IMAGE_DIR) if RAG_IMAGE_DIR else False}\")\nelse:\n    print(\"‚ùå RAG SYSTEM STATUS: NOT READY\")\n    print(\"  Index not loaded. Please re-run this cell and upload the .pkl file.\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD4nCWywHHb"
   },
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4Y7DliIwHHb",
    "outputId": "1b0afa79-0c7b-4dad-f09a-576061eefd03"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4A: Core Helper Functions\n# ============================================================\n\ndef extract_filename_key(filepath):\n    \"\"\"Extract the filename key for lookup from a full path.\"\"\"\n    filename = os.path.basename(filepath)\n    name_no_ext = os.path.splitext(filename)[0]\n    name_cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', name_no_ext, flags=re.IGNORECASE)\n    return name_cleaned.strip()\n\n\ndef get_part_context(filepath):\n    \"\"\"Look up part context from filename. Returns (part_number, context_dict) or (None, None).\"\"\"\n    filename_key = extract_filename_key(filepath)\n\n    if filename_key in filename_to_pn:\n        pn = filename_to_pn[filename_key]\n        lookup_key = normalize_pn(pn)\n        if lookup_key in part_context_db:\n            return pn, part_context_db[lookup_key]\n\n    for ext in ['.pdf', '.PDF']:\n        key = filename_key + ext\n        if key in filename_to_pn:\n            pn = filename_to_pn[key]\n            lookup_key = normalize_pn(pn)\n            if lookup_key in part_context_db:\n                return pn, part_context_db[lookup_key]\n\n    return None, None\n\n\ndef build_context_string(pn, context):\n    \"\"\"Build the context string for the inspection prompt.\"\"\"\n    if context is None:\n        return \"CONTEXT: Unknown Part (General Syntax Check Only). No assembly context available.\"\n\n    desc = context.get('description', 'Unknown')\n    assembly = context.get('assembly', 'Unknown Assembly')\n    siblings = context.get('siblings', 'None listed')\n\n    context_str = f\"\"\"CONTEXT: This is Part {pn} ({desc}).\nIt belongs to the {assembly}.\nIt must assemble with these mating parts: {siblings}.\nCRITICAL: Check for mating tolerances suitable for a {desc}.\"\"\"\n\n    return context_str\n\n\ndef pdf_to_image(pdf_path, dpi=150):\n    \"\"\"Convert first page of PDF to PIL Image.\"\"\"\n    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n    if pages:\n        return pages[0]\n    return None\n\n\nprint(\"‚úÖ Core helper functions defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3aLheGBwHHb",
    "outputId": "7d51c2c9-adbe-4ba6-d5db-a327129c09a3"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4B: Model Query Function\n# ============================================================\n\ndef query_model(messages, max_tokens=1024):\n    \"\"\"Send a query to Qwen2-VL and get response.\"\"\"\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n\n    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n    response = processor.batch_decode(\n        generated_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    )[0]\n\n    return response.strip()\n\n\nprint(\"‚úÖ Model query function defined.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gETA6XD5wHHb",
    "outputId": "c611e8b5-4cc2-438a-82ec-5f54df18e2c2"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4C: RAG Retrieval Function\n# ============================================================\n\ndef retrieve_asme_pages(keywords, top_k=2):\n    \"\"\"Retrieve relevant ASME standard pages based on GD&T keywords.\"\"\"\n    global RAG_IMAGE_DIR\n\n    if not rag_available or rag_embeddings is None:\n        print(\"  WARNING: RAG system not available for retrieval\")\n        return []\n\n    if RAG_IMAGE_DIR is None:\n        print(\"  WARNING: RAG_IMAGE_DIR not set. Run the RAG setup cell first.\")\n        return []\n\n    try:\n        query_vector = search_model.encode([keywords])\n        scores = np.dot(query_vector, rag_embeddings.T).flatten()\n        top_indices = np.argsort(scores)[-top_k:][::-1]\n\n        retrieved_images = []\n        print(f\"  RAG Search: '{keywords[:50]}...'\")\n\n        for idx in top_indices:\n            item = rag_data[idx]\n            rel_path = item['path'].replace('\\\\', '/')\n\n            paths_to_try = [\n                os.path.join(RAG_IMAGE_DIR, rel_path),\n                os.path.join(RAG_IMAGE_DIR, os.path.basename(rel_path)),\n            ]\n\n            path_parts = rel_path.split('/')\n            if len(path_parts) > 1:\n                paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-1]))\n                if len(path_parts) > 2:\n                    paths_to_try.append(os.path.join(RAG_IMAGE_DIR, path_parts[-2], path_parts[-1]))\n\n            print(f\"    - {os.path.basename(rel_path)} (Score: {scores[idx]:.3f})\")\n\n            image_loaded = False\n            for try_path in paths_to_try:\n                if os.path.exists(try_path):\n                    try:\n                        img = Image.open(try_path).convert('RGB')\n                        retrieved_images.append(img)\n                        image_loaded = True\n                        break\n                    except Exception as e:\n                        print(f\"      Error opening image: {e}\")\n\n            if not image_loaded:\n                print(f\"      Image not found in {RAG_IMAGE_DIR}\")\n\n        return retrieved_images\n\n    except Exception as e:\n        print(f\"  RAG retrieval error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return []\n\nprint(\"‚úÖ RAG retrieval function defined.\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 4D: Production Pipeline Helpers (OCR + Tiling)\n# ============================================================\nprint(\"‚öôÔ∏è Initializing Production Pipeline...\")\n\n# Reuse OCR Engine from cell 3C if available\nif 'ocr_engine' not in dir() or ocr_engine is None:\n    print(\"  OCR Engine not found, initializing...\")\n    try:\n        from paddleocr import PaddleOCR\n        ocr_engine = PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n        print(\"  OCR Engine initialized.\")\n    except Exception as e:\n        print(f\"  OCR Engine warning: {e}\")\n        print(\"  OCR will not be available. Vision-only mode will be used.\")\n        ocr_engine = None\nelse:\n    print(\"  OCR Engine already loaded.\")\n\ndef render_pdf_page(pdf_path: str, dpi: int = 300) -> Image.Image:\n    \"\"\"Renders the first page of a PDF to a High-Res PIL Image using PyMuPDF.\"\"\"\n    try:\n        doc = fitz.open(pdf_path)\n        page = doc.load_page(0) \n        zoom = dpi / 72.0\n        mat = fitz.Matrix(zoom, zoom)\n        pix = page.get_pixmap(matrix=mat, alpha=False)\n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        doc.close()\n        return img\n    except Exception as e:\n        print(f\"‚ùå Rendering Error: {e}\")\n        return None\n\ndef run_paddleocr(img: Image.Image) -> List[str]:\n    \"\"\"Runs PaddleOCR on the image and returns a sorted, unique list of text found.\"\"\"\n    if ocr_engine is None:\n        print(\"‚ö†Ô∏è OCR Engine not available\")\n        return []\n    \n    img_np = np.array(img)\n    result = ocr_engine.ocr(img_np, cls=True)\n    \n    texts = []\n    if result and result[0]:\n        for line in result[0]:\n            text_content, confidence = line[1]\n            if confidence > 0.6:\n                clean_text = text_content.replace(\"√ò\", \"DIA \").strip()\n                texts.append(clean_text)\n    \n    return sorted(list(set(texts)))\n\ndef make_overlapping_tiles(full_img: Image.Image) -> List[Tuple[str, Image.Image]]:\n    \"\"\"Splits the image into 4 overlapping quadrants for better resolution.\"\"\"\n    w, h = full_img.size\n    tile_w, tile_h = w // 2, h // 2\n    overlap = int(min(w, h) * 0.15)\n\n    boxes = {\n        \"Top-Left\": (0, 0, tile_w + overlap, tile_h + overlap),\n        \"Top-Right\": (w - (tile_w + overlap), 0, w, tile_h + overlap),\n        \"Bottom-Left\": (0, h - (tile_h + overlap), tile_w + overlap, h),\n        \"Bottom-Right\": (w - (tile_w + overlap), h - (tile_h + overlap), w, h)\n    }\n\n    tiles = []\n    for name, box in boxes.items():\n        tiles.append((name, full_img.crop(box)))\n    return tiles\n\nprint(\"‚úÖ Production Pipeline Helpers Loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmd8w7bcwHHb"
   },
   "source": [
    "## 5. Main Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BcVNR3hwHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 5A: Main Inspection Function (RAG + OCR Hybrid)\n# ============================================================\nimport numpy as np\n\ndef inspect_drawing_rag(drawing_path, verbose=True):\n    \"\"\"\n    Main inspection function for engineering drawings.\n    Uses Vision + OCR + RAG + Chain-of-Thought for comprehensive analysis.\n    \"\"\"\n\n    if verbose:\n        print(f\"\\n{'='*60}\")\n        print(f\"INSPECTING: {os.path.basename(drawing_path)}\")\n        print('='*60)\n\n    # === PHASE 0: PRE-PROCESSING ===\n    if verbose:\n        print(\"\\n[1/5] Identifying part...\")\n\n    pn, context = get_part_context(drawing_path)\n\n    if not context:\n        return {\n            'result': 'FAIL',\n            'part_number': None,\n            'description': None,\n            'details': 'Identity Unknown - Context logic failed.'\n        }\n\n    context_str = build_context_string(pn, context)\n\n    if pn:\n        if verbose:\n            print(f\"  Part Number: {pn}\")\n            print(f\"  Description: {context.get('description', 'N/A')}\")\n            print(f\"  Assembly: {context.get('assembly', 'N/A')}\")\n            if context.get('siblings'):\n                print(f\"  Mating Parts: {context.get('siblings', 'N/A')}\")\n    else:\n        if verbose:\n            print(\"  Part not found in database - general inspection only\")\n\n    if verbose:\n        print(\"\\n[2/5] Loading drawing & running OCR scan...\")\n\n    try:\n        drawing_image = pdf_to_image(drawing_path)\n        if drawing_image is None:\n            return {\n                'result': 'ERROR',\n                'part_number': pn,\n                'description': context.get('description') if context else None,\n                'details': 'Failed to convert PDF to image'\n            }\n        if verbose:\n            print(f\"  Drawing loaded: {drawing_image.size}\")\n    except Exception as e:\n        return {\n            'result': 'ERROR',\n            'part_number': pn,\n            'description': context.get('description') if context else None,\n            'details': f'Error loading PDF: {str(e)}'\n        }\n\n    # OCR EXTRACTION\n    ocr_text_list = []\n    ocr_text_block = \"\"\n\n    try:\n        ocr_input = np.array(drawing_image)\n        ocr_text_list = get_drawing_text_ocr(ocr_input)\n        ocr_text_block = \"\\n\".join(ocr_text_list)\n\n        if verbose:\n            print(f\"  OCR Found {len(ocr_text_list)} text elements: {ocr_text_list[:5]}...\")\n    except Exception as e:\n        if verbose:\n            print(f\"  OCR Warning: {e} - Proceeding with vision-only mode\")\n\n    # === PHASE A: VISION + OCR EXTRACTION ===\n    if verbose:\n        print(\"\\n[3/5] CoT Step 1: Extraction (Vision + OCR)...\")\n\n    if ocr_text_block:\n        extraction_prompt = f\"\"\"You are an Expert Engineering Drawing Scanner.\n\nI have run an automated OCR scan on this drawing. Here is the raw text found:\n--- OCR DATA START ---\n{ocr_text_block}\n--- OCR DATA END ---\n\nYOUR TASK:\nUse the OCR Data to help you visually locate and confirm the features on the drawing image.\nExtract the following strictly. If the OCR list contains the number, trust it.\n\n1. **Thread Callouts** (e.g. 'M10x1.5', '1/4-20 UNC'). Look for these specifically in the OCR data.\n2. **Bore/Hole Dimensions** (e.g. '√ò0.500', '√ò1.00').\n3. **Material Note**.\n4. **GD&T Symbols** (Vision only).\n\nOutput the clean list of features found.\"\"\"\n    else:\n        extraction_prompt = \"\"\"Scan this drawing and extract the exact text for:\n1.  **Thread Callouts** (e.g., '1/4-20 UNC', 'M6x1.0').\n2.  **Bore/Hole Dimensions** with tolerances (e.g., '√ò0.500 +0.001/-0.000').\n3.  **Material Note**.\n4.  **GD&T Symbols**.\nList them exactly as written on the print. Do not analyze yet.\"\"\"\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": drawing_image},\n                {\"type\": \"text\", \"text\": extraction_prompt}\n            ]\n        }\n    ]\n\n    extraction_text = query_model(messages, max_tokens=512)\n    messages.append(\n        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": extraction_text}]}\n    )\n\n    if verbose:\n        print(f\"  Extracted Features:\\n{extraction_text[:300]}...\")\n\n    # === PHASE B: RAG RETRIEVAL ===\n    if verbose:\n        print(\"\\n[4/5] Retrieving ASME reference pages...\")\n\n    asme_images = []\n\n    if rag_available:\n        if verbose:\n            print(\"  Mode: Context-Aware Retrieval (using extracted features)\")\n        rag_query = extraction_text\n        if len(rag_query) < 20:\n            rag_query = \"General Dimensioning Rules\"\n        asme_images = retrieve_asme_pages(rag_query, top_k=2)\n    else:\n        if verbose:\n            print(\"  WARNING: RAG not available - proceeding without ASME references\")\n\n    if verbose:\n        print(f\"  Total ASME pages for audit: {len(asme_images)}\")\n\n    # === PHASE C: STRICT LOGIC COMPARATOR ===\n    if verbose:\n        print(\"\\n[5/5] CoT Step 2: Strict Logic Audit...\")\n\n    mating_parts_str = \"None specified\"\n    if context and context.get('siblings'):\n        mating_parts_str = context.get('siblings')\n\n    audit_prompt = f\"\"\"You are a Strict Logic Comparator.\nCompare the REQUIREMENTS (Context) vs ACTUALS (Extracted Data).\n\n1. REQUIREMENTS (Mating Parts):\n{context_str}\n\n2. ACTUALS (Found on Drawing):\n{extraction_text}\n\nSTRICT RULES:\n- You must verify if the **specific dimension** required by the mating part exists in the ACTUALS.\n- If Mating Part is 'Screw 3/4-16' and ACTUALS contains ONLY 'M10', '√ò0.50' -> **FAIL** (Mismatch).\n- If Mating Part is 'Bearing √ò0.75' and ACTUALS contains '√ò1.75' -> **FAIL** (Mismatch).\n- If the feature is NOT in the ACTUALS list, report: 'CANNOT VERIFY - Feature not found in OCR/Vision data'.\n\n**OUTPUT STRUCTURE (Follow EXACTLY):**\n\n**Line 1:** PASS or FAIL (Overall result)\n\n**Then provide:**\n\n1. **Tier 1 (General)**: State clearly if Material, Title Block, and General Tolerances are PRESENT or MISSING.\n\n2. **Tier 2 (GD&T Syntax)**: Comments on symbol formatting.\n\n3. **Tier 3 (Assembly Fit Analysis):**\n   - 'Mating Part [PN] -> [PASS/FAIL]: [Evidence from ACTUALS list]'\n\n4. **Citations:** Reference ASME images if applicable.\n\n5. **Recommendations**\"\"\"\n\n    content_2 = []\n    for img in asme_images:\n        content_2.append({\"type\": \"image\", \"image\": img})\n    content_2.append({\"type\": \"text\", \"text\": audit_prompt})\n\n    messages.append({\"role\": \"user\", \"content\": content_2})\n    audit_response = query_model(messages, max_tokens=1500)\n\n    # Parse result\n    first_line = audit_response.split('\\n')[0].strip().upper()\n    if 'PASS' in first_line and 'FAIL' not in first_line:\n        result = 'PASS'\n    elif 'FAIL' in first_line:\n        result = 'FAIL'\n    else:\n        response_upper = audit_response.upper()\n        if 'TIER 1 FAILURE' in response_upper or 'TIER 2 FAILURE' in response_upper or 'TIER 3 FAILURE' in response_upper:\n            result = 'FAIL'\n        elif '**FAIL**' in response_upper:\n            result = 'FAIL'\n        elif '**PASS**' in response_upper:\n            result = 'PASS'\n        else:\n            result = 'REVIEW'\n\n    if verbose:\n        print(f\"\\n{'='*60}\")\n        print(f\"RESULT: {result}\")\n        print('='*60)\n        print(audit_response)\n\n    return {\n        'result': result,\n        'part_number': pn,\n        'description': context.get('description') if context else None,\n        'assembly': context.get('assembly') if context else None,\n        'mating_parts': mating_parts_str,\n        'gdt_symbols': extraction_text,\n        'ocr_text_count': len(ocr_text_list),\n        'asme_pages_used': len(asme_images),\n        'details': audit_response\n    }\n\nprint(\"‚úÖ inspect_drawing_rag() function defined.\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 5B: Production Inspection Function (High-Res Tiling)\n# ============================================================\n\ndef inspect_drawing_production(pdf_path, context_str=None):\n    \"\"\"\n    Production-grade inspection using OCR + High-Res Tiling.\n    \n    Uses:\n    1. Deterministic OCR (PaddleOCR) to read text before the LLM sees it\n    2. Dynamic Tiling (4 quadrants) to improve resolution\n    3. PyMuPDF (fitz) for high-speed 300 DPI rendering\n    \"\"\"\n    print(f\"\\n{'='*60}\\nINSPECTING (Production): {pdf_path}\\n{'='*60}\")\n    \n    # Auto-generate context if not provided\n    if context_str is None:\n        pn, ctx = get_part_context(pdf_path)\n        if ctx:\n            context_str = ctx.get('siblings', 'No mating parts specified')\n            print(f\"  Part: {pn} ({ctx.get('description', 'N/A')})\")\n        else:\n            context_str = \"Unknown part - general inspection only\"\n            print(\"  Part not found in database\")\n    \n    # --- Phase A: Perception ---\n    print(\"[1/4] Rendering High-Res Image...\")\n    full_img = render_pdf_page(pdf_path, dpi=300)\n    if not full_img:\n        return \"FAIL: Image Rendering Failed\"\n    print(f\"  Image size: {full_img.size}\")\n    \n    print(\"[2/4] Extracting Deterministic OCR Evidence...\")\n    ocr_texts = run_paddleocr(full_img)\n    ocr_block = \"\\n\".join([f\"- {t}\" for t in ocr_texts[:80]]) \n    print(f\"  > OCR Found {len(ocr_texts)} text elements.\")\n\n    print(\"[3/4] Generating High-Res Tiles...\")\n    tiles = make_overlapping_tiles(full_img)\n    print(f\"  > Generated {len(tiles)} tiles\")\n\n    # --- Phase B: Reasoning ---\n    print(\"[4/4] Running Strict Logic Inference (Qwen2-VL-72B)...\")\n    \n    system_prompt = \"\"\"\n    You are a Senior Quality Control Engineer.\n    \n    **CORE PROTOCOL:**\n    1. **OCR IS AUTHORITY:** The 'OCR EVIDENCE' list is the ground truth for text.\n    2. **VISUAL VERIFICATION:** Use the 'TILES' to visually confirm geometry.\n    3. **STRICT COMPARISON:** Compare the 'MATING HYPOTHESIS' against the 'OCR EVIDENCE'.\n    \n    **FAILURE RULES:**\n    - If Hypothesis needs '3/4-16' and Evidence says 'M10', output **FAIL**.\n    - If Evidence is missing for a specific mating part, output **CANNOT VERIFY**.\n    - Do NOT hallucinate a fit. Mismatches must be flagged.\n    \"\"\"\n\n    user_text = f\"\"\"\n    **PART 1: OCR EVIDENCE (FACTS)**\n    {ocr_block}\n\n    **PART 2: MATING HYPOTHESIS (REQUIREMENTS)**\n    {context_str}\n\n    **TASK:**\n    For each Mating Part in the Hypothesis:\n    1. SEARCH the OCR list and Tiles for the matching feature.\n    2. COMPARE the dimensions/threads strictly.\n    3. REPORT: 'Mating Part [Name] -> [PASS/FAIL]: [Evidence]'\n    \"\"\"\n\n    # Build the Multi-Image Payload\n    content_payload = []\n    content_payload.append({'type': 'image', 'image': full_img})\n    content_payload.append({'type': 'text', 'text': \"FULL DRAWING VIEW\"})\n    \n    for name, tile in tiles:\n        content_payload.append({'type': 'image', 'image': tile})\n        content_payload.append({'type': 'text', 'text': f\"ZOOMED TILE: {name}\"})\n    \n    content_payload.append({'type': 'text', 'text': user_text})\n\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': content_payload}\n    ]\n\n    # Inference\n    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs = [full_img] + [t[1] for t in tiles]\n    \n    inputs = processor(\n        text=[text_input],\n        images=image_inputs,\n        return_tensors=\"pt\",\n        padding=True\n    ).to(model.device)\n\n    generated_ids = model.generate(**inputs, max_new_tokens=1000)\n    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    response = output_text.split(\"assistant\\n\")[-1] if \"assistant\\n\" in output_text else output_text\n    \n    print(f\"\\n{'='*60}\")\n    print(\"RESULT:\")\n    print('='*60)\n    print(response)\n    \n    return response\n\nprint(\"‚úÖ inspect_drawing_production() function defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc7S5QTzwHHb"
   },
   "source": [
    "## 6. Batch Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-tlYl9GwHHb",
    "outputId": "67085e6d-6000-4881-951a-56cc00f04fe1"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 6: Batch Inspection Function\n# ============================================================\n\ndef inspect_batch(drawing_folder, output_file=\"inspection_results.json\", limit=None):\n    \"\"\"Inspect all PDF drawings in a folder.\"\"\"\n    import glob\n    from tqdm.notebook import tqdm\n\n    pdf_files = glob.glob(os.path.join(drawing_folder, \"**/*.pdf\"), recursive=True)\n    pdf_files += glob.glob(os.path.join(drawing_folder, \"**/*.PDF\"), recursive=True)\n    pdf_files = list(set(pdf_files))\n\n    if limit:\n        pdf_files = pdf_files[:limit]\n\n    print(f\"Found {len(pdf_files)} PDF files to inspect\")\n\n    results = []\n    pass_count = 0\n    fail_count = 0\n    error_count = 0\n\n    for pdf_path in tqdm(pdf_files, desc=\"Inspecting\"):\n        try:\n            result = inspect_drawing_rag(pdf_path, verbose=False)\n            result['file'] = os.path.basename(pdf_path)\n            results.append(result)\n\n            if result['result'] == 'PASS':\n                pass_count += 1\n            elif result['result'] == 'FAIL':\n                fail_count += 1\n            else:\n                error_count += 1\n\n        except Exception as e:\n            results.append({\n                'file': os.path.basename(pdf_path),\n                'result': 'ERROR',\n                'details': str(e)\n            })\n            error_count += 1\n\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n\n    print(f\"\\n{'='*60}\")\n    print(\"BATCH INSPECTION SUMMARY\")\n    print('='*60)\n    print(f\"Total inspected: {len(results)}\")\n    print(f\"PASS: {pass_count} ({100*pass_count/len(results):.1f}%)\")\n    print(f\"FAIL: {fail_count} ({100*fail_count/len(results):.1f}%)\")\n    print(f\"ERROR/REVIEW: {error_count} ({100*error_count/len(results):.1f}%)\")\n    print(f\"\\nResults saved to: {output_file}\")\n\n    return results\n\n\nprint(\"‚úÖ Batch inspection function defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM1_6wuZwHHb"
   },
   "source": [
    "## 7. Test the Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0hzx2gEywHHb",
    "outputId": "004f096a-f990-4aed-8a12-4a68c8b57bae"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7A: Single File Test (Upload & Inspect)\n# ============================================================\nfrom google.colab import files\n\nprint(\"Upload a PDF drawing to inspect:\")\nuploaded = files.upload()\n\nif uploaded:\n    test_drawing = list(uploaded.keys())[0]\n    print(f\"Inspecting {test_drawing}...\")\n    result = inspect_drawing_rag(test_drawing, verbose=True)\nelse:\n    print(\"No file uploaded.\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc691493",
    "outputId": "7957e763-56ae-4a25-e007-9938659a6340"
   },
   "source": "# ============================================================\n# CELL 7B: OCR Libraries Install (Backup)\n# ============================================================\n# Run this if OCR is not working properly\n!pip install -q paddlepaddle\n!pip install -q \"paddleocr>=2.0.1\"\n!pip install -q opencv-python-headless\n\nprint(\"‚úÖ OCR Libraries Installed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "666f60f1",
    "outputId": "c558ce4a-40e3-4d30-8092-a2bc406516c1"
   },
   "source": "# ============================================================\n# CELL 7C: Verify RAG Database Location\n# ============================================================\nimport os\nimport glob\n\nrag_db_path = os.path.join(DATA_DIR, \"rag_visual_db\")\n\nprint(f\"Current DATA_DIR: {DATA_DIR}\")\nprint(f\"Code looks for RAG images at: {rag_db_path}\")\n\nif os.path.exists(rag_db_path):\n    print(\"‚úÖ Folder exists.\")\n    images = glob.glob(os.path.join(rag_db_path, \"**\", \"*.png\"), recursive=True)\n    images += glob.glob(os.path.join(rag_db_path, \"**\", \"*.jpg\"), recursive=True)\n    print(f\"‚úÖ Found {len(images)} images in database.\")\nelse:\n    print(\"‚ùå Folder NOT found.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JB5KJmE_wHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7D: Batch Test (Upload ZIP & Inspect All)\n# ============================================================\nfrom google.colab import files\nimport shutil\nimport zipfile\nimport glob\n\nprint(\"Upload a ZIP file containing PDF drawings for batch inspection:\")\nuploaded = files.upload()\n\nif uploaded:\n    zip_filename = next((f for f in uploaded if f.lower().endswith('.zip')), None)\n\n    if zip_filename:\n        batch_dir = \"batch_drawings\"\n        if os.path.exists(batch_dir):\n            shutil.rmtree(batch_dir)\n        os.makedirs(batch_dir, exist_ok=True)\n\n        print(f\"Extracting {zip_filename} to {batch_dir}...\")\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(batch_dir)\n\n        pdfs = glob.glob(os.path.join(batch_dir, \"**/*.pdf\"), recursive=True)\n        print(f\"Found {len(pdfs)} PDFs in archive.\")\n\n        print(f\"Running batch inspection...\")\n        results = inspect_batch(\n            drawing_folder=batch_dir,\n            output_file=\"inspection_results.json\"\n        )\n    else:\n        print(\"No .zip file found in upload.\")\nelse:\n    print(\"No files uploaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG8mlYNMwHHb"
   },
   "source": [
    "## 8. View Failed Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DgLE5trwHHb"
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 8: View Failed Inspections\n# ============================================================\n\ndef show_failures(results):\n    \"\"\"Display details of failed inspections.\"\"\"\n    failures = [r for r in results if r.get('result') == 'FAIL']\n\n    print(f\"\\nFAILED INSPECTIONS: {len(failures)}\")\n    print('='*60)\n\n    for i, fail in enumerate(failures, 1):\n        print(f\"\\n[{i}] {fail.get('file', 'Unknown')}\")\n        print(f\"    Part: {fail.get('part_number', 'N/A')} - {fail.get('description', 'N/A')}\")\n        print(f\"    Assembly: {fail.get('assembly', 'N/A')}\")\n        print(f\"    GD&T Found: {fail.get('gdt_symbols', 'N/A')}\")\n        print(f\"    Details: {fail.get('details', 'N/A')[:500]}...\")\n\n\n# Usage:\n# show_failures(results)\nprint(\"‚úÖ show_failures() function defined.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8024a37d19d149cd8d5fcebb04443799": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b59f6d5995594d909cd39f10e8a3fa61",
       "IPY_MODEL_49bf137ce3594f5fad2c04f000dd8148",
       "IPY_MODEL_f52e2c702ae84250b1f5208bad2fa7ec"
      ],
      "layout": "IPY_MODEL_5b662ad0551d4ac4b9bfac9d9e3bc511"
     }
    },
    "b59f6d5995594d909cd39f10e8a3fa61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0181fba481e34d48b0afe547f430d500",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b613b9eb5cd94dca83d6dea13ae21285",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "49bf137ce3594f5fad2c04f000dd8148": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d53fd4269fc4e5292d7d6fd6b17df93",
      "max": 38,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fd7fee055d724bf38dc9b903c990c052",
      "value": 38
     }
    },
    "f52e2c702ae84250b1f5208bad2fa7ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fce5f0a8a2a242879992d551f3378a17",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_df4f872ed2544b6bb60f515ec1fc6b07",
      "value": "‚Äá38/38‚Äá[10:35&lt;00:00,‚Äá12.59s/it]"
     }
    },
    "5b662ad0551d4ac4b9bfac9d9e3bc511": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0181fba481e34d48b0afe547f430d500": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b613b9eb5cd94dca83d6dea13ae21285": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d53fd4269fc4e5292d7d6fd6b17df93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd7fee055d724bf38dc9b903c990c052": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fce5f0a8a2a242879992d551f3378a17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df4f872ed2544b6bb60f515ec1fc6b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}