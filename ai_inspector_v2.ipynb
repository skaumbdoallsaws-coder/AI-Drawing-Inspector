{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/ai_inspector_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QxQqDl17wtB"
   },
   "source": [
    "# AI Engineering Drawing Inspector v2.0\n",
    "\n",
    "**Single-File QC Pipeline**\n",
    "\n",
    "Outputs:\n",
    "1. `ResolvedPartIdentity.json`\n",
    "2. `DrawingEvidence.json`\n",
    "3. `DiffResult.json`\n",
    "4. `QCReport.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUxTZFYm7wtC",
    "outputId": "01b8c907-8a6d-4ae9-d926-eec713225394",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q pymupdf opencv-python-headless jsonschema pillow pytesseract\n",
    "!pip install -q accelerate qwen-vl-utils bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q json-repair\n",
    "!apt-get install -y poppler-utils tesseract-ocr > /dev/null 2>&1\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import os, json, re, gc\n",
    "import torch\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "DRAWING_PDF_PATH = \"\"\n",
    "SOLIDWORKS_JSON_DIR = \"sw_json_library\"\n",
    "OUTPUT_DIR = \"qc_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "metadata": {
    "id": "BywKsGSW7wtD",
    "outputId": "2951f4f8-0274-4e12-8430-ba56efdd7dd8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: BOM-Robust JSON Loader\n",
    "def load_json_robust(filepath) -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"Load JSON with BOM handling. Tries: utf-8-sig, utf-8, latin-1\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    for enc in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=enc) as f:\n",
    "                return json.load(f), None\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except json.JSONDecodeError as e:\n",
    "            if 'BOM' in str(e) and enc == 'utf-8':\n",
    "                continue\n",
    "            return None, f\"JSON error: {str(e)[:50]}\"\n",
    "        except Exception as e:\n",
    "            return None, f\"Error: {str(e)[:50]}\"\n",
    "    return None, \"Failed all encodings\"\n",
    "\n",
    "print(\"load_json_robust defined\")"
   ],
   "metadata": {
    "id": "Fip3em4Z7wtD",
    "outputId": "d5a33a45-0a96-4d3c-96ec-1743b4ffad86",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: PDF Rendering\n",
    "@dataclass\n",
    "class PageArtifact:\n",
    "    pageIndex0: int\n",
    "    page: int\n",
    "    image: Image.Image\n",
    "    width: int\n",
    "    height: int\n",
    "    dpi: int\n",
    "    direct_text: Optional[str] = None\n",
    "\n",
    "def render_pdf(pdf_path: str, dpi: int = 300) -> List[PageArtifact]:\n",
    "    \"\"\"Render first page of PDF to image.\"\"\"\n",
    "    artifacts = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(0)\n",
    "    zoom = dpi / 72.0\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    direct_text = page.get_text(\"text\")\n",
    "\n",
    "    artifacts.append(PageArtifact(\n",
    "        pageIndex0=0, page=1, image=img,\n",
    "        width=pix.width, height=pix.height, dpi=dpi,\n",
    "        direct_text=direct_text if len(direct_text.strip()) > 10 else None\n",
    "    ))\n",
    "    doc.close()\n",
    "    print(f\"Rendered: {pix.width}x{pix.height}px\")\n",
    "    return artifacts\n",
    "\n",
    "print(\"render_pdf defined\")"
   ],
   "metadata": {
    "id": "g5CgMeOp7wtD",
    "outputId": "7a4eb37a-f1aa-443e-c7b6-500fdd605c71",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: SolidWorks JSON Library\n",
    "@dataclass\n",
    "class SwPartEntry:\n",
    "    json_path: str\n",
    "    part_number: str\n",
    "    filename_stem: str = \"\"\n",
    "    data: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class SwJsonLibrary:\n",
    "    def __init__(self):\n",
    "        self.by_part_number: Dict[str, SwPartEntry] = {}\n",
    "        self.by_filename: Dict[str, SwPartEntry] = {}\n",
    "        self.all_entries: List[SwPartEntry] = []\n",
    "\n",
    "    def _normalize(self, s: str) -> str:\n",
    "        return re.sub(r'[-\\s_]', '', str(s or '')).lower()\n",
    "\n",
    "    def load_from_directory(self, directory: str):\n",
    "        json_files = list(Path(directory).glob(\"**/*.json\"))\n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "        for jp in json_files:\n",
    "            data, err = load_json_robust(jp)\n",
    "            if data is None:\n",
    "                continue\n",
    "            pn = data.get('identity', {}).get('partNumber', '')\n",
    "            entry = SwPartEntry(str(jp), pn, jp.stem, data)\n",
    "            self.all_entries.append(entry)\n",
    "            if pn:\n",
    "                self.by_part_number[pn] = entry\n",
    "                self.by_part_number[self._normalize(pn)] = entry\n",
    "            self.by_filename[jp.stem] = entry\n",
    "            self.by_filename[self._normalize(jp.stem)] = entry\n",
    "        print(f\"Loaded {len(self.all_entries)} files\")\n",
    "\n",
    "    def lookup(self, candidate: str) -> Optional[SwPartEntry]:\n",
    "        if not candidate:\n",
    "            return None\n",
    "        norm = self._normalize(candidate)\n",
    "        return self.by_part_number.get(candidate) or self.by_part_number.get(norm) or \\\n",
    "               self.by_filename.get(candidate) or self.by_filename.get(norm)\n",
    "\n",
    "sw_library = SwJsonLibrary()\n",
    "print(\"SwJsonLibrary defined\")"
   ],
   "metadata": {
    "id": "w7qDW6w27wtE",
    "outputId": "f0b7906c-7392-4a09-be7b-686a377447d2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6: Part Identity Resolution (Robust Matching)\n",
    "\n",
    "@dataclass\n",
    "class ResolvedPartIdentity:\n",
    "    partNumber: str\n",
    "    confidence: float\n",
    "    source: str\n",
    "    swJsonPath: Optional[str] = None\n",
    "    candidates_tried: List[str] = field(default_factory=list)\n",
    "\n",
    "def clean_filename(filename: str) -> str:\n",
    "    \"\"\"Remove known suffixes like Paint, REV, etc.\"\"\"\n",
    "    cleaned = re.sub(r'[\\s_]*(Paint|PAINT)$', '', filename, flags=re.IGNORECASE)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_pn_candidates(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract potential part number candidates from filename.\n",
    "    Handles: 1013572_01, 101357201-03, 314884W_0, 046-935-REV-A\n",
    "    Returns list of candidates (most specific to least).\n",
    "    \"\"\"\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    # Remove duplicate markers like (1), (2)\n",
    "    name_no_ext = re.sub(r'\\s*\\(\\d+\\)$', '', name_no_ext)\n",
    "    cleaned = clean_filename(name_no_ext)\n",
    "    parts = re.split(r'[\\s_]+', cleaned)\n",
    "\n",
    "    if not parts:\n",
    "        return []\n",
    "\n",
    "    base = parts[0]\n",
    "    candidates = []\n",
    "\n",
    "    # 1. Base as-is\n",
    "    candidates.append(base)\n",
    "\n",
    "    # 2. Without hyphens\n",
    "    base_no_hyphen = base.replace('-', '')\n",
    "    if base_no_hyphen != base:\n",
    "        candidates.append(base_no_hyphen)\n",
    "\n",
    "    # 3. Remove letter suffixes (046-935A -> 046-935)\n",
    "    if base and base[-1].isalpha() and len(base) > 1:\n",
    "        candidates.append(base[:-1])\n",
    "        candidates.append(base[:-1].replace('-', ''))\n",
    "\n",
    "    # 4. Handle revision pattern (046-935-01 -> 046-935)\n",
    "    rev_match = re.match(r'^(.+)-(\\d{1,2})$', base)\n",
    "    if rev_match:\n",
    "        main_part = rev_match.group(1)\n",
    "        candidates.append(main_part)\n",
    "        candidates.append(main_part.replace('-', ''))\n",
    "\n",
    "    # 5. Handle REV suffix (046-935-REV-A -> 046-935)\n",
    "    rev_alpha = re.match(r'^(.+?)[-_]?REV[-_]?[A-Z0-9]*$', base, re.IGNORECASE)\n",
    "    if rev_alpha:\n",
    "        candidates.append(rev_alpha.group(1))\n",
    "        candidates.append(rev_alpha.group(1).replace('-', ''))\n",
    "\n",
    "    # 6. Peeling - progressively remove trailing digits\n",
    "    temp = base_no_hyphen\n",
    "    while len(temp) > 5:\n",
    "        temp = temp[:-1]\n",
    "        candidates.append(temp)\n",
    "\n",
    "    # Remove duplicates, preserve order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return unique\n",
    "\n",
    "def resolve_part_identity(pdf_path: str, artifacts: List[PageArtifact], sw_lib: SwJsonLibrary) -> ResolvedPartIdentity:\n",
    "    \"\"\"Resolve part identity using robust filename matching.\"\"\"\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    candidates = extract_pn_candidates(filename)\n",
    "\n",
    "    # Try each candidate against SW library\n",
    "    for candidate in candidates:\n",
    "        entry = sw_lib.lookup(candidate)\n",
    "        if entry:\n",
    "            return ResolvedPartIdentity(\n",
    "                partNumber=entry.part_number or candidate,\n",
    "                confidence=1.0,\n",
    "                source=\"filename+sw\",\n",
    "                swJsonPath=entry.json_path,\n",
    "                candidates_tried=candidates\n",
    "            )\n",
    "\n",
    "    # Try PDF embedded text\n",
    "    for art in artifacts:\n",
    "        if art.direct_text:\n",
    "            text_candidates = extract_pn_candidates(art.direct_text[:200])\n",
    "            for candidate in text_candidates[:5]:\n",
    "                entry = sw_lib.lookup(candidate)\n",
    "                if entry:\n",
    "                    return ResolvedPartIdentity(\n",
    "                        partNumber=entry.part_number or candidate,\n",
    "                        confidence=0.8,\n",
    "                        source=\"pdf_text+sw\",\n",
    "                        swJsonPath=entry.json_path,\n",
    "                        candidates_tried=candidates + text_candidates[:5]\n",
    "                    )\n",
    "\n",
    "    # Fallback - use first candidate or filename stem\n",
    "    fallback_pn = candidates[0] if candidates else Path(pdf_path).stem\n",
    "    return ResolvedPartIdentity(\n",
    "        partNumber=fallback_pn,\n",
    "        confidence=0.3,\n",
    "        source=\"fallback\",\n",
    "        swJsonPath=None,\n",
    "        candidates_tried=candidates\n",
    "    )\n",
    "\n",
    "print(\"resolve_part_identity defined (robust matching)\")"
   ],
   "metadata": {
    "id": "U5TWtUzhJnRw",
    "outputId": "af4f4ff0-57d8-4e10-e41b-7fce6868e3c4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Load SolidWorks Library (Upload ZIP)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(SOLIDWORKS_JSON_DIR) or not list(Path(SOLIDWORKS_JSON_DIR).glob(\"*.json\")):\n",
    "    print(\"Upload your sw_json_library.zip file:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded:\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as z:\n",
    "                z.extractall(SOLIDWORKS_JSON_DIR)\n",
    "            print(f\"Extracted to {SOLIDWORKS_JSON_DIR}\")\n",
    "            break\n",
    "\n",
    "sw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\n",
    "print(f\"Library ready: {len(sw_library.all_entries)} parts indexed\")"
   ],
   "metadata": {
    "id": "qOgdhn3iJnRx",
    "outputId": "aca8b36f-0851-4ae6-be78-11cf21105587",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Upload and Render PDF Drawing\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Upload your PDF drawing:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded:\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        DRAWING_PDF_PATH = filename\n",
    "        break\n",
    "\n",
    "print(f\"Processing: {DRAWING_PDF_PATH}\")\n",
    "artifacts = render_pdf(DRAWING_PDF_PATH)\n",
    "\n",
    "# Display the rendered image\n",
    "if artifacts:\n",
    "    display(artifacts[0].image.resize((800, int(800 * artifacts[0].height / artifacts[0].width))))"
   ],
   "metadata": {
    "id": "ZQJrQtNzJnRx",
    "outputId": "a93c7de9-08d9-4bb0-fbd6-1901ca1f8e5a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 9: Resolve Part Identity\n",
    "part_identity = resolve_part_identity(DRAWING_PDF_PATH, artifacts, sw_library)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RESOLVED PART IDENTITY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Part Number:  {part_identity.partNumber}\")\n",
    "print(f\"Confidence:   {part_identity.confidence}\")\n",
    "print(f\"Source:       {part_identity.source}\")\n",
    "print(f\"SW JSON:      {part_identity.swJsonPath or 'Not found'}\")\n",
    "print(f\"Candidates:   {part_identity.candidates_tried[:5]}\")\n",
    "\n",
    "# Save to output\n",
    "identity_out = os.path.join(OUTPUT_DIR, \"ResolvedPartIdentity.json\")\n",
    "with open(identity_out, 'w') as f:\n",
    "    json.dump(asdict(part_identity), f, indent=2)\n",
    "print(f\"\\nSaved: {identity_out}\")"
   ],
   "metadata": {
    "id": "nWyBd12_JnRx",
    "outputId": "6e99b752-e819-4c32-a7bd-c70159361c0b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 10: Load LightOnOCR-2 and Run OCR\n",
    "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
    "from google.colab import userdata\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Get HF token\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "except:\n",
    "    hf_token = None\n",
    "\n",
    "print(\"Loading LightOnOCR-2-1B...\")\n",
    "ocr_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ocr_dtype = torch.bfloat16 if ocr_device == \"cuda\" else torch.float32\n",
    "\n",
    "ocr_processor = LightOnOcrProcessor.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    \"lightonai/LightOnOCR-2-1B\",\n",
    "    torch_dtype=ocr_dtype,\n",
    "    token=hf_token\n",
    ").to(ocr_device)\n",
    "\n",
    "print(f\"LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "\n",
    "def run_lighton_ocr(image: Image.Image) -> List[str]:\n",
    "    \"\"\"Run LightOnOCR-2 on image, return list of text lines.\"\"\"\n",
    "    global ocr_model, ocr_processor, ocr_device, ocr_dtype\n",
    "\n",
    "    img = image.convert(\"RGB\")\n",
    "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": img}]}]\n",
    "\n",
    "    inputs = ocr_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
    "\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return [line.strip() for line in output_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "# Run OCR on the drawing\n",
    "print(\"Running OCR on drawing...\")\n",
    "ocr_lines = run_lighton_ocr(artifacts[0].image)\n",
    "print(f\"OCR extracted {len(ocr_lines)} lines\")\n",
    "print(\"\\nFirst 10 lines:\")\n",
    "for line in ocr_lines[:10]:\n",
    "    print(f\"  {line}\")"
   ],
   "metadata": {
    "id": "NQuVga-jJnRx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "5e4e284cd7ab48c3844b8c384ab761f2",
      "0cfdacd08fa042f8be56e2d8da167a39",
      "31e69cda6ba646409858731986dd20ce",
      "86744362b5c34d79bc1af50c2b09eacb",
      "68d0bafb0ffa416cb5db15705e93e21d",
      "842142a108a64a5b9ce022040897ed63",
      "14458669c2964eddb86137d77a16013f",
      "c1dce60d1173473f811b6f552562a929",
      "ab0ec7d782844ffdb92e4ef0503fa92b",
      "41047b56182b4040b42002c758a74c1c",
      "37e9b18aa9a743a18dad942b59e3b5f6"
     ]
    },
    "outputId": "19c837e4-827f-4b5a-f36d-eccdaa5a520d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 10b: Qwen2.5-VL Drawing Understanding (with JSON repair)\nfrom transformers import AutoModelForVision2Seq, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom json_repair import repair_json\n\n# Clear some GPU memory before loading Qwen\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Loading Qwen2.5-VL-7B for drawing understanding...\")\nqwen_model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\nqwen_processor = AutoProcessor.from_pretrained(qwen_model_id, trust_remote_code=True)\nqwen_model = AutoModelForVision2Seq.from_pretrained(\n    qwen_model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nprint(f\"Qwen loaded: {qwen_model.get_memory_footprint() / 1e9:.2f} GB\")\n\ndef analyze_drawing_with_qwen(image: Image.Image) -> Dict[str, Any]:\n    \"\"\"Use Qwen-VL to understand the engineering drawing visually.\"\"\"\n\n    prompt = \"\"\"Analyze this engineering drawing and identify all features. Return a JSON object with:\n\n{\n  \"partDescription\": \"brief description of the part\",\n  \"views\": [\"list of views shown: TOP, FRONT, SIDE, ISOMETRIC, SECTION, DETAIL\"],\n  \"features\": [\n    {\n      \"type\": \"TappedHole|ThroughHole|BlindHole|Counterbore|Countersink|Slot|Fillet|Chamfer|Thread\",\n      \"description\": \"brief description\",\n      \"callout\": \"the dimension/callout text if visible\",\n      \"quantity\": 1,\n      \"location\": \"where on the part\"\n    }\n  ],\n  \"material\": \"material if shown in title block\",\n  \"titleBlockInfo\": {\n    \"partNumber\": \"if visible\",\n    \"revision\": \"if visible\",\n    \"scale\": \"if visible\"\n  },\n  \"notes\": [\"any general notes visible on drawing\"]\n}\n\nBe thorough - identify ALL holes, threads, chamfers, fillets, and other machined features you can see.\nOnly return valid JSON, no other text.\"\"\"\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }\n    ]\n\n    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n\n    inputs = qwen_processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\"\n    ).to(qwen_model.device)\n\n    with torch.no_grad():\n        output_ids = qwen_model.generate(**inputs, max_new_tokens=4096, temperature=0.1)\n\n    generated_ids = output_ids[0, inputs.input_ids.shape[1]:]\n    response = qwen_processor.decode(generated_ids, skip_special_tokens=True)\n\n    # Parse JSON from response with repair\n    try:\n        # Try to extract JSON block from response\n        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n        if json_match:\n            json_str = json_match.group(1)\n        else:\n            json_match = re.search(r'\\{[\\s\\S]*\\}', response)\n            json_str = json_match.group() if json_match else response\n\n        # Try direct parse first\n        try:\n            return json.loads(json_str)\n        except json.JSONDecodeError:\n            # Use json-repair to fix malformed JSON\n            print(\"  Attempting JSON repair...\")\n            repaired = repair_json(json_str)\n            return json.loads(repaired)\n\n    except Exception as e:\n        return {\"raw_response\": response[:1000], \"parse_error\": str(e)}\n\n# Analyze the drawing\nprint(\"Analyzing drawing with Qwen-VL...\")\nqwen_understanding = analyze_drawing_with_qwen(artifacts[0].image)\n\nprint(\"=\"*50)\nprint(\"QWEN DRAWING UNDERSTANDING\")\nprint(\"=\"*50)\n\nif \"parse_error\" not in qwen_understanding:\n    print(f\"Part: {qwen_understanding.get('partDescription', 'N/A')}\")\n    print(f\"Views: {qwen_understanding.get('views', [])}\")\n    print(f\"Material: {qwen_understanding.get('material', 'N/A')}\")\n    print(f\"\\nFeatures identified: {len(qwen_understanding.get('features', []))}\")\n    for f in qwen_understanding.get('features', [])[:10]:\n        print(f\"  - {f.get('type')}: {f.get('callout', f.get('description', ''))}\")\n    if qwen_understanding.get('notes'):\n        print(f\"\\nNotes: {qwen_understanding.get('notes', [])[:3]}\")\nelse:\n    print(f\"Parse error: {qwen_understanding.get('parse_error')}\")\n    print(f\"Raw response:\\n{qwen_understanding.get('raw_response', '')[:500]}\")\n\n# Save understanding\nunderstanding_out = os.path.join(OUTPUT_DIR, \"QwenUnderstanding.json\")\nwith open(understanding_out, 'w') as f:\n    json.dump(qwen_understanding, f, indent=2)\nprint(f\"\\nSaved: {understanding_out}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "783e96d8cc404e5882273fd7570908a1",
      "06fe060030444e89a4a201d6d9bc6aeb",
      "56f26bca996e4513817f2241d94787e2",
      "c490c827c52d4774907440c6947239bf",
      "264972f9a69547e1a790d6463adb339d",
      "a14854a59b854e889d9c045ffc8b1fe3",
      "68fa0cd60efe4286a4bc7678f6f7c496",
      "544c818f6de24f3688c3b0b98903bb1e",
      "25399bb24c7a45f7a28549566953f57c",
      "7fe6af4ee1d446389d58edbee92dc8e0",
      "37eeb98191574935b2b44983cce98e47",
      "51b6c54e40b7432d9b76d689774fe5fa",
      "89ccb4c5592f4ca2b4b592f73b65de7c",
      "8813c7c37eae4ba198e01546822c2bfb",
      "60b7e965d171497c864c45b26f2451ae",
      "8ab85fbe595c4211bddf0e39f626b82d",
      "bdfd9a2d88b248079fbfe6c64500f2e6",
      "e38e6a5545d94ef5ba74a35def443ea7",
      "5b39297b545144d182536251ae7df0f5",
      "310f120a4f6e4948af9d2bd365b5031a",
      "5630a3cb91ca4ba8ad7c9b05cb8a4a64",
      "c1f3dd826a5b4dc2a96e8bc5218359be",
      "ba623ec0c0ca4ce499af4de4c3109cbd",
      "8ed36fe4109e40289ceb3f1c8dcbaeca",
      "a1745e2aee6d49229b5b891c79b6ad9f",
      "2f491a651a2843aa995fdca2cd37b1e4",
      "726451de32a84cdba64783d3010f3704",
      "02cf9f12c67c4470925e6f2bb441738f",
      "af3ad8d919cb4f639745026fd3f458c9",
      "0c1ea4ee36d24e3e9fd40314fa0738ba",
      "7e32fcc86e9c433eb4c30afbfae27d28",
      "1f196ad7785e4bfe8adae962baf90bf4",
      "2aedb4fa3c7e4e3abc5be11e6fd7da6d",
      "047c265f52ef4fa798ad214682e57b97",
      "01c49ae628ca42c7adadf1732b262f1e",
      "adb82352258d46429ee68f0cfdf1b782",
      "534cbe1c1ba143c1aa874a5d75adb0bc",
      "630ac6e4d9ec4b8d90ae995c509b5c18",
      "54a0667b01534779a78bf3de8bead0f9",
      "76f6c4471c43483a9ec28accdc0f0605",
      "8665a3c1cbb047939a0ef0424497d783",
      "90566e4405e94763b08751aa99922d3e",
      "726c1b75ad4d44bfaf1b6f09a10274b6",
      "ad4bc72f0a40473d8313e5119139a3a1",
      "3051ae2850c047f48f5b580c611cf5a5",
      "30a0e4cf2ca443e2a61b2e876b0e32e4",
      "a931855f198041be9e9f5a8d62b72257",
      "88e6ef93ac644809b4dfe741a8c51d6c",
      "60b092af260346b49753dbfa782dd1d1",
      "b803ec98ab60476694b95b8fcdaf0c5a",
      "cedf2e2e9f2842a2a198c01bb2fac6a8",
      "b89d6708dcc74f13afb1c96ae055e183",
      "696578a2a3574cceae0fb57cc9947fba",
      "970d94d9456d49be9448336b573ec271",
      "dab01d75a8b646c8af2ab4cf2fb24e30",
      "d2e15ddf8dda442f8aec468e15e6349b",
      "39669f74f6af450aaca90e7cf9a9e491",
      "2d960c990c10445691208d1e43089c2e",
      "dc25a7291a054395a4567a56ba51c64a",
      "f1cadb836e1642b78fdac1b7acedabe3",
      "74875f3b11e64f8c90d8497bb380fab1",
      "e50f931f12074dad9c3268a6df262ab2",
      "da87ed5c426345c49fcf94147a08e9fc",
      "f2b0a85e442d4f8baa57ff5c911a3f40",
      "fc1d33ac2a8a41a09bb1e666247ab79a",
      "5ea6a4e3728f41c3b55c2975e2dab315",
      "8894ad1772214775a94bfa4d3180bc3e",
      "4962c167066747fcb6ca0ed2d3c85274",
      "ba256701773040a69b1852e43e096a4e",
      "c1f9552eb3394d309b90dbfb359e5c78",
      "39297fa65c6746979a476426db20e6e8",
      "693924a198f244c2ba7ef45a9d9d8dd8",
      "92eb7cade325486db791080e4fcc4071",
      "18588588c44443e6add68c35fb2b232f",
      "d3681f1d8a584cc79c05c4ee27e8d88f",
      "ba3af1a4124842afa89052049cf14fd1",
      "01d1450232c34bb083e4bf4411d75239",
      "6ff3ba0d0aa048a194f95406c8563585",
      "2ff0c870c46c4633bbd676b9ecffca0b",
      "3c3ff097e59b488c8437d76fe39cdca8",
      "4fa901a4395044c4ad7084429069ea90",
      "bfc60223394d480a90642cad7750b8fb",
      "0b89d2ed5b4e4c49bc5848ffecafa49f",
      "d46c9ee8adf9489dae16ae9d443f1e01",
      "2f82ac4c5a974e57a8d1140d3f42a2aa",
      "abf24d793d90461bba6069234c43a1e0",
      "07b982e7bd7945ec9a0a0eb537288981",
      "eb7f6ecaf4ce4b74b7218c2211f9313f",
      "4010c7b7df9a4c129d91e68e171d1794",
      "93cc8d90bb574142b420e002af65bdec",
      "d71b1cfa0de24fa4a649ffb7693260a6",
      "376d0ba034cb42faab65743d06c0da60",
      "1a68f57079f94f1f8a2008962270fc94",
      "c2fab513604445f99b98ea8b60f099aa",
      "9699baa97e2c464f9a8689472cc1e8f2",
      "5b2fb1cc9c5c473b82463f8ff11731e5",
      "6dbee3fd9ce242ab92f3ba88a24faed1",
      "42fd7de69ea749d89b5ed0099a97905f",
      "6e7d1b889c794b84be65845134827307",
      "783e2c498f3d42f2b598d31830d5e91b",
      "10c0ae5fac674168992a4a2668a39d9b",
      "05a153bce85441c9aee2c7e60c908da9",
      "894593f5dac54dddb415694ce993fb31",
      "4ad6e1b7d7a34bd4aa9ed13ad5b94115",
      "f5eeb4bfe1de42adbe384fad4da611ec",
      "8876f12bed294deca712ee08381c502b",
      "a8234738f91d49e480124c50428e58e9",
      "daef25b71c7a45e5a7df85366e013b7b",
      "f9c6722c3759455a9c4c813922b85b53",
      "6dad16404e814b49b9f18627cc5c55c7",
      "0e7618dd70f941e19c9a7af6fdfcd17f",
      "881d4c00485745bda2bf1ce319a6ba28",
      "6d69a062fd324b9989150c3ada39172b",
      "da1ad7deb18d448898937086a45fa24b",
      "f4ad03339a404b188e7abebd2e0c3e10",
      "32b5d431dea741859daa3178a6974964",
      "a7a6a8021b554d2d8c2d25aadde10593",
      "46ee74b3b18c4ca8b445b5d7c8784c6f",
      "82b94254f9534891a9ecc4a34fdb5929",
      "fad61aac04ea4448a6fc7175426fc21a",
      "62ee7fb33042435aa158838436949c4c"
     ]
    },
    "id": "IZg8-vwCYxps",
    "outputId": "375513ec-e534-4826-b368-3a1ba4971dac"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 11: Merge OCR + Qwen Understanding into Enriched Evidence\n",
    "\n",
    "INCH_TO_MM = 25.4\n",
    "\n",
    "# Regex patterns for extracting callouts from OCR\n",
    "PATTERNS = {\n",
    "    'metric_thread': r'M(\\d+(?:\\.\\d+)?)\\s*[xX]\\s*(\\d+(?:\\.\\d+)?)',\n",
    "    'imperial_thread': r'(\\d+/\\d+)\\s*-\\s*(\\d+)',\n",
    "    'thru_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*(?:mm|MM|\")?\\s*THRU',\n",
    "    'blind_hole': r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*|\\d+)\\s*[xX]\\s*(\\d+\\.?\\d*)\\s*(?:DEEP|DP)',\n",
    "    'fillet': r'\\bR(\\d+\\.?\\d*)\\b',\n",
    "    'chamfer': r'(\\d+\\.?\\d*)\\s*[xX]\\s*45\\s*[\u00b0]?',\n",
    "}\n",
    "\n",
    "def is_likely_imperial(value: float, raw_text: str) -> bool:\n",
    "    if '\"' in raw_text or 'IN' in raw_text.upper():\n",
    "        return True\n",
    "    if value < 1.0 and 'mm' not in raw_text.lower():\n",
    "        return True\n",
    "    imperial_sizes = [0.125, 0.1875, 0.25, 0.281, 0.3125, 0.375, 0.4375, 0.5, 0.625, 0.75, 0.875]\n",
    "    for imp in imperial_sizes:\n",
    "        if abs(value - imp) < 0.01:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convert_to_mm(value: float, raw_text: str) -> float:\n",
    "    if is_likely_imperial(value, raw_text):\n",
    "        return round(value * INCH_TO_MM, 3)\n",
    "    return value\n",
    "\n",
    "def parse_ocr_callouts(ocr_lines: List[str]) -> List[Dict]:\n",
    "    \"\"\"Extract callouts from OCR text.\"\"\"\n",
    "    callouts = []\n",
    "    raw_text = \"\\n\".join(ocr_lines)\n",
    "\n",
    "    for match in re.finditer(PATTERNS['metric_thread'], raw_text, re.IGNORECASE):\n",
    "        callouts.append({\n",
    "            'calloutType': 'TappedHole',\n",
    "            'thread': {'standard': 'Metric', 'nominalDiameterMm': float(match.group(1)), 'pitch': float(match.group(2))},\n",
    "            'raw': match.group(0), 'source': 'ocr'\n",
    "        })\n",
    "\n",
    "    for match in re.finditer(PATTERNS['thru_hole'], raw_text, re.IGNORECASE):\n",
    "        raw = match.group(0)\n",
    "        val = float(match.group(1))\n",
    "        callouts.append({\n",
    "            'calloutType': 'Hole', 'diameterMm': convert_to_mm(val, raw), 'diameterRaw': val,\n",
    "            'isImperial': is_likely_imperial(val, raw), 'isThrough': True, 'raw': raw, 'source': 'ocr'\n",
    "        })\n",
    "\n",
    "    for match in re.finditer(PATTERNS['fillet'], raw_text, re.IGNORECASE):\n",
    "        raw = match.group(0)\n",
    "        val = float(match.group(1))\n",
    "        callouts.append({'calloutType': 'Fillet', 'radiusMm': convert_to_mm(val, raw), 'raw': raw, 'source': 'ocr'})\n",
    "\n",
    "    for match in re.finditer(PATTERNS['chamfer'], raw_text, re.IGNORECASE):\n",
    "        raw = match.group(0)\n",
    "        val = float(match.group(1))\n",
    "        callouts.append({'calloutType': 'Chamfer', 'distance1Mm': convert_to_mm(val, raw), 'angleDegrees': 45, 'raw': raw, 'source': 'ocr'})\n",
    "\n",
    "    return callouts\n",
    "\n",
    "def parse_qwen_features(qwen_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Convert Qwen features to callout format.\"\"\"\n",
    "    callouts = []\n",
    "    if \"parse_error\" in qwen_data:\n",
    "        return callouts\n",
    "\n",
    "    type_map = {\n",
    "        'TappedHole': 'TappedHole', 'ThroughHole': 'Hole', 'BlindHole': 'Hole',\n",
    "        'Counterbore': 'Hole', 'Countersink': 'Hole', 'Fillet': 'Fillet',\n",
    "        'Chamfer': 'Chamfer', 'Thread': 'TappedHole', 'Slot': 'Slot'\n",
    "    }\n",
    "\n",
    "    for feat in qwen_data.get('features', []):\n",
    "        ftype = feat.get('type', '')\n",
    "        callout_type = type_map.get(ftype, ftype)\n",
    "        callout = feat.get('callout', '')\n",
    "        qty = feat.get('quantity', 1)\n",
    "\n",
    "        entry = {\n",
    "            'calloutType': callout_type,\n",
    "            'description': feat.get('description', ''),\n",
    "            'location': feat.get('location', ''),\n",
    "            'quantity': qty,\n",
    "            'raw': callout,\n",
    "            'source': 'qwen'\n",
    "        }\n",
    "\n",
    "        # Try to parse dimensions from Qwen's callout text\n",
    "        if callout:\n",
    "            thread_match = re.search(r'M(\\d+(?:\\.\\d+)?)[xX](\\d+(?:\\.\\d+)?)', callout)\n",
    "            if thread_match:\n",
    "                entry['thread'] = {\n",
    "                    'standard': 'Metric',\n",
    "                    'nominalDiameterMm': float(thread_match.group(1)),\n",
    "                    'pitch': float(thread_match.group(2))\n",
    "                }\n",
    "\n",
    "            hole_match = re.search(r'[oO\u00d8\u2205\u03c6]?\\s*(\\.?\\d+\\.?\\d*)', callout)\n",
    "            if hole_match and callout_type == 'Hole':\n",
    "                val = float(hole_match.group(1))\n",
    "                entry['diameterMm'] = convert_to_mm(val, callout)\n",
    "                entry['diameterRaw'] = val\n",
    "                entry['isThrough'] = 'THRU' in callout.upper()\n",
    "\n",
    "        callouts.append(entry)\n",
    "\n",
    "    return callouts\n",
    "\n",
    "def merge_evidence(ocr_callouts: List[Dict], qwen_callouts: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Merge OCR and Qwen callouts, preferring OCR for dimensions but using Qwen for context.\"\"\"\n",
    "    merged = []\n",
    "    used_qwen = set()\n",
    "\n",
    "    for ocr in ocr_callouts:\n",
    "        merged_entry = ocr.copy()\n",
    "        merged_entry['sources'] = ['ocr']\n",
    "\n",
    "        # Try to find matching Qwen feature for additional context\n",
    "        for qi, qwen in enumerate(qwen_callouts):\n",
    "            if qi in used_qwen:\n",
    "                continue\n",
    "            if ocr.get('calloutType') == qwen.get('calloutType'):\n",
    "                # Check if dimensions roughly match\n",
    "                if ocr.get('thread') and qwen.get('thread'):\n",
    "                    if ocr['thread'].get('nominalDiameterMm') == qwen['thread'].get('nominalDiameterMm'):\n",
    "                        merged_entry['location'] = qwen.get('location', '')\n",
    "                        merged_entry['description'] = qwen.get('description', '')\n",
    "                        merged_entry['sources'].append('qwen')\n",
    "                        used_qwen.add(qi)\n",
    "                        break\n",
    "                elif ocr.get('diameterMm') and qwen.get('diameterMm'):\n",
    "                    if abs(ocr['diameterMm'] - qwen['diameterMm']) < 1.0:\n",
    "                        merged_entry['location'] = qwen.get('location', '')\n",
    "                        merged_entry['description'] = qwen.get('description', '')\n",
    "                        merged_entry['sources'].append('qwen')\n",
    "                        used_qwen.add(qi)\n",
    "                        break\n",
    "\n",
    "        merged.append(merged_entry)\n",
    "\n",
    "    # Add Qwen features not matched to OCR (may be features OCR missed)\n",
    "    for qi, qwen in enumerate(qwen_callouts):\n",
    "        if qi not in used_qwen:\n",
    "            qwen['sources'] = ['qwen_only']\n",
    "            merged.append(qwen)\n",
    "\n",
    "    return merged\n",
    "\n",
    "# Parse both sources\n",
    "ocr_callouts = parse_ocr_callouts(ocr_lines)\n",
    "qwen_callouts = parse_qwen_features(qwen_understanding)\n",
    "\n",
    "# Merge\n",
    "merged_callouts = merge_evidence(ocr_callouts, qwen_callouts)\n",
    "\n",
    "# Build enriched evidence\n",
    "evidence = {\n",
    "    'schemaVersion': '1.2.0',\n",
    "    'partNumber': part_identity.partNumber,\n",
    "    'extractedAt': datetime.now().isoformat() + 'Z',\n",
    "    'sources': {\n",
    "        'ocr': {'model': 'LightOnOCR-2-1B', 'lineCount': len(ocr_lines)},\n",
    "        'vision': {'model': 'Qwen2-VL-7B', 'featureCount': len(qwen_callouts)}\n",
    "    },\n",
    "    'drawingInfo': {\n",
    "        'views': qwen_understanding.get('views', []),\n",
    "        'partDescription': qwen_understanding.get('partDescription', ''),\n",
    "        'material': qwen_understanding.get('material', ''),\n",
    "        'titleBlock': qwen_understanding.get('titleBlockInfo', {}),\n",
    "        'notes': qwen_understanding.get('notes', [])\n",
    "    },\n",
    "    'foundCallouts': merged_callouts,\n",
    "    'rawOcrSample': ocr_lines[:15]\n",
    "}\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MERGED DRAWING EVIDENCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"OCR callouts:   {len(ocr_callouts)}\")\n",
    "print(f\"Qwen features:  {len(qwen_callouts)}\")\n",
    "print(f\"Merged total:   {len(merged_callouts)}\")\n",
    "print(f\"\\nDrawing info:\")\n",
    "print(f\"  Views: {evidence['drawingInfo']['views']}\")\n",
    "print(f\"  Material: {evidence['drawingInfo']['material']}\")\n",
    "print(f\"\\nMerged callouts:\")\n",
    "for c in merged_callouts[:10]:\n",
    "    sources = '+'.join(c.get('sources', []))\n",
    "    extra = f\" [{sources}]\"\n",
    "    if c.get('location'):\n",
    "        extra += f\" @ {c['location']}\"\n",
    "    print(f\"  {c['calloutType']}: {c.get('raw', c.get('description', ''))}{extra}\")\n",
    "\n",
    "# Save\n",
    "evidence_out = os.path.join(OUTPUT_DIR, \"DrawingEvidence.json\")\n",
    "with open(evidence_out, 'w') as f:\n",
    "    json.dump(evidence, f, indent=2)\n",
    "print(f\"\\nSaved: {evidence_out}\")"
   ],
   "metadata": {
    "id": "3ZlPb5FJQBHV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 12: Generate DiffResult (Fixed: uses comparison.holeGroups)\n",
    "\n",
    "def extract_sw_requirements(sw_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract requirements from SolidWorks JSON using comparison.holeGroups.\"\"\"\n",
    "    requirements = []\n",
    "\n",
    "    # Primary source: comparison.holeGroups (reconciled/canonical data)\n",
    "    comparison = sw_data.get('comparison', {})\n",
    "    hole_groups = comparison.get('holeGroups', [])\n",
    "\n",
    "    for hg in hole_groups:\n",
    "        hole_type = hg.get('holeType', '')\n",
    "        canonical = hg.get('canonical', '')\n",
    "        count = hg.get('count', 1)\n",
    "        diameters = hg.get('diameters', {})\n",
    "        thread = hg.get('thread', {})\n",
    "\n",
    "        if hole_type == 'Tapped':\n",
    "            # Tapped hole - extract thread info\n",
    "            requirements.append({\n",
    "                'type': 'TappedHole',\n",
    "                'thread': {\n",
    "                    'standard': thread.get('standard', 'Metric'),\n",
    "                    'nominalDiameterMm': thread.get('majorDiameterMm') or diameters.get('threadNominalDiameterMm'),\n",
    "                    'pitch': thread.get('pitch'),\n",
    "                    'callout': thread.get('callout', canonical)\n",
    "                },\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "        elif hole_type == 'Through':\n",
    "            # Plain through hole\n",
    "            diameter_mm = diameters.get('pilotOrTapDrillDiameterMm')\n",
    "            requirements.append({\n",
    "                'type': 'Hole',\n",
    "                'diameterMm': diameter_mm,\n",
    "                'diameterInches': diameters.get('pilotOrTapDrillDiameterInches'),\n",
    "                'isThrough': True,\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "        elif hole_type == 'Blind':\n",
    "            # Blind hole\n",
    "            diameter_mm = diameters.get('pilotOrTapDrillDiameterMm')\n",
    "            requirements.append({\n",
    "                'type': 'Hole',\n",
    "                'diameterMm': diameter_mm,\n",
    "                'isThrough': False,\n",
    "                'count': count,\n",
    "                'canonical': canonical,\n",
    "                'source': 'sw_comparison.holeGroups'\n",
    "            })\n",
    "\n",
    "    # Fallback: features.holeWizardHoles if no comparison data\n",
    "    if not requirements:\n",
    "        features = sw_data.get('features', {})\n",
    "        for hole in features.get('holeWizardHoles', []):\n",
    "            if hole.get('isTapped'):\n",
    "                thread_size = hole.get('threadSize', '')\n",
    "                # Parse M6x1.0 format\n",
    "                m = re.match(r'M(\\d+(?:\\.\\d+)?)[xX](\\d+(?:\\.\\d+)?)', thread_size)\n",
    "                if m:\n",
    "                    requirements.append({\n",
    "                        'type': 'TappedHole',\n",
    "                        'thread': {\n",
    "                            'standard': 'Metric',\n",
    "                            'nominalDiameterMm': float(m.group(1)),\n",
    "                            'pitch': float(m.group(2)),\n",
    "                            'callout': thread_size\n",
    "                        },\n",
    "                        'count': hole.get('instanceCount', 1),\n",
    "                        'source': 'sw_features.holeWizardHoles'\n",
    "                    })\n",
    "            else:\n",
    "                requirements.append({\n",
    "                    'type': 'Hole',\n",
    "                    'diameterMm': hole.get('diameter', 0) * 1000,  # meters to mm\n",
    "                    'isThrough': hole.get('isThrough', False),\n",
    "                    'count': hole.get('instanceCount', 1),\n",
    "                    'source': 'sw_features.holeWizardHoles'\n",
    "                })\n",
    "\n",
    "        # Fillets\n",
    "        for fillet in features.get('fillets', []):\n",
    "            requirements.append({\n",
    "                'type': 'Fillet',\n",
    "                'radiusMm': fillet.get('radius'),\n",
    "                'source': 'sw_features'\n",
    "            })\n",
    "\n",
    "        # Chamfers\n",
    "        for chamfer in features.get('chamfers', []):\n",
    "            requirements.append({\n",
    "                'type': 'Chamfer',\n",
    "                'distance1Mm': chamfer.get('distance1'),\n",
    "                'angleDegrees': chamfer.get('angle', 45),\n",
    "                'source': 'sw_features'\n",
    "            })\n",
    "\n",
    "    return requirements\n",
    "\n",
    "def compare_callout_to_requirement(callout: Dict, req: Dict, tolerance: float = 0.5) -> bool:\n",
    "    \"\"\"Check if a drawing callout matches a SW requirement.\"\"\"\n",
    "    ctype = callout.get('calloutType')\n",
    "    rtype = req.get('type')\n",
    "\n",
    "    if ctype != rtype:\n",
    "        return False\n",
    "\n",
    "    if ctype == 'Hole':\n",
    "        d1 = callout.get('diameterMm', 0)\n",
    "        d2 = req.get('diameterMm', 0)\n",
    "        if d1 and d2 and abs(d1 - d2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'TappedHole':\n",
    "        t1 = callout.get('thread', {})\n",
    "        t2 = req.get('thread', {})\n",
    "        # Match by nominal diameter (within 0.1mm tolerance)\n",
    "        nom1 = t1.get('nominalDiameterMm', 0)\n",
    "        nom2 = t2.get('nominalDiameterMm', 0)\n",
    "        if nom1 and nom2 and abs(nom1 - nom2) < 0.1:\n",
    "            # Also check pitch if available\n",
    "            p1 = t1.get('pitch')\n",
    "            p2 = t2.get('pitch')\n",
    "            if p1 and p2:\n",
    "                return abs(p1 - p2) < 0.01\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'Fillet':\n",
    "        r1 = callout.get('radiusMm', 0)\n",
    "        r2 = req.get('radiusMm', 0)\n",
    "        if r1 and r2 and abs(r1 - r2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    elif ctype == 'Chamfer':\n",
    "        d1 = callout.get('distance1Mm', 0)\n",
    "        d2 = req.get('distance1Mm', 0)\n",
    "        if d1 and d2 and abs(d1 - d2) <= tolerance:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def generate_diff_result(evidence: Dict, sw_data: Dict) -> Dict:\n",
    "    \"\"\"Compare drawing evidence against SolidWorks requirements.\"\"\"\n",
    "    callouts = evidence.get('foundCallouts', [])\n",
    "    requirements = extract_sw_requirements(sw_data)\n",
    "\n",
    "    found = []\n",
    "    missing = []\n",
    "    matched_callouts = set()\n",
    "    matched_requirements = set()\n",
    "\n",
    "    # Check each requirement against callouts\n",
    "    for ri, req in enumerate(requirements):\n",
    "        match_found = False\n",
    "        for ci, callout in enumerate(callouts):\n",
    "            if ci not in matched_callouts and compare_callout_to_requirement(callout, req):\n",
    "                found.append({\n",
    "                    'status': 'FOUND',\n",
    "                    'requirement': req,\n",
    "                    'evidence': callout,\n",
    "                    'note': f\"Matched: {req.get('canonical', req.get('type'))}\"\n",
    "                })\n",
    "                matched_callouts.add(ci)\n",
    "                matched_requirements.add(ri)\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            missing.append({\n",
    "                'status': 'MISSING',\n",
    "                'requirement': req,\n",
    "                'evidence': None,\n",
    "                'note': f\"Not found in drawing: {req.get('canonical', req.get('type'))}\"\n",
    "            })\n",
    "\n",
    "    # Extra callouts not matched to any requirement\n",
    "    extra = []\n",
    "    for ci, callout in enumerate(callouts):\n",
    "        if ci not in matched_callouts:\n",
    "            extra.append({\n",
    "                'status': 'EXTRA',\n",
    "                'requirement': None,\n",
    "                'evidence': callout,\n",
    "                'note': f\"In drawing but not in SW: {callout.get('raw', callout.get('calloutType'))}\"\n",
    "            })\n",
    "\n",
    "    diff_result = {\n",
    "        'partNumber': evidence.get('partNumber'),\n",
    "        'generatedAt': datetime.now().isoformat() + 'Z',\n",
    "        'summary': {\n",
    "            'totalRequirements': len(requirements),\n",
    "            'found': len(found),\n",
    "            'missing': len(missing),\n",
    "            'extra': len(extra),\n",
    "            'matchRate': f\"{len(found)/len(requirements)*100:.1f}%\" if requirements else \"N/A\"\n",
    "        },\n",
    "        'details': {\n",
    "            'found': found,\n",
    "            'missing': missing,\n",
    "            'extra': extra\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return diff_result\n",
    "\n",
    "# Load SW data and generate diff\n",
    "if part_identity.swJsonPath:\n",
    "    sw_data, err = load_json_robust(part_identity.swJsonPath)\n",
    "    if sw_data:\n",
    "        # Show what we're extracting\n",
    "        requirements = extract_sw_requirements(sw_data)\n",
    "        print(\"=\"*50)\n",
    "        print(\"SW REQUIREMENTS EXTRACTED\")\n",
    "        print(\"=\"*50)\n",
    "        for req in requirements:\n",
    "            print(f\"  {req['type']}: {req.get('canonical', req.get('thread', {}).get('callout', ''))}\")\n",
    "\n",
    "        diff_result = generate_diff_result(evidence, sw_data)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DIFF RESULT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Part: {diff_result['partNumber']}\")\n",
    "        print(f\"  Total Requirements: {diff_result['summary']['totalRequirements']}\")\n",
    "        print(f\"  FOUND:   {diff_result['summary']['found']}\")\n",
    "        print(f\"  MISSING: {diff_result['summary']['missing']}\")\n",
    "        print(f\"  EXTRA:   {diff_result['summary']['extra']}\")\n",
    "        print(f\"  Match Rate: {diff_result['summary']['matchRate']}\")\n",
    "\n",
    "        if diff_result['details']['found']:\n",
    "            print(\"\\nMatched:\")\n",
    "            for item in diff_result['details']['found']:\n",
    "                print(f\"  \u2713 {item['note']}\")\n",
    "\n",
    "        if diff_result['details']['missing']:\n",
    "            print(\"\\nMissing from drawing:\")\n",
    "            for item in diff_result['details']['missing']:\n",
    "                print(f\"  \u2717 {item['note']}\")\n",
    "\n",
    "        if diff_result['details']['extra']:\n",
    "            print(\"\\nExtra in drawing:\")\n",
    "            for item in diff_result['details']['extra']:\n",
    "                print(f\"  ? {item['note']}\")\n",
    "\n",
    "        # Save\n",
    "        diff_out = os.path.join(OUTPUT_DIR, \"DiffResult.json\")\n",
    "        with open(diff_out, 'w') as f:\n",
    "            json.dump(diff_result, f, indent=2)\n",
    "        print(f\"\\nSaved: {diff_out}\")\n",
    "    else:\n",
    "        print(f\"Error loading SW JSON: {err}\")\n",
    "else:\n",
    "    print(\"No SW JSON path - cannot generate diff\")"
   ],
   "metadata": {
    "id": "eZuYl-gdQBHV"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}