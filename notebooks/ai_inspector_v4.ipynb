{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Drawing Inspector v4.0\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skaumbdoallsaws-coder/AI-Drawing-Inspector/blob/main/notebooks/ai_inspector_v4.ipynb)\n",
    "\n",
    "**Modular Type-Aware Architecture**\n",
    "\n",
    "This notebook uses the `ai_inspector` package for shared utilities while keeping\n",
    "all v3 functionality including assembly context, siblings, and mate relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q pymupdf pillow openai\n",
    "!pip install -q accelerate qwen-vl-utils bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q json-repair\n",
    "\n",
    "# Clone and install ai_inspector package\n",
    "!git clone https://github.com/skaumbdoallsaws-coder/AI-Drawing-Inspector.git /content/AI-tool 2>/dev/null || (cd /content/AI-tool && git pull)\n",
    "%cd /content/AI-tool\n",
    "!pip install -q -e .\n",
    "print('Dependencies installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and Configuration\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "from google.colab import files, userdata\n",
    "from IPython.display import display, Markdown\n",
    "from PIL import Image\n",
    "\n",
    "# ai_inspector package imports\n",
    "from ai_inspector import classify_drawing, DrawingType, __version__\n",
    "from ai_inspector.utils import render_pdf, SwJsonLibrary, extract_pdf_text, load_json_robust\n",
    "from ai_inspector.utils.pdf_render import PageArtifact\n",
    "from ai_inspector.analyzers import resolve_part_identity, ResolvedPartIdentity\n",
    "from ai_inspector.extractors.ocr import preprocess_ocr_text, parse_ocr_callouts, PATTERNS\n",
    "from ai_inspector.extractors.vlm import (\n",
    "    FEATURE_EXTRACTION_PROMPT, QUALITY_AUDIT_PROMPT,\n",
    "    BOM_EXTRACTION_PROMPT, MANUFACTURING_NOTES_PROMPT,\n",
    "    PAGE_CLASSIFICATION_PROMPT\n",
    ")\n",
    "from ai_inspector.comparison import (\n",
    "    extract_sw_requirements, extract_mate_requirements,\n",
    "    generate_diff_result, create_stub_diff_result\n",
    ")\n",
    "\n",
    "print(f'ai_inspector v{__version__} loaded')\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = '/content/output'\n",
    "SOLIDWORKS_JSON_DIR = '/content/sw_json_library'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(SOLIDWORKS_JSON_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Output directory: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load AI Models\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from transformers import LightOnOcrForConditionalGeneration, LightOnOcrProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from json_repair import repair_json\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load LightOnOCR-2\n",
    "print('Loading LightOnOCR-2-1B...')\n",
    "ocr_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ocr_dtype = torch.bfloat16 if ocr_device == 'cuda' else torch.float32\n",
    "\n",
    "ocr_processor = LightOnOcrProcessor.from_pretrained('lightonai/LightOnOCR-2-1B', token=hf_token)\n",
    "ocr_model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    'lightonai/LightOnOCR-2-1B', torch_dtype=ocr_dtype, token=hf_token\n",
    ").to(ocr_device)\n",
    "print(f'LightOnOCR-2 loaded: {ocr_model.get_memory_footprint() / 1e9:.2f} GB')\n",
    "\n",
    "# Load Qwen2.5-VL\n",
    "print('\\nLoading Qwen2.5-VL-7B...')\n",
    "qwen_processor = AutoProcessor.from_pretrained('Qwen/Qwen2.5-VL-7B-Instruct', trust_remote_code=True)\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    'Qwen/Qwen2.5-VL-7B-Instruct',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f'Qwen2.5-VL loaded: {qwen_model.get_memory_footprint() / 1e9:.2f} GB')\n",
    "\n",
    "def run_lighton_ocr(image: Image.Image) -> List[str]:\n",
    "    \"\"\"Run LightOnOCR-2 on image, return list of text lines.\"\"\"\n",
    "    img = image.convert('RGB')\n",
    "    conversation = [{'role': 'user', 'content': [{'type': 'image', 'image': img}]}]\n",
    "    inputs = ocr_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device=ocr_device, dtype=ocr_dtype) if v.is_floating_point() else v.to(ocr_device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output_ids = ocr_model.generate(**inputs, max_new_tokens=2048)\n",
    "    generated_ids = output_ids[0, inputs['input_ids'].shape[1]:]\n",
    "    output_text = ocr_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "    return [line.strip() for line in output_text.split('\\n') if line.strip()]\n",
    "\n",
    "def run_qwen_analysis(image: Image.Image, prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run Qwen2.5-VL with a given prompt and return parsed JSON.\"\"\"\n",
    "    messages = [{'role': 'user', 'content': [{'type': 'image', 'image': image}, {'type': 'text', 'text': prompt}]}]\n",
    "    text = qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = qwen_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors='pt').to(qwen_model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = qwen_model.generate(**inputs, max_new_tokens=4096, temperature=0.1)\n",
    "    generated_ids = output_ids[0, inputs.input_ids.shape[1]:]\n",
    "    response = qwen_processor.decode(generated_ids, skip_special_tokens=True)\n",
    "    try:\n",
    "        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response)\n",
    "        json_str = json_match.group(1) if json_match else re.search(r'\\{[\\s\\S]*\\}', response).group()\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return json.loads(repair_json(json_str))\n",
    "    except Exception as e:\n",
    "        return {'raw_response': response[:1000], 'parse_error': str(e)}\n",
    "\n",
    "print('\\nModels ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load SolidWorks Library + Assembly Context Databases\n",
    "import zipfile\n",
    "\n",
    "print('Upload your sw_json_library.zip file:')\n",
    "print('(Should contain part JSONs + optionally sw_part_context_complete.json and sw_inspector_requirements.json)')\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded:\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f'Extracting {filename}...')\n",
    "        with zipfile.ZipFile(filename, 'r') as z:\n",
    "            z.extractall(SOLIDWORKS_JSON_DIR)\n",
    "        print(f'Extracted to {SOLIDWORKS_JSON_DIR}')\n",
    "        break\n",
    "\n",
    "# Load SW library using package\n",
    "sw_library = SwJsonLibrary()\n",
    "sw_library.load_from_directory(SOLIDWORKS_JSON_DIR)\n",
    "print(f'\\nLibrary ready: {len(sw_library)} parts indexed')\n",
    "\n",
    "# === Load Mating & Sibling Context Databases ===\n",
    "part_context_db = {}\n",
    "inspector_requirements_db = {}\n",
    "\n",
    "def _try_load_json(name, search_paths):\n",
    "    \"\"\"Try loading a JSON file from multiple paths.\"\"\"\n",
    "    for p in search_paths:\n",
    "        full = os.path.join(p, name)\n",
    "        if os.path.exists(full):\n",
    "            with open(full, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            print(f'  Loaded {name}: {len(data)} entries from {p}')\n",
    "            return data\n",
    "    return None\n",
    "\n",
    "_search_paths = [SOLIDWORKS_JSON_DIR, os.path.join(SOLIDWORKS_JSON_DIR, '..'), '/content']\n",
    "\n",
    "print('\\nLoading assembly context databases...')\n",
    "part_context_db = _try_load_json('sw_part_context_complete.json', _search_paths) or {}\n",
    "inspector_requirements_db = _try_load_json('sw_inspector_requirements.json', _search_paths) or {}\n",
    "\n",
    "if not part_context_db:\n",
    "    print('  sw_part_context_complete.json not found - sibling/mating context unavailable')\n",
    "if not inspector_requirements_db:\n",
    "    print('  sw_inspector_requirements.json not found - mate-derived thread requirements unavailable')\n",
    "\n",
    "def get_part_context(part_number: str) -> Optional[Dict]:\n",
    "    \"\"\"Look up part in context DB by new_pn, old_pn, or normalized variants.\"\"\"\n",
    "    if not part_context_db:\n",
    "        return None\n",
    "    candidates = [part_number, part_number.upper(), part_number.lower(),\n",
    "                  part_number.replace('-', ''), part_number.replace('_', '')]\n",
    "    for key in candidates:\n",
    "        if key in part_context_db:\n",
    "            return part_context_db[key]\n",
    "    for key, entry in part_context_db.items():\n",
    "        identity = entry.get('identity', {})\n",
    "        if identity.get('new_pn') == part_number or identity.get('old_pn') == part_number:\n",
    "            return entry\n",
    "    return None\n",
    "\n",
    "def get_inspector_requirements(part_number: str) -> Optional[Dict]:\n",
    "    \"\"\"Look up inspector requirements by part number.\"\"\"\n",
    "    if not inspector_requirements_db:\n",
    "        return None\n",
    "    candidates = [part_number, part_number.upper(), part_number.lower(), part_number.replace('-', '')]\n",
    "    for key in candidates:\n",
    "        if key in inspector_requirements_db:\n",
    "            return inspector_requirements_db[key]\n",
    "    ctx = get_part_context(part_number)\n",
    "    if ctx:\n",
    "        old_pn = ctx.get('identity', {}).get('old_pn', '')\n",
    "        if old_pn:\n",
    "            for key in [old_pn, old_pn.replace('-', ''), old_pn.upper()]:\n",
    "                if key in inspector_requirements_db:\n",
    "                    return inspector_requirements_db[key]\n",
    "    return None\n",
    "\n",
    "print(f'\\nAssembly context: {len(part_context_db)} parts')\n",
    "print(f'Inspector requirements: {len(inspector_requirements_db)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Upload and Render PDF Drawing\n",
    "print('Upload your PDF drawing:')\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded:\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        DRAWING_PDF_PATH = filename\n",
    "        break\n",
    "\n",
    "print(f'\\nProcessing: {DRAWING_PDF_PATH}')\n",
    "print('='*50)\n",
    "\n",
    "# Render PDF using package\n",
    "artifacts = render_pdf(DRAWING_PDF_PATH)\n",
    "\n",
    "# Classify drawing type\n",
    "pdf_text = extract_pdf_text(DRAWING_PDF_PATH)\n",
    "classification = classify_drawing(pdf_text)\n",
    "\n",
    "print(f'\\nDrawing Type: {classification.drawing_type.value}')\n",
    "print(f'Confidence: {classification.confidence:.0%}')\n",
    "print(f'Signals: {classification.signals}')\n",
    "print(f'Use OCR: {classification.use_ocr}')\n",
    "print(f'Use Qwen: {classification.use_qwen}')\n",
    "\n",
    "# Display first page\n",
    "display(artifacts[0].get_thumbnail(800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Resolve Part Identity + Display Assembly Context\n",
    "part_identity = resolve_part_identity(DRAWING_PDF_PATH, artifacts, sw_library)\n",
    "\n",
    "print('='*50)\n",
    "print('RESOLVED PART IDENTITY')\n",
    "print('='*50)\n",
    "print(f'Part Number:  {part_identity.partNumber}')\n",
    "print(f'Confidence:   {part_identity.confidence}')\n",
    "print(f'Source:       {part_identity.source}')\n",
    "print(f'SW JSON:      {part_identity.swJsonPath or \"Not found\"}')\n",
    "print(f'Candidates:   {part_identity.candidates_tried[:5]}')\n",
    "\n",
    "# === Assembly Context Lookup ===\n",
    "assembly_context = None\n",
    "mate_thread_reqs = []\n",
    "\n",
    "ctx = get_part_context(part_identity.partNumber)\n",
    "if ctx:\n",
    "    assembly_context = ctx\n",
    "    hierarchy = ctx.get('hierarchy', {})\n",
    "    mating = ctx.get('mating', {})\n",
    "    siblings = hierarchy.get('siblings', [])\n",
    "    mates_with = mating.get('mates_with', [])\n",
    "    mate_reqs_from_mates = mating.get('requirements_from_mates', [])\n",
    "\n",
    "    print()\n",
    "    print('='*50)\n",
    "    print('ASSEMBLY CONTEXT')\n",
    "    print('='*50)\n",
    "    print(f'Parent Assembly: {hierarchy.get(\"parent_assembly\", \"Unknown\")}')\n",
    "    print(f'Hierarchy Path:  {hierarchy.get(\"hierarchy_path\", \"N/A\")}')\n",
    "\n",
    "    if siblings:\n",
    "        print(f'\\nSiblings ({len(siblings)} parts in same assembly):')\n",
    "        for s in siblings[:8]:\n",
    "            print(f'  - {s.get(\"pn\", s.get(\"name\", \"?\"))} ({s.get(\"desc\", s.get(\"description\", \"\"))})')\n",
    "        if len(siblings) > 8:\n",
    "            print(f'  ... and {len(siblings) - 8} more')\n",
    "\n",
    "    if mates_with:\n",
    "        print(f'\\nMate Relationships ({len(mates_with)} total):')\n",
    "        for m in mates_with[:6]:\n",
    "            thread_info = f\" [{m['thread']}x{m.get('pitch', '')}]\" if m.get('thread') else ''\n",
    "            print(f'  - {m.get(\"mate_type\", \"MATE\")}: {m.get(\"part\", \"?\")} ({m.get(\"description\", \"\")}){thread_info}')\n",
    "        if len(mates_with) > 6:\n",
    "            print(f'  ... and {len(mates_with) - 6} more')\n",
    "\n",
    "    if mate_reqs_from_mates:\n",
    "        print(f'\\nMate-Derived Requirements ({len(mate_reqs_from_mates)}):')\n",
    "        for req in mate_reqs_from_mates[:5]:\n",
    "            print(f'  - {req}')\n",
    "        if len(mate_reqs_from_mates) > 5:\n",
    "            print(f'  ... and {len(mate_reqs_from_mates) - 5} more')\n",
    "else:\n",
    "    print('\\nAssembly context: Not found in database')\n",
    "\n",
    "# Check inspector requirements\n",
    "insp_reqs = get_inspector_requirements(part_identity.partNumber)\n",
    "if insp_reqs:\n",
    "    reqs_by_type = insp_reqs.get('requirements_by_type', {})\n",
    "    thread_holes = reqs_by_type.get('thread_holes', [])\n",
    "    if thread_holes:\n",
    "        print(f'\\nThread Hole Requirements from Mates ({len(thread_holes)}):')\n",
    "        for th in thread_holes:\n",
    "            print(f'  - {th}')\n",
    "        mate_thread_reqs = thread_holes\n",
    "\n",
    "# Save outputs\n",
    "identity_out = os.path.join(OUTPUT_DIR, 'ResolvedPartIdentity.json')\n",
    "with open(identity_out, 'w') as f:\n",
    "    json.dump(asdict(part_identity), f, indent=2)\n",
    "print(f'\\nSaved: {identity_out}')\n",
    "\n",
    "if assembly_context:\n",
    "    ctx_out = os.path.join(OUTPUT_DIR, 'AssemblyContext.json')\n",
    "    with open(ctx_out, 'w') as f:\n",
    "        json.dump({\n",
    "            'partNumber': part_identity.partNumber,\n",
    "            'hierarchy': assembly_context.get('hierarchy', {}),\n",
    "            'mating': assembly_context.get('mating', {}),\n",
    "            'identity': assembly_context.get('identity', {}),\n",
    "        }, f, indent=2)\n",
    "    print(f'Saved: {ctx_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Page Classification + Run OCR and Qwen Analysis\n",
    "print('='*50)\n",
    "print('PAGE CLASSIFICATION')\n",
    "print('='*50)\n",
    "\n",
    "# Classify each page\n",
    "pages_needing_ocr = []\n",
    "pages_with_bom = []\n",
    "pages_with_details = []\n",
    "\n",
    "for art in artifacts:\n",
    "    page_class = run_qwen_analysis(art.image, PAGE_CLASSIFICATION_PROMPT)\n",
    "    page_type = page_class.get('pageType', 'PART_DETAIL')\n",
    "    art.drawing_type = page_type\n",
    "    art.has_bom = page_class.get('hasBOM', False)\n",
    "    \n",
    "    # Override: PART_DETAIL always gets OCR\n",
    "    if page_type == 'PART_DETAIL' or page_type == 'MIXED':\n",
    "        art.needs_ocr = True\n",
    "        pages_needing_ocr.append(art)\n",
    "        pages_with_details.append(art)\n",
    "    elif page_type == 'ASSEMBLY_BOM':\n",
    "        art.needs_ocr = False\n",
    "        pages_with_bom.append(art)\n",
    "    else:\n",
    "        art.needs_ocr = classification.use_ocr\n",
    "        if art.needs_ocr:\n",
    "            pages_needing_ocr.append(art)\n",
    "    \n",
    "    print(f'  Page {art.page}: {page_type} (OCR: {art.needs_ocr}, BOM: {art.has_bom})')\n",
    "\n",
    "overall_drawing_type = classification.drawing_type.value.upper()\n",
    "print(f'\\nOverall type: {overall_drawing_type}')\n",
    "print(f'Pages needing OCR: {[p.page for p in pages_needing_ocr]}')\n",
    "print(f'Pages with BOM: {[p.page for p in pages_with_bom]}')\n",
    "\n",
    "# === Run OCR ===\n",
    "print('\\n' + '='*50)\n",
    "print('RUNNING OCR')\n",
    "print('='*50)\n",
    "\n",
    "all_ocr_lines = []\n",
    "if pages_needing_ocr:\n",
    "    print(f'Running OCR on {len(pages_needing_ocr)} page(s)...')\n",
    "    for art in pages_needing_ocr:\n",
    "        print(f'  Processing Page {art.page}...')\n",
    "        try:\n",
    "            page_ocr = run_lighton_ocr(art.image)\n",
    "            print(f'    Extracted {len(page_ocr)} lines')\n",
    "            all_ocr_lines.extend(page_ocr)\n",
    "        except Exception as e:\n",
    "            print(f'    Error: {e}')\n",
    "    print(f'\\nTotal OCR lines: {len(all_ocr_lines)}')\n",
    "else:\n",
    "    print('SKIPPING OCR - No pages require text extraction')\n",
    "\n",
    "ocr_lines = all_ocr_lines\n",
    "\n",
    "# === Run Qwen Analyses ===\n",
    "print('\\n' + '='*50)\n",
    "print('QWEN DRAWING ANALYSIS')\n",
    "print('='*50)\n",
    "\n",
    "# Select primary image for analysis\n",
    "primary_image = pages_with_details[0].image if pages_with_details else artifacts[0].image\n",
    "\n",
    "print('Analyzing features...')\n",
    "qwen_understanding = run_qwen_analysis(primary_image, FEATURE_EXTRACTION_PROMPT)\n",
    "if 'parse_error' not in qwen_understanding:\n",
    "    print(f'  Part: {qwen_understanding.get(\"partDescription\", \"N/A\")}')\n",
    "    print(f'  Features: {len(qwen_understanding.get(\"features\", []))}')\n",
    "\n",
    "print('\\nAuditing drawing quality...')\n",
    "drawing_quality = run_qwen_analysis(artifacts[0].image, QUALITY_AUDIT_PROMPT)\n",
    "\n",
    "print('\\nExtracting BOM...')\n",
    "bom_image = pages_with_bom[0].image if pages_with_bom else artifacts[0].image\n",
    "bom_data = run_qwen_analysis(bom_image, BOM_EXTRACTION_PROMPT)\n",
    "print(f'  BOM found: {bom_data.get(\"hasBOM\", False)}')\n",
    "\n",
    "print('\\nExtracting manufacturing notes...')\n",
    "mfg_notes = run_qwen_analysis(artifacts[0].image, MANUFACTURING_NOTES_PROMPT)\n",
    "\n",
    "# Save Qwen outputs\n",
    "qwen_out = os.path.join(OUTPUT_DIR, 'QwenUnderstanding.json')\n",
    "with open(qwen_out, 'w') as f:\n",
    "    json.dump({\n",
    "        'featureAnalysis': qwen_understanding,\n",
    "        'qualityAudit': drawing_quality,\n",
    "        'bomExtraction': bom_data,\n",
    "        'manufacturingNotes': mfg_notes\n",
    "    }, f, indent=2)\n",
    "print(f'\\nSaved: {qwen_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Merge Evidence + Compare to SW Requirements\n",
    "print('='*50)\n",
    "print('MERGING EVIDENCE')\n",
    "print('='*50)\n",
    "\n",
    "# Parse OCR callouts using package\n",
    "ocr_callouts = parse_ocr_callouts(ocr_lines, verbose=True) if ocr_lines else []\n",
    "print(f'OCR callouts parsed: {len(ocr_callouts)}')\n",
    "\n",
    "# Convert Qwen features to callout format\n",
    "qwen_callouts = []\n",
    "for feat in qwen_understanding.get('features', []):\n",
    "    ftype = feat.get('type', '').lower()\n",
    "    callout = feat.get('callout', '')\n",
    "    if 'tapped' in ftype or 'thread' in ftype:\n",
    "        qwen_callouts.append({'calloutType': 'TappedHole', 'raw': callout, 'source': 'qwen'})\n",
    "    elif 'through' in ftype and 'hole' in ftype:\n",
    "        qwen_callouts.append({'calloutType': 'Hole', 'isThrough': True, 'raw': callout, 'source': 'qwen'})\n",
    "    elif 'blind' in ftype and 'hole' in ftype:\n",
    "        qwen_callouts.append({'calloutType': 'Hole', 'isThrough': False, 'raw': callout, 'source': 'qwen'})\n",
    "    elif 'fillet' in ftype:\n",
    "        qwen_callouts.append({'calloutType': 'Fillet', 'raw': callout, 'source': 'qwen'})\n",
    "    elif 'chamfer' in ftype:\n",
    "        qwen_callouts.append({'calloutType': 'Chamfer', 'raw': callout, 'source': 'qwen'})\n",
    "\n",
    "# Merge with deduplication\n",
    "seen_raws = set(c.get('raw', '') for c in ocr_callouts)\n",
    "merged_callouts = list(ocr_callouts)\n",
    "for qc in qwen_callouts:\n",
    "    if qc.get('raw') and qc['raw'] not in seen_raws:\n",
    "        merged_callouts.append(qc)\n",
    "        seen_raws.add(qc['raw'])\n",
    "\n",
    "print(f'Merged callouts: {len(merged_callouts)}')\n",
    "\n",
    "# Build evidence dict\n",
    "evidence = {\n",
    "    'schemaVersion': '4.0.0',\n",
    "    'partNumber': part_identity.partNumber,\n",
    "    'units': 'inches',\n",
    "    'drawingInfo': {\n",
    "        'partDescription': qwen_understanding.get('partDescription', ''),\n",
    "        'material': qwen_understanding.get('material', ''),\n",
    "        'views': qwen_understanding.get('views', []),\n",
    "        'notes': qwen_understanding.get('notes', []),\n",
    "    },\n",
    "    'foundCallouts': merged_callouts,\n",
    "    'rawOcrSample': ocr_lines[:20] if ocr_lines else []\n",
    "}\n",
    "\n",
    "evidence_out = os.path.join(OUTPUT_DIR, 'Evidence.json')\n",
    "with open(evidence_out, 'w') as f:\n",
    "    json.dump(evidence, f, indent=2)\n",
    "print(f'Saved: {evidence_out}')\n",
    "\n",
    "# === Generate DiffResult ===\n",
    "print('\\n' + '='*50)\n",
    "print('COMPARISON TO SW REQUIREMENTS')\n",
    "print('='*50)\n",
    "\n",
    "sw_data = None\n",
    "diff_result = None\n",
    "has_sw_comparison = False\n",
    "\n",
    "if part_identity.swJsonPath:\n",
    "    sw_data, err = load_json_robust(part_identity.swJsonPath)\n",
    "    if sw_data:\n",
    "        has_sw_comparison = True\n",
    "        \n",
    "        # Extract requirements from SW\n",
    "        requirements = extract_sw_requirements(sw_data)\n",
    "        \n",
    "        # Add mate-derived requirements\n",
    "        mate_reqs = extract_mate_requirements(\n",
    "            part_identity.partNumber,\n",
    "            inspector_requirements=insp_reqs,\n",
    "            part_context=assembly_context\n",
    "        )\n",
    "        if mate_reqs:\n",
    "            print(f'Adding {len(mate_reqs)} mate-derived requirements')\n",
    "            requirements.extend(mate_reqs)\n",
    "        \n",
    "        print(f'\\nSW Requirements: {len(requirements)}')\n",
    "        for req in requirements[:10]:\n",
    "            if req['type'] == 'TappedHole':\n",
    "                print(f'  - {req[\"type\"]}: {req.get(\"thread\", {}).get(\"callout\", \"\")} [{req.get(\"source\", \"\")}]')\n",
    "            elif req['type'] == 'Hole':\n",
    "                print(f'  - {req[\"type\"]}: {req.get(\"diameterInches\", 0):.4f}\" [{req.get(\"source\", \"\")}]')\n",
    "            else:\n",
    "                print(f'  - {req[\"type\"]}: {req.get(\"canonical\", \"\")} [{req.get(\"source\", \"\")}]')\n",
    "        \n",
    "        # Generate comparison\n",
    "        diff_result = generate_diff_result(\n",
    "            callouts=merged_callouts,\n",
    "            requirements=requirements,\n",
    "            part_number=part_identity.partNumber\n",
    "        )\n",
    "    else:\n",
    "        print(f'Error loading SW JSON: {err}')\n",
    "\n",
    "if not has_sw_comparison:\n",
    "    print('NO SOLIDWORKS DATA AVAILABLE')\n",
    "    diff_result = create_stub_diff_result(part_identity.partNumber)\n",
    "\n",
    "# Display results\n",
    "print('\\n' + '='*50)\n",
    "print('DIFF RESULT')\n",
    "print('='*50)\n",
    "summary = diff_result.get('summary', {})\n",
    "print(f'Total Requirements: {summary.get(\"totalRequirements\", 0)}')\n",
    "print(f'FOUND:   {summary.get(\"found\", 0)}')\n",
    "print(f'MISSING: {summary.get(\"missing\", 0)}')\n",
    "print(f'EXTRA:   {summary.get(\"extra\", 0)}')\n",
    "print(f'Match Rate: {summary.get(\"matchRate\", \"N/A\")}')\n",
    "if summary.get('mateRequirements', 0) > 0:\n",
    "    print(f'Mate-Derived: {summary.get(\"mateRequirements\", 0)}')\n",
    "\n",
    "if diff_result.get('details', {}).get('missing'):\n",
    "    print('\\nMissing from drawing:')\n",
    "    for item in diff_result['details']['missing'][:5]:\n",
    "        print(f'  X {item.get(\"note\", \"\")}')\n",
    "\n",
    "diff_out = os.path.join(OUTPUT_DIR, 'DiffResult.json')\n",
    "with open(diff_out, 'w') as f:\n",
    "    json.dump(diff_result, f, indent=2)\n",
    "print(f'\\nSaved: {diff_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Display Summary + Download Outputs\n",
    "print('='*50)\n",
    "print('INSPECTION SUMMARY')\n",
    "print('='*50)\n",
    "\n",
    "print(f'\\nPart Number: {part_identity.partNumber}')\n",
    "print(f'Drawing Type: {classification.drawing_type.value}')\n",
    "print(f'SW Data: {\"Available\" if has_sw_comparison else \"NOT AVAILABLE\"}')\n",
    "\n",
    "if has_sw_comparison:\n",
    "    print(f'\\nMatch Rate: {diff_result.get(\"summary\", {}).get(\"matchRate\", \"N/A\")}')\n",
    "    missing_count = diff_result.get('summary', {}).get('missing', 0)\n",
    "    if missing_count == 0:\n",
    "        print('Status: PASS - All requirements found on drawing')\n",
    "    else:\n",
    "        print(f'Status: REVIEW NEEDED - {missing_count} requirement(s) missing')\n",
    "else:\n",
    "    print('Status: REVIEW NEEDED - No CAD comparison available')\n",
    "\n",
    "if assembly_context:\n",
    "    print(f'\\nAssembly Context: Available')\n",
    "    print(f'  Parent: {assembly_context.get(\"hierarchy\", {}).get(\"parent_assembly\", \"N/A\")}')\n",
    "    print(f'  Siblings: {len(assembly_context.get(\"hierarchy\", {}).get(\"siblings\", []))}')\n",
    "    print(f'  Mates: {len(assembly_context.get(\"mating\", {}).get(\"mates_with\", []))}')\n",
    "\n",
    "# List output files\n",
    "print('\\n' + '='*50)\n",
    "print('OUTPUT FILES')\n",
    "print('='*50)\n",
    "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f'  {filename} ({size:,} bytes)')\n",
    "\n",
    "# Download all outputs\n",
    "print('\\nDownloading outputs...')\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    files.download(os.path.join(OUTPUT_DIR, filename))\n",
    "\n",
    "print('\\nDone!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
